{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Amundsen is a data discovery and metadata engine for improving the productivity of data analysts, data scientists and engineers when interacting with data. It does that today by indexing data resources (tables, dashboards, streams, etc.) and powering a page-rank style search based on usage patterns (e.g. highly queried tables show up earlier than less queried tables). Think of it as Google search for data. The project is named after Norwegian explorer Roald Amundsen , the first person to discover the South Pole. Amundsen is hosted by the LF AI & Data Foundation . It includes three microservices, one data ingestion library and one common library. amundsenfrontendlibrary : Frontend service which is a Flask application with a React frontend. amundsensearchlibrary : Search service, which leverages Elasticsearch for search capabilities, is used to power frontend metadata searching. amundsenmetadatalibrary : Metadata service, which leverages Neo4j or Apache Atlas as the persistent layer, to provide various metadata. amundsendatabuilder : Data ingestion library for building metadata graph and search index. Users could either load the data with a python script with the library or with an Airflow DAG importing the library. amundsencommon : Amundsen Common library holds common codes among microservices in Amundsen. amundsengremlin : Amundsen Gremlin library holds code used for converting model objects into vertices and edges in gremlin. It\u2019s used for loading data into an AWS Neptune backend. amundsenrds : Amundsenrds contains ORM models to support relational database as metadata backend store in Amundsen. The schema in ORM models follows the logic of databuilder models. Amundsenrds will be used in databuilder and metadatalibrary for metadata storage and retrieval with relational databases. Homepage \u00b6 https://www.amundsen.io/ Documentation \u00b6 https://www.amundsen.io/amundsen/ Requirements \u00b6 Python = 3.6 or 3.7 Node = v10 or v12 (v14 may have compatibility issues) npm >= 6 User Interface \u00b6 Please note that the mock images only served as demonstration purpose. Landing Page : The landing page for Amundsen including 1. search bars; 2. popular used tables; Search Preview : See inline search results as you type Table Detail Page : Visualization of a Hive / Redshift table Column detail : Visualization of columns of a Hive / Redshift table which includes an optional stats display Data Preview Page : Visualization of table data preview which could integrate with Apache Superset or other Data Visualization Tools. Get Involved in the Community \u00b6 Want help or want to help? Use the button in our header to join our slack channel. Contributions are also more than welcome! As explained in CONTRIBUTING.md there are many ways to contribute, it does not all have to be code with new features and bug fixes, also documentation, like FAQ entries, bug reports, blog posts sharing experiences etc. all help move Amundsen forward. If you find a security vulnerability, please follow this guide . Getting Started \u00b6 Please visit the Amundsen installation documentation for a quick start to bootstrap a default version of Amundsen with dummy data. Architecture Overview \u00b6 Please visit Architecture for Amundsen architecture overview. Supported Entities \u00b6 Tables (from Databases) People (from HR systems) Dashboards Supported Integrations \u00b6 Table Connectors \u00b6 Amazon Athena Amazon Glue and anything built over it Amazon Redshift Apache Cassandra Apache Druid Apache Hive CSV dbt Delta Lake Google BigQuery IBM DB2 Microsoft SQL Server MySQL Oracle (through dbapi or sql_alchemy) PostgreSQL Trino (formerly Presto SQL) Vertica Snowflake Amundsen can also connect to any database that provides dbapi or sql_alchemy interface (which most DBs provide). Table Column Statistics \u00b6 Pandas Profiling Dashboard Connectors \u00b6 Apache Superset Mode Analytics Redash Tableau ETL Orchestration \u00b6 Apache Airflow Installation \u00b6 Please visit Installation guideline on how to install Amundsen. Roadmap \u00b6 Please visit Roadmap if you are interested in Amundsen upcoming roadmap items. Blog Posts and Interviews \u00b6 Amundsen - Lyft\u2019s data discovery & metadata engine (April 2019) Software Engineering Daily podcast on Amundsen (April 2019) How Lyft Drives Data Discovery (July 2019) Data Engineering podcast on Solving Data Discovery At Lyft (Aug 2019) Open Sourcing Amundsen: A Data Discovery And Metadata Platform (Oct 2019) Adding Data Quality into Amundsen with Programmatic Descriptions by Sam Shuster from Edmunds.com (May 2020) Facilitating Data discovery with Apache Atlas and Amundsen by Mariusz G\u00f3rski from ING (June 2020) Using Amundsen to Support User Privacy via Metadata Collection at Square by Alyssa Ransbury from Square (July 14, 2020) Amundsen Joins LF AI as New Incubation Project (Aug 11, 2020) Amundsen: one year later (Oct 6, 2020) Talks \u00b6 Disrupting Data Discovery { slides , recording } (Strata SF, March 2019) Amundsen: A Data Discovery Platform from Lyft { slides } (Data Council SF, April 2019) Disrupting Data Discovery { slides } (Strata London, May 2019) ING Data Analytics Platform (Amundsen is mentioned) { slides , recording } (Kubecon Barcelona, May 2019) Disrupting Data Discovery { slides , recording } (Making Big Data Easy SF, May 2019) Disrupting Data Discovery { slides , recording } (Neo4j Graph Tour Santa Monica, September 2019) Disrupting Data Discovery { slides } (IDEAS SoCal AI & Data Science Conference, Oct 2019) Data Discovery with Amundsen by Gerard Toonstra from Coolblue { slides } and { talk } (BigData Vilnius 2019) Towards Enterprise Grade Data Discovery and Data Lineage with Apache Atlas and Amundsen by Verdan Mahmood and Marek Wiewiorka from ING { slides , talk } (Big Data Technology Warsaw Summit 2020) Airflow @ Lyft (which covers how we integrate Airflow and Amundsen) by Tao Feng { slides and website } (Airflow Summit 2020) Data DAGs with lineage for fun and for profit by Bolke de Bruin { website } (Airflow Summit 2020) Solving Data Discovery Challenges at Lyft with Amundsen, an Open-source Metadata Platform by Tao Feng ( Data+AI summit Europe 2020 ) Data Discovery at Databricks with Amundsen by Tao Feng and Tianru Zhou ( Data+AI summit NA 2021 ) Related Articles \u00b6 How LinkedIn, Uber, Lyft, Airbnb and Netflix are Solving Data Management and Discovery for Machine Learning Solutions Data Discovery in 2020 4 Data Trends to Watch in 2020 Work-Bench Snapshot: The Evolution of Data Discovery & Catalog Future of Data Engineering Governance and Discovery A Data Engineer\u2019s Perspective On Data Democratization Graph Technology Landscape 2020 In-house Data Discovery platforms Linux Foundation AI Foundation Landscape Lyft\u2019s Amundsen: Data-Discovery with Built-In Trust How to find and organize your data from the command-line Data Discovery Platform at Bagelcode Cataloging Tools for Data Teams An Overview of Data Discovery Platforms and Open Source Solutions Hacking Data Discovery in AWS with Amundsen at SEEK A step-by-step guide deploying Amundsen on Google Cloud Platform Machine Learning Features discovery with Feast and Amundsen Data discovery at REA group Community meetings \u00b6 Community meetings are held on the first Thursday of every month at 9 AM Pacific, Noon Eastern, 6 PM Central European Time. Link to join Upcoming meetings & notes \u00b6 You can the exact date for the next meeting and the agenda a few weeks before the meeting in this doc . Notes from all past meetings are available here . Who uses Amundsen? \u00b6 Here is the list of organizations that are using Amundsen today. If your organization uses Amundsen, please file a PR and update this list. Currently officially using Amundsen: Asana Bagelcode Bang & Olufsen Brex Cameo Chan Zuckerberg Initiative Cimpress Technology Coles Group Convoy Databricks Data Sprints Dcard Devoted Health DHI Group Edmunds Everfi Gusto Hurb ING Instacart iRobot Lett LMC Loft Lyft Merlin PicPay Plarium Krasnodar PUBG Rapido REA Group Remitly Snap Square Tile WeTransfer Workday License \u00b6 Apache 2.0 License.","title":"Overview"},{"location":"#homepage","text":"https://www.amundsen.io/","title":"Homepage"},{"location":"#documentation","text":"https://www.amundsen.io/amundsen/","title":"Documentation"},{"location":"#requirements","text":"Python = 3.6 or 3.7 Node = v10 or v12 (v14 may have compatibility issues) npm >= 6","title":"Requirements"},{"location":"#user-interface","text":"Please note that the mock images only served as demonstration purpose. Landing Page : The landing page for Amundsen including 1. search bars; 2. popular used tables; Search Preview : See inline search results as you type Table Detail Page : Visualization of a Hive / Redshift table Column detail : Visualization of columns of a Hive / Redshift table which includes an optional stats display Data Preview Page : Visualization of table data preview which could integrate with Apache Superset or other Data Visualization Tools.","title":"User Interface"},{"location":"#get-involved-in-the-community","text":"Want help or want to help? Use the button in our header to join our slack channel. Contributions are also more than welcome! As explained in CONTRIBUTING.md there are many ways to contribute, it does not all have to be code with new features and bug fixes, also documentation, like FAQ entries, bug reports, blog posts sharing experiences etc. all help move Amundsen forward. If you find a security vulnerability, please follow this guide .","title":"Get Involved in the Community"},{"location":"#getting-started","text":"Please visit the Amundsen installation documentation for a quick start to bootstrap a default version of Amundsen with dummy data.","title":"Getting Started"},{"location":"#architecture-overview","text":"Please visit Architecture for Amundsen architecture overview.","title":"Architecture Overview"},{"location":"#supported-entities","text":"Tables (from Databases) People (from HR systems) Dashboards","title":"Supported Entities"},{"location":"#supported-integrations","text":"","title":"Supported Integrations"},{"location":"#table-connectors","text":"Amazon Athena Amazon Glue and anything built over it Amazon Redshift Apache Cassandra Apache Druid Apache Hive CSV dbt Delta Lake Google BigQuery IBM DB2 Microsoft SQL Server MySQL Oracle (through dbapi or sql_alchemy) PostgreSQL Trino (formerly Presto SQL) Vertica Snowflake Amundsen can also connect to any database that provides dbapi or sql_alchemy interface (which most DBs provide).","title":"Table Connectors"},{"location":"#table-column-statistics","text":"Pandas Profiling","title":"Table Column Statistics"},{"location":"#dashboard-connectors","text":"Apache Superset Mode Analytics Redash Tableau","title":"Dashboard Connectors"},{"location":"#etl-orchestration","text":"Apache Airflow","title":"ETL Orchestration"},{"location":"#installation","text":"Please visit Installation guideline on how to install Amundsen.","title":"Installation"},{"location":"#roadmap","text":"Please visit Roadmap if you are interested in Amundsen upcoming roadmap items.","title":"Roadmap"},{"location":"#blog-posts-and-interviews","text":"Amundsen - Lyft\u2019s data discovery & metadata engine (April 2019) Software Engineering Daily podcast on Amundsen (April 2019) How Lyft Drives Data Discovery (July 2019) Data Engineering podcast on Solving Data Discovery At Lyft (Aug 2019) Open Sourcing Amundsen: A Data Discovery And Metadata Platform (Oct 2019) Adding Data Quality into Amundsen with Programmatic Descriptions by Sam Shuster from Edmunds.com (May 2020) Facilitating Data discovery with Apache Atlas and Amundsen by Mariusz G\u00f3rski from ING (June 2020) Using Amundsen to Support User Privacy via Metadata Collection at Square by Alyssa Ransbury from Square (July 14, 2020) Amundsen Joins LF AI as New Incubation Project (Aug 11, 2020) Amundsen: one year later (Oct 6, 2020)","title":"Blog Posts and Interviews"},{"location":"#talks","text":"Disrupting Data Discovery { slides , recording } (Strata SF, March 2019) Amundsen: A Data Discovery Platform from Lyft { slides } (Data Council SF, April 2019) Disrupting Data Discovery { slides } (Strata London, May 2019) ING Data Analytics Platform (Amundsen is mentioned) { slides , recording } (Kubecon Barcelona, May 2019) Disrupting Data Discovery { slides , recording } (Making Big Data Easy SF, May 2019) Disrupting Data Discovery { slides , recording } (Neo4j Graph Tour Santa Monica, September 2019) Disrupting Data Discovery { slides } (IDEAS SoCal AI & Data Science Conference, Oct 2019) Data Discovery with Amundsen by Gerard Toonstra from Coolblue { slides } and { talk } (BigData Vilnius 2019) Towards Enterprise Grade Data Discovery and Data Lineage with Apache Atlas and Amundsen by Verdan Mahmood and Marek Wiewiorka from ING { slides , talk } (Big Data Technology Warsaw Summit 2020) Airflow @ Lyft (which covers how we integrate Airflow and Amundsen) by Tao Feng { slides and website } (Airflow Summit 2020) Data DAGs with lineage for fun and for profit by Bolke de Bruin { website } (Airflow Summit 2020) Solving Data Discovery Challenges at Lyft with Amundsen, an Open-source Metadata Platform by Tao Feng ( Data+AI summit Europe 2020 ) Data Discovery at Databricks with Amundsen by Tao Feng and Tianru Zhou ( Data+AI summit NA 2021 )","title":"Talks"},{"location":"#related-articles","text":"How LinkedIn, Uber, Lyft, Airbnb and Netflix are Solving Data Management and Discovery for Machine Learning Solutions Data Discovery in 2020 4 Data Trends to Watch in 2020 Work-Bench Snapshot: The Evolution of Data Discovery & Catalog Future of Data Engineering Governance and Discovery A Data Engineer\u2019s Perspective On Data Democratization Graph Technology Landscape 2020 In-house Data Discovery platforms Linux Foundation AI Foundation Landscape Lyft\u2019s Amundsen: Data-Discovery with Built-In Trust How to find and organize your data from the command-line Data Discovery Platform at Bagelcode Cataloging Tools for Data Teams An Overview of Data Discovery Platforms and Open Source Solutions Hacking Data Discovery in AWS with Amundsen at SEEK A step-by-step guide deploying Amundsen on Google Cloud Platform Machine Learning Features discovery with Feast and Amundsen Data discovery at REA group","title":"Related Articles"},{"location":"#community-meetings","text":"Community meetings are held on the first Thursday of every month at 9 AM Pacific, Noon Eastern, 6 PM Central European Time. Link to join","title":"Community meetings"},{"location":"#upcoming-meetings-notes","text":"You can the exact date for the next meeting and the agenda a few weeks before the meeting in this doc . Notes from all past meetings are available here .","title":"Upcoming meetings &amp; notes"},{"location":"#who-uses-amundsen","text":"Here is the list of organizations that are using Amundsen today. If your organization uses Amundsen, please file a PR and update this list. Currently officially using Amundsen: Asana Bagelcode Bang & Olufsen Brex Cameo Chan Zuckerberg Initiative Cimpress Technology Coles Group Convoy Databricks Data Sprints Dcard Devoted Health DHI Group Edmunds Everfi Gusto Hurb ING Instacart iRobot Lett LMC Loft Lyft Merlin PicPay Plarium Krasnodar PUBG Rapido REA Group Remitly Snap Square Tile WeTransfer Workday","title":"Who uses Amundsen?"},{"location":"#license","text":"Apache 2.0 License.","title":"License"},{"location":"CONTRIBUTING/","text":"Contributing Guide \u00b6 Reporting an Issue \u00b6 The easiest way you can contribute to Amundsen is by creating issues. For that, please use the issues section of the Amundsen repository and search for a similar problem. If you don\u2019t find it, submit your bug, question, proposal or feature request. In the case of bugs, please be descriptive and, if possible, include a screenshot of the issue. Creating Pull Requests \u00b6 Before sending a Pull Request with significant changes, please use the issue tracker to discuss the potential improvements you want to make. First-Time Contributors \u00b6 If this is your first contribution to open source, you can follow this tutorial or check this video series to learn about the contribution workflow with GitHub. We always have tickets labeled \u2018good first issue\u2019 and \u2018help wanted\u2019 . These are a great starting point if you want to contribute. Don\u2019t hesitate to ask questions about the issue if you are not sure about the strategy to follow. Requesting a Feature \u00b6 We have created a Roadmap document with our plans for next releases, however, we are open to hear your ideas for new features! For that, you can create an issue and select the \u201cFeature Proposal\u201d template. Fill in as much information as possible, and if you can, add responses to the following questions: We\u2019ll need to add a new model or change any existing model? What would the Migration Plan look like? Will it be backwards-compatible? Which alternatives did you consider? Setup \u00b6 To start contributing to Amundsen, you need to set up your machine to develop with the project. For that, we have prepareda a Developer Guide that will guide you to set up your environment to develop locally with Amundsen. Next Steps \u00b6 Once you have your environment set and ready to go, you can check our documentation and the project\u2019s Roadmap to see what\u2019s coming.","title":"Contributing Guide"},{"location":"CONTRIBUTING/#contributing-guide","text":"","title":"Contributing Guide"},{"location":"CONTRIBUTING/#reporting-an-issue","text":"The easiest way you can contribute to Amundsen is by creating issues. For that, please use the issues section of the Amundsen repository and search for a similar problem. If you don\u2019t find it, submit your bug, question, proposal or feature request. In the case of bugs, please be descriptive and, if possible, include a screenshot of the issue.","title":"Reporting an Issue"},{"location":"CONTRIBUTING/#creating-pull-requests","text":"Before sending a Pull Request with significant changes, please use the issue tracker to discuss the potential improvements you want to make.","title":"Creating Pull Requests"},{"location":"CONTRIBUTING/#first-time-contributors","text":"If this is your first contribution to open source, you can follow this tutorial or check this video series to learn about the contribution workflow with GitHub. We always have tickets labeled \u2018good first issue\u2019 and \u2018help wanted\u2019 . These are a great starting point if you want to contribute. Don\u2019t hesitate to ask questions about the issue if you are not sure about the strategy to follow.","title":"First-Time Contributors"},{"location":"CONTRIBUTING/#requesting-a-feature","text":"We have created a Roadmap document with our plans for next releases, however, we are open to hear your ideas for new features! For that, you can create an issue and select the \u201cFeature Proposal\u201d template. Fill in as much information as possible, and if you can, add responses to the following questions: We\u2019ll need to add a new model or change any existing model? What would the Migration Plan look like? Will it be backwards-compatible? Which alternatives did you consider?","title":"Requesting a Feature"},{"location":"CONTRIBUTING/#setup","text":"To start contributing to Amundsen, you need to set up your machine to develop with the project. For that, we have prepareda a Developer Guide that will guide you to set up your environment to develop locally with Amundsen.","title":"Setup"},{"location":"CONTRIBUTING/#next-steps","text":"Once you have your environment set and ready to go, you can check our documentation and the project\u2019s Roadmap to see what\u2019s coming.","title":"Next Steps"},{"location":"architecture/","text":"Architecture \u00b6 The following diagram shows the overall architecture for Amundsen. Frontend \u00b6 The frontend service serves as web UI portal for users interaction. It is Flask-based web app which representation layer is built with React with Redux, Bootstrap, Webpack, and Babel. Search \u00b6 The search service proxy leverages Elasticsearch\u2019s search functionality (or Apache Atlas\u2019s search API, if that\u2019s the backend you picked) and provides a RESTful API to serve search requests from the frontend service. This API is documented and live explorable through OpenAPI aka \u201cSwagger\u201d. Currently only table resources are indexed and searchable. The search index is built with the databuilder elasticsearch publisher . Metadata \u00b6 The metadata service currently uses a Neo4j proxy to interact with Neo4j graph db and serves frontend service\u2019s metadata. The metadata is represented as a graph model: The above diagram shows how metadata is modeled in Amundsen. Databuilder \u00b6 Amundsen provides a data ingestion library for building the metadata. At Lyft, we build the metadata once a day using an Airflow DAG ( examples ). In addition to \u201creal use\u201d the databuilder is also employed as a handy tool to ingest some \u201cpre-cooked\u201d demo data used in the Quickstart guide. This allows you to have a supersmall sample of data to explore so many of the features in Amundsen are lit up without you even having to setup any connections to databases etc. to ingest real data.","title":"Architecture"},{"location":"architecture/#architecture","text":"The following diagram shows the overall architecture for Amundsen.","title":"Architecture"},{"location":"architecture/#frontend","text":"The frontend service serves as web UI portal for users interaction. It is Flask-based web app which representation layer is built with React with Redux, Bootstrap, Webpack, and Babel.","title":"Frontend"},{"location":"architecture/#search","text":"The search service proxy leverages Elasticsearch\u2019s search functionality (or Apache Atlas\u2019s search API, if that\u2019s the backend you picked) and provides a RESTful API to serve search requests from the frontend service. This API is documented and live explorable through OpenAPI aka \u201cSwagger\u201d. Currently only table resources are indexed and searchable. The search index is built with the databuilder elasticsearch publisher .","title":"Search"},{"location":"architecture/#metadata","text":"The metadata service currently uses a Neo4j proxy to interact with Neo4j graph db and serves frontend service\u2019s metadata. The metadata is represented as a graph model: The above diagram shows how metadata is modeled in Amundsen.","title":"Metadata"},{"location":"architecture/#databuilder","text":"Amundsen provides a data ingestion library for building the metadata. At Lyft, we build the metadata once a day using an Airflow DAG ( examples ). In addition to \u201creal use\u201d the databuilder is also employed as a handy tool to ingest some \u201cpre-cooked\u201d demo data used in the Quickstart guide. This allows you to have a supersmall sample of data to explore so many of the features in Amundsen are lit up without you even having to setup any connections to databases etc. to ingest real data.","title":"Databuilder"},{"location":"deployment-best-practices/","text":"Amundsen allows for many modifications, and many require code-level modifications. Until we put together a \u201cpaved path\u201d suggestion on how to manage such a set-up, for now we will document what community members are doing independenly. If you have a production Amundsen deployment, please edit this doc to describe your setup. Notes from community meeting 2020-12-03 \u00b6 These notes are from 2 companies a community round-table: https://www.youtube.com/watch?v=gVf7S98bnyg Brex \u00b6 What modifications have you made? We\u2019ve added backups We wanted table descriptions to come solely from code. How do you deploy/secure Amundsen? Our hosting is behind VPN, use OIDC What do you use Amundsen for? Our primary use case: if I change this table, what dashboards will it break. We also do PII tagging ETL pipeline puts docs in Snowflake REA group \u00b6 Why did you choose Amundsen? We don\u2019t have data stewards or formal roles who work on documentation. We liked that Amundsen didn\u2019t rely on curated/manual documentation. Google Data Catalog doesn\u2019t allow you to search for data that you don\u2019t have access to. Things that we considered in other vendors - business metric glossary, column level lineage. How do you deploy Amundsen? Deployment on ECS. Built docker images on our own. Deployment is done so that metadata is not lost. Looked into backing metadata in AWS, but decided not to. Instead use block storage so even if the instance goes down, the metadata is still there. We only index prod data sets. We don\u2019t consider Amundsen as a source of truth. Thus, we don\u2019t let people to enable update descriptions. ETL indexer gets descriptions from BQ and puts it into Amundsen. Postgres/source tables need some work to get descriptions from Go into Amundsen. Some changes we\u2019d like to make: Authentication and Authorization Workflow for requesting access to data you can\u2019t already access: right now we don\u2019t have a workflow for requesting access that\u2019s connected to Amundsen. Seems like an area of investment. Data Lineage Business metrics glossary Q&A Why build your own images? Want to make sure system image and code running on the image should be tightly controlled. Patch over the specific files on top of Amundsen upstream code. Don\u2019t fork right now. We chose to patch and not fork. What was the process of getting alpha users onboard and getting feedback? Chose ~8 people who had different roles and different tenure. Then did UX interviews.","title":"Deployment best practices"},{"location":"deployment-best-practices/#notes-from-community-meeting-2020-12-03","text":"These notes are from 2 companies a community round-table: https://www.youtube.com/watch?v=gVf7S98bnyg","title":"Notes from community meeting 2020-12-03"},{"location":"deployment-best-practices/#brex","text":"What modifications have you made? We\u2019ve added backups We wanted table descriptions to come solely from code. How do you deploy/secure Amundsen? Our hosting is behind VPN, use OIDC What do you use Amundsen for? Our primary use case: if I change this table, what dashboards will it break. We also do PII tagging ETL pipeline puts docs in Snowflake","title":"Brex"},{"location":"deployment-best-practices/#rea-group","text":"Why did you choose Amundsen? We don\u2019t have data stewards or formal roles who work on documentation. We liked that Amundsen didn\u2019t rely on curated/manual documentation. Google Data Catalog doesn\u2019t allow you to search for data that you don\u2019t have access to. Things that we considered in other vendors - business metric glossary, column level lineage. How do you deploy Amundsen? Deployment on ECS. Built docker images on our own. Deployment is done so that metadata is not lost. Looked into backing metadata in AWS, but decided not to. Instead use block storage so even if the instance goes down, the metadata is still there. We only index prod data sets. We don\u2019t consider Amundsen as a source of truth. Thus, we don\u2019t let people to enable update descriptions. ETL indexer gets descriptions from BQ and puts it into Amundsen. Postgres/source tables need some work to get descriptions from Go into Amundsen. Some changes we\u2019d like to make: Authentication and Authorization Workflow for requesting access to data you can\u2019t already access: right now we don\u2019t have a workflow for requesting access that\u2019s connected to Amundsen. Seems like an area of investment. Data Lineage Business metrics glossary Q&A Why build your own images? Want to make sure system image and code running on the image should be tightly controlled. Patch over the specific files on top of Amundsen upstream code. Don\u2019t fork right now. We chose to patch and not fork. What was the process of getting alpha users onboard and getting feedback? Chose ~8 people who had different roles and different tenure. Then did UX interviews.","title":"REA group"},{"location":"developer_guide/","text":"Developer Guide \u00b6 This repository uses git submodules to link the code for all of Amundsen\u2019s libraries into a central location. This document offers guidance on how to develop locally with this setup. This workflow leverages docker and docker-compose in a very similar manner to our installation documentation , to spin up instances of all 3 of Amundsen\u2019s services connected with an instances of Neo4j and ElasticSearch which ingest dummy data. Cloning the Repository \u00b6 If cloning the repository for the first time, run the following command to clone the repository and pull the submodules: $ git clone --recursive git@github.com:amundsen-io/amundsen.git If you have already cloned the repository but your submodules are empty, from your cloned amundsen directory run: $ git submodule init $ git submodule update After cloning the repository you can change directories into any of the upstream folders and work in those directories as you normally would. You will have full access to all of the git features, and working in the upstream directories will function the same as if you were working in a cloned version of that repository. Local Development \u00b6 Ensure you have the latest code \u00b6 Beyond running git pull origin master in your local amundsen directory, the submodules for our libraries also have to be manually updated to point to the latest versions of each libraries\u2019 code. When creating a new branch on amundsen to begin local work, ensure your local submodules are pointing to the latest code for each library by running: $ git submodule update --remote Building local changes \u00b6 First, be sure that you have first followed the installation documentation and can spin up a default version of Amundsen without any issues. If you have already completed this step, be sure to have stopped and removed those containers by running: $ docker-compose -f docker-amundsen.yml down Launch the containers needed for local development (the -d option launches in background) : $ docker-compose -f docker-amundsen-local.yml up -d After making local changes rebuild and relaunch modified containers: $ docker-compose -f docker-amundsen-local.yml build \\ && docker-compose -f docker-amundsen-local.yml up -d Optionally, to still tail logs, in a different terminal you can: $ docker-compose -f docker-amundsen-local.yml logs --tail = 3 -f ## - or just tail single container(s): $ docker logs amundsenmetadata --tail 10 -f Local data \u00b6 Local data is persisted under .local/ (at the root of the project), clean up the following directories to reset the databases: # reset elasticsearch rm -rf .local/elasticsearch # reset neo4j rm -rf .local/neo4j Troubleshooting \u00b6 If you have made a change in amundsen/amundsenfrontendlibrary and do not see your changes, this could be due to your browser\u2019s caching behaviors. Either execute a hard refresh (recommended) or clear your browser cache (last resort). Testing Amundsen frontend locally \u00b6 Amundsen has an instruction regarding local frontend launch here Here are some additional changes you might need for windows (OS Win 10): amundsen_application/config.py, set LOCAL_HOST = \u2018127.0.0.1\u2019 amundsen_application/wsgi.py, set host=\u2018127.0.0.1\u2019 (for other microservices also need to change port here because the default is 5000) (using that approach you can run locally another microservices as well if needed) Once you have a running frontend microservice, the rest of Amundsen components can be launched with docker-compose from the root Amundsen project (don\u2019t forget to remove frontend microservice section from docker-amundsen.yml): docker-compose -f docker-amundsen.yml up https://github.com/amundsen-io/amundsen/blob/master/docs/installation.md Developing Dockerbuild file \u00b6 When making edits to Dockerbuild file (docker-amundsen-local.yml) it is good to see what you are getting wrong locally. To do that you build it docker build . And then the output should include a line like so at the step right before it failed: Step 3 /20 : RUN git clone --recursive git://github.com/amundsen-io/amundsenfrontendlibrary.git && cd amundsenfrontendlibrary && git submodule foreach git pull origin master ---> Using cache ---> ec052612747e You can then launch a container from this image like so docker container run -it --name = debug ec052612747e /bin/sh Building and Testing Amundsen Frontend Docker Image (or any other service) \u00b6 Build your image docker build --no-cache . it is recommended that you use \u2013no-cache so you aren\u2019t accidentally using an old version of an image. Determine the hash of your images by running docker images and getting the id of your most recent image Go to your locally cloned amundsen repo and edit the docker compose file \u201cdocker-amundsen.yml\u201d to have the amundsenfrontend image point to the hash of the image that you built amundsenfrontend : #image: amundsendev/amundsen-frontend:1.0.9 #image: 1234.dkr.ecr.us-west-2.amazonaws.com/edmunds/amundsen-frontend:2020-01-21 image : 0312d0ac3938 Pushing image to ECR and using in K8s \u00b6 Assumptions: You have an aws account You have aws command line set up and ready to go Choose an ECR repository you\u2019d like to push to (or create a new one) https://us-west-2.console.aws.amazon.com/ecr/repositories Click onto repository name and open \u201cView push commands\u201d cheat sheet 2b. Login it would look something like this: aws ecr get-login --no-include-email --region us-west-2 Then execute what is returned by above Follow the instructions (you may need to install first AWS CLI, aws-okta and configure your AWS credentials if you haven\u2019t done it before) Given image name is amundsen-frontend, build, tag and push commands will be the following: Here you can see the tag is YYYY-MM-dd but you should choose whatever you like. docker build -t amundsen-frontend:{YYYY-MM-dd} . docker tag amundsen-frontend:{YYYY-MM-dd} <?>.dkr.ecr.<?>.amazonaws.com/amundsen-frontend:{YYYY-MM-dd} docker push <?>.dkr.ecr.<?>.amazonaws.com/amundsen-frontend:{YYYY-MM-dd} Go to the helm/{env}/amundsen/values.yaml and modify to the image tag that you want to use. When updating amundsen-frontend, make sure to do a hard refresh of amundsen with emptying the cache, otherwise you will see stale version of webpage. Test search service in local using staging or production data \u00b6 To test in local, we need to stand up Elasticsearch, publish index data, and stand up Elastic search Standup Elasticsearch \u00b6 Running Elasticsearch via Docker. To install Docker, go here Example: 1 docker run -p 9200:9200 -p 9300:9300 -e \"discovery.type=single-node\" docker.elastic.co/elasticsearch/elasticsearch:6.2.4 (Optional) Standup Kibana \u00b6 1 docker run --link ecstatic_edison:elasticsearch -p 5601:5601 docker.elastic.co/kibana/kibana:6.2.4 *Note that ecstatic_edison is container_id of Elasticsearch container. Update it if it\u2019s different by looking at docker ps Publish Table index through Databuilder \u00b6 Install Databuilder \u00b6 1 2 3 4 5 6 7 cd ~/src/ git clone git@github.com:amundsen-io/amundsendatabuilder.git cd ~/src/amundsendatabuilder virtualenv venv source venv/bin/activate python setup.py install pip install -r requirements.txt Publish Table index \u00b6 First fill this two environment variables: NEO4J_ENDPOINT , CREDENTIALS_NEO4J_PASSWORD 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 $ python import logging import os import uuid from elasticsearch import Elasticsearch from pyhocon import ConfigFactory from databuilder.extractor.neo4j_extractor import Neo4jExtractor from databuilder.extractor.neo4j_search_data_extractor import Neo4jSearchDataExtractor from databuilder.job.job import DefaultJob from databuilder.loader.file_system_elasticsearch_json_loader import FSElasticsearchJSONLoader from databuilder.publisher.elasticsearch_publisher import ElasticsearchPublisher from databuilder.task.task import DefaultTask logging.basicConfig(level=logging.INFO) neo4j_user = 'neo4j' neo4j_password = os.getenv('CREDENTIALS_NEO4J_PASSWORD') neo4j_endpoint = os.getenv('NEO4J_ENDPOINT') elasticsearch_client = Elasticsearch([ {'host': 'localhost'}, ]) data_file_path = '/var/tmp/amundsen/elasticsearch_upload/es_data.json' elasticsearch_new_index = 'table_search_index_{hex_str}'.format(hex_str=uuid.uuid4().hex) logging.info(\"Elasticsearch new index: \" + elasticsearch_new_index) elasticsearch_doc_type = 'table' elasticsearch_index_alias = 'table_search_index' job_config = ConfigFactory.from_dict({ 'extractor.search_data.extractor.neo4j.{}'.format(Neo4jExtractor.GRAPH_URL_CONFIG_KEY): neo4j_endpoint, 'extractor.search_data.extractor.neo4j.{}'.format(Neo4jExtractor.MODEL_CLASS_CONFIG_KEY): 'databuilder.models.table_elasticsearch_document.TableESDocument', 'extractor.search_data.extractor.neo4j.{}'.format(Neo4jExtractor.NEO4J_AUTH_USER): neo4j_user, 'extractor.search_data.extractor.neo4j.{}'.format(Neo4jExtractor.NEO4J_AUTH_PW): neo4j_password, 'loader.filesystem.elasticsearch.{}'.format(FSElasticsearchJSONLoader.FILE_PATH_CONFIG_KEY): data_file_path, 'loader.filesystem.elasticsearch.{}'.format(FSElasticsearchJSONLoader.FILE_MODE_CONFIG_KEY): 'w', 'publisher.elasticsearch.{}'.format(ElasticsearchPublisher.FILE_PATH_CONFIG_KEY): data_file_path, 'publisher.elasticsearch.{}'.format(ElasticsearchPublisher.FILE_MODE_CONFIG_KEY): 'r', 'publisher.elasticsearch.{}'.format(ElasticsearchPublisher.ELASTICSEARCH_CLIENT_CONFIG_KEY): elasticsearch_client, 'publisher.elasticsearch.{}'.format(ElasticsearchPublisher.ELASTICSEARCH_NEW_INDEX_CONFIG_KEY): elasticsearch_new_index, 'publisher.elasticsearch.{}'.format(ElasticsearchPublisher.ELASTICSEARCH_DOC_TYPE_CONFIG_KEY): elasticsearch_doc_type, 'publisher.elasticsearch.{}'.format(ElasticsearchPublisher.ELASTICSEARCH_ALIAS_CONFIG_KEY): elasticsearch_index_alias, }) job = DefaultJob(conf=job_config, task=DefaultTask(extractor=Neo4jSearchDataExtractor(), loader=FSElasticsearchJSONLoader()), publisher=ElasticsearchPublisher()) if neo4j_password: job.launch() else: raise ValueError('Add environment variable CREDENTIALS_NEO4J_PASSWORD') Standup Search service \u00b6 Follow this instruction Test the search API with this command: 1 curl -vv \"http://localhost:5001/search?query_term=test&page_index=0\"","title":"Overview"},{"location":"developer_guide/#developer-guide","text":"This repository uses git submodules to link the code for all of Amundsen\u2019s libraries into a central location. This document offers guidance on how to develop locally with this setup. This workflow leverages docker and docker-compose in a very similar manner to our installation documentation , to spin up instances of all 3 of Amundsen\u2019s services connected with an instances of Neo4j and ElasticSearch which ingest dummy data.","title":"Developer Guide"},{"location":"developer_guide/#cloning-the-repository","text":"If cloning the repository for the first time, run the following command to clone the repository and pull the submodules: $ git clone --recursive git@github.com:amundsen-io/amundsen.git If you have already cloned the repository but your submodules are empty, from your cloned amundsen directory run: $ git submodule init $ git submodule update After cloning the repository you can change directories into any of the upstream folders and work in those directories as you normally would. You will have full access to all of the git features, and working in the upstream directories will function the same as if you were working in a cloned version of that repository.","title":"Cloning the Repository"},{"location":"developer_guide/#local-development","text":"","title":"Local Development"},{"location":"developer_guide/#ensure-you-have-the-latest-code","text":"Beyond running git pull origin master in your local amundsen directory, the submodules for our libraries also have to be manually updated to point to the latest versions of each libraries\u2019 code. When creating a new branch on amundsen to begin local work, ensure your local submodules are pointing to the latest code for each library by running: $ git submodule update --remote","title":"Ensure you have the latest code"},{"location":"developer_guide/#building-local-changes","text":"First, be sure that you have first followed the installation documentation and can spin up a default version of Amundsen without any issues. If you have already completed this step, be sure to have stopped and removed those containers by running: $ docker-compose -f docker-amundsen.yml down Launch the containers needed for local development (the -d option launches in background) : $ docker-compose -f docker-amundsen-local.yml up -d After making local changes rebuild and relaunch modified containers: $ docker-compose -f docker-amundsen-local.yml build \\ && docker-compose -f docker-amundsen-local.yml up -d Optionally, to still tail logs, in a different terminal you can: $ docker-compose -f docker-amundsen-local.yml logs --tail = 3 -f ## - or just tail single container(s): $ docker logs amundsenmetadata --tail 10 -f","title":"Building local changes"},{"location":"developer_guide/#local-data","text":"Local data is persisted under .local/ (at the root of the project), clean up the following directories to reset the databases: # reset elasticsearch rm -rf .local/elasticsearch # reset neo4j rm -rf .local/neo4j","title":"Local data"},{"location":"developer_guide/#troubleshooting","text":"If you have made a change in amundsen/amundsenfrontendlibrary and do not see your changes, this could be due to your browser\u2019s caching behaviors. Either execute a hard refresh (recommended) or clear your browser cache (last resort).","title":"Troubleshooting"},{"location":"developer_guide/#testing-amundsen-frontend-locally","text":"Amundsen has an instruction regarding local frontend launch here Here are some additional changes you might need for windows (OS Win 10): amundsen_application/config.py, set LOCAL_HOST = \u2018127.0.0.1\u2019 amundsen_application/wsgi.py, set host=\u2018127.0.0.1\u2019 (for other microservices also need to change port here because the default is 5000) (using that approach you can run locally another microservices as well if needed) Once you have a running frontend microservice, the rest of Amundsen components can be launched with docker-compose from the root Amundsen project (don\u2019t forget to remove frontend microservice section from docker-amundsen.yml): docker-compose -f docker-amundsen.yml up https://github.com/amundsen-io/amundsen/blob/master/docs/installation.md","title":"Testing Amundsen frontend locally"},{"location":"developer_guide/#developing-dockerbuild-file","text":"When making edits to Dockerbuild file (docker-amundsen-local.yml) it is good to see what you are getting wrong locally. To do that you build it docker build . And then the output should include a line like so at the step right before it failed: Step 3 /20 : RUN git clone --recursive git://github.com/amundsen-io/amundsenfrontendlibrary.git && cd amundsenfrontendlibrary && git submodule foreach git pull origin master ---> Using cache ---> ec052612747e You can then launch a container from this image like so docker container run -it --name = debug ec052612747e /bin/sh","title":"Developing Dockerbuild file"},{"location":"developer_guide/#building-and-testing-amundsen-frontend-docker-image-or-any-other-service","text":"Build your image docker build --no-cache . it is recommended that you use \u2013no-cache so you aren\u2019t accidentally using an old version of an image. Determine the hash of your images by running docker images and getting the id of your most recent image Go to your locally cloned amundsen repo and edit the docker compose file \u201cdocker-amundsen.yml\u201d to have the amundsenfrontend image point to the hash of the image that you built amundsenfrontend : #image: amundsendev/amundsen-frontend:1.0.9 #image: 1234.dkr.ecr.us-west-2.amazonaws.com/edmunds/amundsen-frontend:2020-01-21 image : 0312d0ac3938","title":"Building and Testing Amundsen Frontend Docker Image (or any other service)"},{"location":"developer_guide/#pushing-image-to-ecr-and-using-in-k8s","text":"Assumptions: You have an aws account You have aws command line set up and ready to go Choose an ECR repository you\u2019d like to push to (or create a new one) https://us-west-2.console.aws.amazon.com/ecr/repositories Click onto repository name and open \u201cView push commands\u201d cheat sheet 2b. Login it would look something like this: aws ecr get-login --no-include-email --region us-west-2 Then execute what is returned by above Follow the instructions (you may need to install first AWS CLI, aws-okta and configure your AWS credentials if you haven\u2019t done it before) Given image name is amundsen-frontend, build, tag and push commands will be the following: Here you can see the tag is YYYY-MM-dd but you should choose whatever you like. docker build -t amundsen-frontend:{YYYY-MM-dd} . docker tag amundsen-frontend:{YYYY-MM-dd} <?>.dkr.ecr.<?>.amazonaws.com/amundsen-frontend:{YYYY-MM-dd} docker push <?>.dkr.ecr.<?>.amazonaws.com/amundsen-frontend:{YYYY-MM-dd} Go to the helm/{env}/amundsen/values.yaml and modify to the image tag that you want to use. When updating amundsen-frontend, make sure to do a hard refresh of amundsen with emptying the cache, otherwise you will see stale version of webpage.","title":"Pushing image to ECR and using in K8s"},{"location":"developer_guide/#test-search-service-in-local-using-staging-or-production-data","text":"To test in local, we need to stand up Elasticsearch, publish index data, and stand up Elastic search","title":"Test search service in local using staging or production data"},{"location":"developer_guide/#standup-elasticsearch","text":"Running Elasticsearch via Docker. To install Docker, go here Example: 1 docker run -p 9200:9200 -p 9300:9300 -e \"discovery.type=single-node\" docker.elastic.co/elasticsearch/elasticsearch:6.2.4","title":"Standup Elasticsearch"},{"location":"developer_guide/#optional-standup-kibana","text":"1 docker run --link ecstatic_edison:elasticsearch -p 5601:5601 docker.elastic.co/kibana/kibana:6.2.4 *Note that ecstatic_edison is container_id of Elasticsearch container. Update it if it\u2019s different by looking at docker ps","title":"(Optional) Standup Kibana"},{"location":"developer_guide/#publish-table-index-through-databuilder","text":"","title":"Publish Table index through Databuilder"},{"location":"developer_guide/#install-databuilder","text":"1 2 3 4 5 6 7 cd ~/src/ git clone git@github.com:amundsen-io/amundsendatabuilder.git cd ~/src/amundsendatabuilder virtualenv venv source venv/bin/activate python setup.py install pip install -r requirements.txt","title":"Install Databuilder"},{"location":"developer_guide/#publish-table-index","text":"First fill this two environment variables: NEO4J_ENDPOINT , CREDENTIALS_NEO4J_PASSWORD 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 $ python import logging import os import uuid from elasticsearch import Elasticsearch from pyhocon import ConfigFactory from databuilder.extractor.neo4j_extractor import Neo4jExtractor from databuilder.extractor.neo4j_search_data_extractor import Neo4jSearchDataExtractor from databuilder.job.job import DefaultJob from databuilder.loader.file_system_elasticsearch_json_loader import FSElasticsearchJSONLoader from databuilder.publisher.elasticsearch_publisher import ElasticsearchPublisher from databuilder.task.task import DefaultTask logging.basicConfig(level=logging.INFO) neo4j_user = 'neo4j' neo4j_password = os.getenv('CREDENTIALS_NEO4J_PASSWORD') neo4j_endpoint = os.getenv('NEO4J_ENDPOINT') elasticsearch_client = Elasticsearch([ {'host': 'localhost'}, ]) data_file_path = '/var/tmp/amundsen/elasticsearch_upload/es_data.json' elasticsearch_new_index = 'table_search_index_{hex_str}'.format(hex_str=uuid.uuid4().hex) logging.info(\"Elasticsearch new index: \" + elasticsearch_new_index) elasticsearch_doc_type = 'table' elasticsearch_index_alias = 'table_search_index' job_config = ConfigFactory.from_dict({ 'extractor.search_data.extractor.neo4j.{}'.format(Neo4jExtractor.GRAPH_URL_CONFIG_KEY): neo4j_endpoint, 'extractor.search_data.extractor.neo4j.{}'.format(Neo4jExtractor.MODEL_CLASS_CONFIG_KEY): 'databuilder.models.table_elasticsearch_document.TableESDocument', 'extractor.search_data.extractor.neo4j.{}'.format(Neo4jExtractor.NEO4J_AUTH_USER): neo4j_user, 'extractor.search_data.extractor.neo4j.{}'.format(Neo4jExtractor.NEO4J_AUTH_PW): neo4j_password, 'loader.filesystem.elasticsearch.{}'.format(FSElasticsearchJSONLoader.FILE_PATH_CONFIG_KEY): data_file_path, 'loader.filesystem.elasticsearch.{}'.format(FSElasticsearchJSONLoader.FILE_MODE_CONFIG_KEY): 'w', 'publisher.elasticsearch.{}'.format(ElasticsearchPublisher.FILE_PATH_CONFIG_KEY): data_file_path, 'publisher.elasticsearch.{}'.format(ElasticsearchPublisher.FILE_MODE_CONFIG_KEY): 'r', 'publisher.elasticsearch.{}'.format(ElasticsearchPublisher.ELASTICSEARCH_CLIENT_CONFIG_KEY): elasticsearch_client, 'publisher.elasticsearch.{}'.format(ElasticsearchPublisher.ELASTICSEARCH_NEW_INDEX_CONFIG_KEY): elasticsearch_new_index, 'publisher.elasticsearch.{}'.format(ElasticsearchPublisher.ELASTICSEARCH_DOC_TYPE_CONFIG_KEY): elasticsearch_doc_type, 'publisher.elasticsearch.{}'.format(ElasticsearchPublisher.ELASTICSEARCH_ALIAS_CONFIG_KEY): elasticsearch_index_alias, }) job = DefaultJob(conf=job_config, task=DefaultTask(extractor=Neo4jSearchDataExtractor(), loader=FSElasticsearchJSONLoader()), publisher=ElasticsearchPublisher()) if neo4j_password: job.launch() else: raise ValueError('Add environment variable CREDENTIALS_NEO4J_PASSWORD')","title":"Publish Table index"},{"location":"developer_guide/#standup-search-service","text":"Follow this instruction Test the search API with this command: 1 curl -vv \"http://localhost:5001/search?query_term=test&page_index=0\"","title":"Standup Search service"},{"location":"faq/","text":"FAQ \u00b6 How to select between Neo4j and Atlas as backend for Amundsen? \u00b6 Why Neo4j? \u00b6 Amundsen has direct influence over the data model if you use neo4j. This, at least initially, will benefit the speed by which new features in amundsen can arrive. Neo4j for it is the market leader in Graph database and also was proven by Airbnb\u2019s Data portal on their Data discovery tool. Why Atlas? \u00b6 Atlas has lineage support already available. It\u2019s been tried and tested. Tag/Badge propagation is supported. It has a robust authentication and authorization system. Atlas does data governance adding amundsen for discovery makes it best of both worlds. It has support for push based due to its many plugins. The free version of Neo4j does not have authorization support (Enterprise version does). Your question should actually be why use \u201cneo4j over janusgraph\u201d cause that is the right level of comparison. Atlas adds a whole bunch on top of the graph database. Why not Atlas? \u00b6 Atlas is developed with data governance in mind and not with data discovery. Atlas seems to have a slow development cycle and it\u2019s community is not very responsive although some small improvements have been made. Amundsen databuilder integration is not yet supported which puts more strain on the end user to populate the required entities. What are the prerequisites to use Apache Atlas as backend for Amundsen? \u00b6 To run Amundsen with Atlas, latest versions of following components should be used: 1. Apache Atlas - built from master branch. Ref 103e867cc126ddb84e64bf262791a01a55bee6e5 (or higher). 2. amundsenatlastypes - library for installing Atlas entity definitions specific to Amundsen integration. Version 1.1.0 (or higher). How to migrate from Amundsen 1.x -> 2.x? \u00b6 v2.0 renames a handful of fields in the services to be more consistent. Unfortunately one side effect is that the 2.0 versions of the services will need to be deployed simultaneously, as they are not interoperable with the 1.x versions. Additionally, some indexed field names in the elasticsearch document change as well, so if you\u2019re using elasticsearch, you\u2019ll need to republish Elasticsearch index via Databuilder job. The data in the metadata store, however, can be preserved when migrating from 1.x to 2.0. v2.0 deployments consists of deployment of all three services along with republishing Elasticsearch document on Table with v2.0 Databuilder. Keep in mind there is likely to be some downtime as v2.0.0, between deploying 3 services and re-seeding the elasticsearch indexes, so it might be ideal to stage a rollout by datacenter/environment if uptime is key How to avoid certain metadatas in Amundsen got erased by databuilder ingestion? \u00b6 By default, databuilder always upserts the metadata. If you want to prevent that happens on certain type of metadata, you could add the following config to your databuilder job\u2019s config 'publisher.neo4j. {} ' . format ( neo4j_csv_publisher . NEO4J_CREATE_ONLY_NODES ): [ DESCRIPTION_NODE_LABEL ], This config means that databuilder will only update the table / column description if it doesn\u2019t exist before which could be the table is newly created. This is useful when we treat Amundsen graph as the source of truth for certain types of metadata (e.g description). How to capture all Google Analytics? \u00b6 Users are likely to have some sort of adblocker installed, making your Google Analytics less accurate. To put a proxy in place to bypass any adblockers and capture all analytics, follow these steps: Follow https://github.com/ZitRos/save-analytics-from-content-blockers#setup to set up your own proxy server. In the same repository, run npm run mask www.googletagmanager.com/gtag/js?id=UA-XXXXXXXXX and save the output. In your custom frontend, override https://github.com/amundsen-io/amundsenfrontendlibrary/blob/master/amundsen_application/static/templates/fragments/google-analytics-loader.html#L6 to Now, note that network requests to www.googletagmanager.com will be sent from behind your masked proxy endpoint, saving your analytics from content blockers!","title":"FAQ"},{"location":"faq/#faq","text":"","title":"FAQ"},{"location":"faq/#how-to-select-between-neo4j-and-atlas-as-backend-for-amundsen","text":"","title":"How to select between Neo4j and Atlas as backend for Amundsen?"},{"location":"faq/#why-neo4j","text":"Amundsen has direct influence over the data model if you use neo4j. This, at least initially, will benefit the speed by which new features in amundsen can arrive. Neo4j for it is the market leader in Graph database and also was proven by Airbnb\u2019s Data portal on their Data discovery tool.","title":"Why Neo4j?"},{"location":"faq/#why-atlas","text":"Atlas has lineage support already available. It\u2019s been tried and tested. Tag/Badge propagation is supported. It has a robust authentication and authorization system. Atlas does data governance adding amundsen for discovery makes it best of both worlds. It has support for push based due to its many plugins. The free version of Neo4j does not have authorization support (Enterprise version does). Your question should actually be why use \u201cneo4j over janusgraph\u201d cause that is the right level of comparison. Atlas adds a whole bunch on top of the graph database.","title":"Why Atlas?"},{"location":"faq/#why-not-atlas","text":"Atlas is developed with data governance in mind and not with data discovery. Atlas seems to have a slow development cycle and it\u2019s community is not very responsive although some small improvements have been made. Amundsen databuilder integration is not yet supported which puts more strain on the end user to populate the required entities.","title":"Why not Atlas?"},{"location":"faq/#what-are-the-prerequisites-to-use-apache-atlas-as-backend-for-amundsen","text":"To run Amundsen with Atlas, latest versions of following components should be used: 1. Apache Atlas - built from master branch. Ref 103e867cc126ddb84e64bf262791a01a55bee6e5 (or higher). 2. amundsenatlastypes - library for installing Atlas entity definitions specific to Amundsen integration. Version 1.1.0 (or higher).","title":"What are the prerequisites to use Apache Atlas as backend for Amundsen?"},{"location":"faq/#how-to-migrate-from-amundsen-1x-2x","text":"v2.0 renames a handful of fields in the services to be more consistent. Unfortunately one side effect is that the 2.0 versions of the services will need to be deployed simultaneously, as they are not interoperable with the 1.x versions. Additionally, some indexed field names in the elasticsearch document change as well, so if you\u2019re using elasticsearch, you\u2019ll need to republish Elasticsearch index via Databuilder job. The data in the metadata store, however, can be preserved when migrating from 1.x to 2.0. v2.0 deployments consists of deployment of all three services along with republishing Elasticsearch document on Table with v2.0 Databuilder. Keep in mind there is likely to be some downtime as v2.0.0, between deploying 3 services and re-seeding the elasticsearch indexes, so it might be ideal to stage a rollout by datacenter/environment if uptime is key","title":"How to migrate from Amundsen 1.x -&gt; 2.x?"},{"location":"faq/#how-to-avoid-certain-metadatas-in-amundsen-got-erased-by-databuilder-ingestion","text":"By default, databuilder always upserts the metadata. If you want to prevent that happens on certain type of metadata, you could add the following config to your databuilder job\u2019s config 'publisher.neo4j. {} ' . format ( neo4j_csv_publisher . NEO4J_CREATE_ONLY_NODES ): [ DESCRIPTION_NODE_LABEL ], This config means that databuilder will only update the table / column description if it doesn\u2019t exist before which could be the table is newly created. This is useful when we treat Amundsen graph as the source of truth for certain types of metadata (e.g description).","title":"How to avoid certain metadatas in Amundsen got erased by databuilder ingestion?"},{"location":"faq/#how-to-capture-all-google-analytics","text":"Users are likely to have some sort of adblocker installed, making your Google Analytics less accurate. To put a proxy in place to bypass any adblockers and capture all analytics, follow these steps: Follow https://github.com/ZitRos/save-analytics-from-content-blockers#setup to set up your own proxy server. In the same repository, run npm run mask www.googletagmanager.com/gtag/js?id=UA-XXXXXXXXX and save the output. In your custom frontend, override https://github.com/amundsen-io/amundsenfrontendlibrary/blob/master/amundsen_application/static/templates/fragments/google-analytics-loader.html#L6 to Now, note that network requests to www.googletagmanager.com will be sent from behind your masked proxy endpoint, saving your analytics from content blockers!","title":"How to capture all Google Analytics?"},{"location":"installation/","text":"Installation \u00b6 Bootstrap a default version of Amundsen using Docker \u00b6 The following instructions are for setting up a version of Amundsen using Docker. Make sure you have at least 3GB available to docker. Install docker and docker-compose . Clone this repo and its submodules by running: $ git clone --recursive git@github.com:amundsen-io/amundsen.git Enter the cloned directory and run below: # For Neo4j Backend $ docker-compose -f docker-amundsen.yml up # For Atlas $ docker-compose -f docker-amundsen-atlas.yml up If it\u2019s your first time, you may want to proactively go through troubleshooting steps, especially the first one related to heap memory for ElasticSearch and Docker engine memory allocation (leading to Docker error 137). Ingest provided sample data into Neo4j by doing the following: (Please skip if you are using Atlas backend) In a separate terminal window, change directory to databuilder . sample_data_loader python script included in examples/ directory uses elasticsearch client , pyhocon and other libraries. Install the dependencies in a virtual env and run the script by following the commands below: $ python3 -m venv venv $ source venv/bin/activate $ pip3 install --upgrade pip $ pip3 install -r requirements.txt $ python3 setup.py install $ python3 example/scripts/sample_data_loader.py View UI at http://localhost:5000 and try to search test , it should return some result. We could also do an exact matched search for table entity. For example: search test_table1 in table field and it return the records that matched. Atlas Note: Atlas takes some time to boot properly. So you may not be able to see the results immediately after docker-compose up command. Atlas would be ready once you\u2019ll have the following output in the docker output Amundsen Entity Definitions Created... Verify setup \u00b6 You can verify dummy data has been ingested into Neo4j by by visiting http://localhost:7474/browser/ and run MATCH (n:Table) RETURN n LIMIT 25 in the query box. You should see two tables: hive.test_schema.test_table1 hive.test_schema.test_table2 You can verify the data has been loaded into the metadataservice by visiting: http://localhost:5000/table_detail/gold/hive/test_schema/test_table1 http://localhost:5000/table_detail/gold/dynamo/test_schema/test_table2 Troubleshooting \u00b6 If the docker container doesn\u2019t have enough heap memory for Elastic Search, es_amundsen will fail during docker-compose . docker-compose error: es_amundsen | [1]: max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144] Increase the heap memory detailed instructions here Edit /etc/sysctl.conf Make entry vm.max_map_count=262144 . Save and exit. Reload settings $ sysctl -p Restart docker-compose If docker-amundsen-local.yml stops because of org.elasticsearch.bootstrap.StartupException: java.lang.IllegalStateException: Failed to create node environment , then es_amundsen cannot write to .local/elasticsearch . chown -R 1000:1000 .local/elasticsearch Restart docker-compose If when running the sample data loader you recieve a connection error related to ElasticSearch or like this for Neo4j: Traceback (most recent call last): File \"/home/ubuntu/amundsen/amundsendatabuilder/venv/lib/python3.6/site-packages/neobolt/direct.py\", line 831, in _connect s.connect(resolved_address) ConnectionRefusedError: [Errno 111] Connection refused If elastic search container stops with an error max file descriptors [4096] for elasticsearch process is too low, increase to at least [65535] , then add the below code to the file docker-amundsen-local.yml in the elasticsearch definition. ulimits: nofile: soft: 65535 hard: 65535 Then check if all 5 Amundsen related containers are running with docker ps ? Can you connect to the Neo4j UI at http://localhost:7474/browser/ and similarly the raw ES API at http://localhost:9200 ? Does Docker logs reveal any serious issues? If ES container crashed with Docker error 137 on the first call from the website ( http://localhost:5000/ ), this is because you are using the default Docker engine memory allocation of 2GB. The minimum needed for all the containers to run with the loaded sample data is 3GB. To do this go to your Docker -> Preferences -> Resources -> Advanced and increase the Memory , then restart the Docker engine.","title":"Quick Start"},{"location":"installation/#installation","text":"","title":"Installation"},{"location":"installation/#bootstrap-a-default-version-of-amundsen-using-docker","text":"The following instructions are for setting up a version of Amundsen using Docker. Make sure you have at least 3GB available to docker. Install docker and docker-compose . Clone this repo and its submodules by running: $ git clone --recursive git@github.com:amundsen-io/amundsen.git Enter the cloned directory and run below: # For Neo4j Backend $ docker-compose -f docker-amundsen.yml up # For Atlas $ docker-compose -f docker-amundsen-atlas.yml up If it\u2019s your first time, you may want to proactively go through troubleshooting steps, especially the first one related to heap memory for ElasticSearch and Docker engine memory allocation (leading to Docker error 137). Ingest provided sample data into Neo4j by doing the following: (Please skip if you are using Atlas backend) In a separate terminal window, change directory to databuilder . sample_data_loader python script included in examples/ directory uses elasticsearch client , pyhocon and other libraries. Install the dependencies in a virtual env and run the script by following the commands below: $ python3 -m venv venv $ source venv/bin/activate $ pip3 install --upgrade pip $ pip3 install -r requirements.txt $ python3 setup.py install $ python3 example/scripts/sample_data_loader.py View UI at http://localhost:5000 and try to search test , it should return some result. We could also do an exact matched search for table entity. For example: search test_table1 in table field and it return the records that matched. Atlas Note: Atlas takes some time to boot properly. So you may not be able to see the results immediately after docker-compose up command. Atlas would be ready once you\u2019ll have the following output in the docker output Amundsen Entity Definitions Created...","title":"Bootstrap a default version of Amundsen using Docker"},{"location":"installation/#verify-setup","text":"You can verify dummy data has been ingested into Neo4j by by visiting http://localhost:7474/browser/ and run MATCH (n:Table) RETURN n LIMIT 25 in the query box. You should see two tables: hive.test_schema.test_table1 hive.test_schema.test_table2 You can verify the data has been loaded into the metadataservice by visiting: http://localhost:5000/table_detail/gold/hive/test_schema/test_table1 http://localhost:5000/table_detail/gold/dynamo/test_schema/test_table2","title":"Verify setup"},{"location":"installation/#troubleshooting","text":"If the docker container doesn\u2019t have enough heap memory for Elastic Search, es_amundsen will fail during docker-compose . docker-compose error: es_amundsen | [1]: max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144] Increase the heap memory detailed instructions here Edit /etc/sysctl.conf Make entry vm.max_map_count=262144 . Save and exit. Reload settings $ sysctl -p Restart docker-compose If docker-amundsen-local.yml stops because of org.elasticsearch.bootstrap.StartupException: java.lang.IllegalStateException: Failed to create node environment , then es_amundsen cannot write to .local/elasticsearch . chown -R 1000:1000 .local/elasticsearch Restart docker-compose If when running the sample data loader you recieve a connection error related to ElasticSearch or like this for Neo4j: Traceback (most recent call last): File \"/home/ubuntu/amundsen/amundsendatabuilder/venv/lib/python3.6/site-packages/neobolt/direct.py\", line 831, in _connect s.connect(resolved_address) ConnectionRefusedError: [Errno 111] Connection refused If elastic search container stops with an error max file descriptors [4096] for elasticsearch process is too low, increase to at least [65535] , then add the below code to the file docker-amundsen-local.yml in the elasticsearch definition. ulimits: nofile: soft: 65535 hard: 65535 Then check if all 5 Amundsen related containers are running with docker ps ? Can you connect to the Neo4j UI at http://localhost:7474/browser/ and similarly the raw ES API at http://localhost:9200 ? Does Docker logs reveal any serious issues? If ES container crashed with Docker error 137 on the first call from the website ( http://localhost:5000/ ), this is because you are using the default Docker engine memory allocation of 2GB. The minimum needed for all the containers to run with the loaded sample data is 3GB. To do this go to your Docker -> Preferences -> Resources -> Advanced and increase the Memory , then restart the Docker engine.","title":"Troubleshooting"},{"location":"issue_labeling/","text":"Issue and Feature Labeling \u00b6 On Amundsen, we aim to be methodical on using issue labels, offering our community a way to understand what are the issues about and their status within or development process. We use a bunch of GitHub labels. They are a mix of custom labels and the default Github labels for open-source projects. We base these labels on four main types: status labels , issue type labels , project labels , and the \u201cother\u201d category . Read on to learn more about them. Status Labels \u00b6 They show at a glance the status and progress of each issue Prefixed with \u201cStatus:\u201d, followed by the label Only one status label will be applied to any particular issue Labels \u00b6 Status: Needs Reproducing \u2013 For bugs that need to be reproduced in order to get fixed Status: Review Needed \u2013 Issue that needs review to be considered Status: Accepted \u2013 Feature that we want to implement going forward Status: In Progress \u2013 Issue that is being worked on right now. Status: Completed \u2013 Issue is completed and on master Status: Abandoned \u2013 Issue we won\u2019t go ahead and implement, or that needs a \u201cchampion\u201d to take it through Status: Blocked \u2013 Issue blocked by any reason (dependencies, previous work, etc.) Status: On Hold \u2013 Issue that is being considered but stopped due to lack of resources or changes in the roadmap Here is a diagram representing these states within the lifecycles: Type Labels \u00b6 They show the type of the issue Prefixed with \u201cType:\u201d, followed by the label Labels \u00b6 Type: Bug \u2013 An unexpected problem or unintended behavior Type: Feature \u2013 A new feature request Type: Maintenance \u2013 A regular maintenance chore or task, including refactors, build system, CI, performance improvements Type: Documentation \u2013 A documentation improvement task Type: Question \u2013 An issue or PR that needs more information or a user question Project Labels \u00b6 They indicate which project the issue refers to Prefixed with \u201cProject:\u201d, followed by the name of the project Labels \u00b6 Project: Common \u2013 From amundsencommon Project: Databuilder \u2013 From amundsendatabuilder Project: Frontend \u2013 From amundsenfrontendlibrary Project: Metadata \u2013 From amundsenmetadatalibrary Project: Search \u2013 From amundsensearchlibrary Project: k8s \u2013 Related to the Kubernetes helm chart Project: All \u2013 Related to all the projects above Other Labels \u00b6 Some of these are part of the standard GitHub labels and intended for OSS contributors Some are related to the tools we use to maintain the library They are not prefixed Labels \u00b6 help wanted \u2013 Indicates we are looking for contributors on this issue good first issue \u2013 Indicates the issue is a great one to tackle by newcomers to the project or OSS in general keep fresh \u2013 Avoids getting the issue archived by our stale bot","title":"Issue and Feature Labeling"},{"location":"issue_labeling/#issue-and-feature-labeling","text":"On Amundsen, we aim to be methodical on using issue labels, offering our community a way to understand what are the issues about and their status within or development process. We use a bunch of GitHub labels. They are a mix of custom labels and the default Github labels for open-source projects. We base these labels on four main types: status labels , issue type labels , project labels , and the \u201cother\u201d category . Read on to learn more about them.","title":"Issue and Feature Labeling"},{"location":"issue_labeling/#status-labels","text":"They show at a glance the status and progress of each issue Prefixed with \u201cStatus:\u201d, followed by the label Only one status label will be applied to any particular issue","title":"Status Labels"},{"location":"issue_labeling/#labels","text":"Status: Needs Reproducing \u2013 For bugs that need to be reproduced in order to get fixed Status: Review Needed \u2013 Issue that needs review to be considered Status: Accepted \u2013 Feature that we want to implement going forward Status: In Progress \u2013 Issue that is being worked on right now. Status: Completed \u2013 Issue is completed and on master Status: Abandoned \u2013 Issue we won\u2019t go ahead and implement, or that needs a \u201cchampion\u201d to take it through Status: Blocked \u2013 Issue blocked by any reason (dependencies, previous work, etc.) Status: On Hold \u2013 Issue that is being considered but stopped due to lack of resources or changes in the roadmap Here is a diagram representing these states within the lifecycles:","title":"Labels"},{"location":"issue_labeling/#type-labels","text":"They show the type of the issue Prefixed with \u201cType:\u201d, followed by the label","title":"Type Labels"},{"location":"issue_labeling/#labels_1","text":"Type: Bug \u2013 An unexpected problem or unintended behavior Type: Feature \u2013 A new feature request Type: Maintenance \u2013 A regular maintenance chore or task, including refactors, build system, CI, performance improvements Type: Documentation \u2013 A documentation improvement task Type: Question \u2013 An issue or PR that needs more information or a user question","title":"Labels"},{"location":"issue_labeling/#project-labels","text":"They indicate which project the issue refers to Prefixed with \u201cProject:\u201d, followed by the name of the project","title":"Project Labels"},{"location":"issue_labeling/#labels_2","text":"Project: Common \u2013 From amundsencommon Project: Databuilder \u2013 From amundsendatabuilder Project: Frontend \u2013 From amundsenfrontendlibrary Project: Metadata \u2013 From amundsenmetadatalibrary Project: Search \u2013 From amundsensearchlibrary Project: k8s \u2013 Related to the Kubernetes helm chart Project: All \u2013 Related to all the projects above","title":"Labels"},{"location":"issue_labeling/#other-labels","text":"Some of these are part of the standard GitHub labels and intended for OSS contributors Some are related to the tools we use to maintain the library They are not prefixed","title":"Other Labels"},{"location":"issue_labeling/#labels_3","text":"help wanted \u2013 Indicates we are looking for contributors on this issue good first issue \u2013 Indicates the issue is a great one to tackle by newcomers to the project or OSS in general keep fresh \u2013 Avoids getting the issue archived by our stale bot","title":"Labels"},{"location":"k8s_install/","text":"Amundsen K8s Helm Charts \u00b6 Source code can be found here What is this? \u00b6 This is setup templates for deploying amundsen on k8s (kubernetes) , using helm. How do I get started? \u00b6 Make sure you have the following command line clients setup: k8s (kubectl) helm Build out a cloud based k8s cluster, such as Amazon EKS Ensure you can connect to your cluster with cli tools in step 1. Prerequisites \u00b6 Helm 2.14+ Kubernetes 1.14+ Chart Requirements \u00b6 Repository Name Version https://charts.helm.sh/stable elasticsearch 1.32.0 Chart Values \u00b6 Key Type Default Description LONG_RANDOM_STRING int 1234 A long random string. You should probably provide your own. This is needed for OIDC. affinity object {} amundsen application wide configuration of affinity. This applies to search, metadata, frontend and neo4j. Elasticsearch has it\u2019s own configuation properties for this. ref dnsZone string \"teamname.company.com\" DEPRECATED - its not standard to pre construct urls this way. The dns zone (e.g. group-qa.myaccount.company.com) the app is running in. Used to construct dns hostnames (on aws only). dockerhubImagePath string \"amundsendev\" DEPRECATED - this is not useful, it would be better to just allow the whole image to be swapped instead. The image path for dockerhub. elasticsearch.client.replicas int 1 only running amundsen on 1 client replica elasticsearch.cluster.env.EXPECTED_MASTER_NODES int 1 required to match master.replicas elasticsearch.cluster.env.MINIMUM_MASTER_NODES int 1 required to match master.replicas elasticsearch.cluster.env.RECOVER_AFTER_MASTER_NODES int 1 required to match master.replicas elasticsearch.data.replicas int 1 only running amundsen on 1 data replica elasticsearch.enabled bool true set this to false, if you want to provide your own ES instance. elasticsearch.master.replicas int 1 only running amundsen on 1 master replica environment string \"dev\" DEPRECATED - its not standard to pre construct urls this way. The environment the app is running in. Used to construct dns hostnames (on aws only) and ports. frontEnd.OIDC_AUTH_SERVER_ID string nil The authorization server id for OIDC. frontEnd.OIDC_CLIENT_ID string nil The client id for OIDC. frontEnd.OIDC_CLIENT_SECRET string \"\" The client secret for OIDC. frontEnd.OIDC_ORG_URL string nil The organization URL for OIDC. frontEnd.OVERWRITE_REDIRECT_URI string nil The redirect uri for OIDC. frontEnd.affinity object {} Frontend pod specific affinity. frontEnd.annotations object {} Frontend service specific tolerations. frontEnd.baseUrl string \"http://localhost\" used by notifications util to provide links to amundsen pages in emails. frontEnd.createOidcSecret bool false OIDC needs some configuration. If you want the chart to make your secrets, set this to true and set the next four values. If you don\u2019t want to configure your secrets via helm, you can still use the amundsen-oidc-config.yaml as a template frontEnd.image string \"amundsendev/amundsen-frontend\" The image of the frontend container. frontEnd.imagePullSecrets list [] Optional pod imagePullSecrets ref frontEnd.imageTag string \"2.3.0\" The image tag of the frontend container. frontEnd.nodeSelector object {} Frontend pod specific nodeSelector. frontEnd.oidcEnabled bool false To enable auth via OIDC, set this to true. frontEnd.podAnnotations object {} Frontend pod specific annotations. frontEnd.replicas int 1 How many replicas of the frontend service to run. frontEnd.resources object {} See pod resourcing ref frontEnd.serviceName string \"frontend\" The frontend service name. frontEnd.servicePort int 80 The port the frontend service will be exposed on via the loadbalancer. frontEnd.serviceType string \"ClusterIP\" The frontend service type. See service types ref frontEnd.tolerations list [] Frontend pod specific tolerations. metadata.affinity object {} Metadata pod specific affinity. metadata.annotations object {} Metadata service specific tolerations. metadata.image string \"amundsendev/amundsen-metadata\" The image of the metadata container. metadata.imagePullSecrets list [] Optional pod imagePullSecrets ref metadata.imageTag string \"2.5.5\" The image tag of the metadata container. metadata.neo4jEndpoint string nil The name of the service hosting neo4j on your cluster, if you bring your own. You should only need to change this, if you don\u2019t use the version in this chart. metadata.nodeSelector object {} Metadata pod specific nodeSelector. metadata.podAnnotations object {} Metadata pod specific annotations. metadata.replicas int 1 How many replicas of the metadata service to run. metadata.resources object {} See pod resourcing ref metadata.serviceName string \"metadata\" The metadata service name. metadata.serviceType string \"ClusterIP\" The metadata service type. See service types ref metadata.tolerations list [] Metadata pod specific tolerations. neo4j.affinity object {} neo4j specific affinity. neo4j.annotations object {} neo4j service specific tolerations. neo4j.backup object {\"enabled\":false,\"podAnnotations\":{},\"s3Path\":\"s3://dev/null\",\"schedule\":\"0 * * * *\"} If enabled is set to true, make sure and set the s3 path as well. neo4j.backup.s3Path string \"s3://dev/null\" The s3path to write to for backups. neo4j.backup.schedule string \"0 * * * *\" The schedule to run backups on. Defaults to hourly. neo4j.config object {\"dbms\":{\"heap_initial_size\":\"1G\",\"heap_max_size\":\"2G\",\"pagecache_size\":\"2G\"}} Neo4j application specific configuration. This type of configuration is why the charts/stable version is not used. See ref neo4j.config.dbms object {\"heap_initial_size\":\"1G\",\"heap_max_size\":\"2G\",\"pagecache_size\":\"2G\"} dbms config for neo4j neo4j.config.dbms.heap_initial_size string \"1G\" the initial java heap for neo4j neo4j.config.dbms.heap_max_size string \"2G\" the max java heap for neo4j neo4j.config.dbms.pagecache_size string \"2G\" the page cache size for neo4j neo4j.enabled bool true If neo4j is enabled as part of this chart, or not. Set this to false if you want to provide your own version. neo4j.nodeSelector object {} neo4j specific nodeSelector. neo4j.persistence object {} Neo4j persistence. Turn this on to keep your data between pod crashes, etc. This is also needed for backups. neo4j.podAnnotations object {} neo4j pod specific annotations. neo4j.resources object {} See pod resourcing ref neo4j.tolerations list [] neo4j specific tolerations. neo4j.version string \"3.3.0\" The neo4j application version used by amundsen. nodeSelector object {} amundsen application wide configuration of nodeSelector. This applies to search, metadata, frontend and neo4j. Elasticsearch has it\u2019s own configuation properties for this. ref podAnnotations object {} amundsen application wide configuration of podAnnotations. This applies to search, metadata, frontend and neo4j. Elasticsearch has it\u2019s own configuation properties for this. ref provider string \"aws\" The cloud provider the app is running in. Used to construct dns hostnames (on aws only). search.affinity object {} Search pod specific affinity. search.annotations object {} Search service specific tolerations. search.elasticSearchCredentials object {} The elasticsearch user and password. This should only be set if you bring your own elasticsearch cluster in which case you must also set elasticsearch.enabled to false search.elasticsearchEndpoint string nil The name of the service hosting elasticsearch on your cluster, if you bring your own. You should only need to change this, if you don\u2019t use the version in this chart. search.image string \"amundsendev/amundsen-search\" The image of the search container. search.imagePullSecrets list [] Optional pod imagePullSecrets ref search.imageTag string \"2.4.0\" The image tag of the search container. search.nodeSelector object {} Search pod specific nodeSelector. search.podAnnotations object {} Search pod specific annotations. search.replicas int 1 How many replicas of the search service to run. search.resources object {} See pod resourcing ref search.serviceName string \"search\" The search service name. search.serviceType string \"ClusterIP\" The search service type. See service types ref search.tolerations list [] Search pod specific tolerations. tolerations list [] amundsen application wide configuration of tolerations. This applies to search, metadata, frontend and neo4j. Elasticsearch has it\u2019s own configuation properties for this. ref Autogenerated from chart metadata using helm-docs v1.5.0 Neo4j DBMS Config? \u00b6 You may want to override the default memory usage for Neo4J. In particular, if you\u2019re just test-driving a deployment and your node exits with status 137, you should set the usage to smaller values: config : dbms : heap_initial_size : 1G heap_max_size : 2G pagecache_size : 2G With this values file, you can then install Amundsen using Helm 2 with: helm install ./templates/helm --values impl/helm/dev/values.yaml For Helm 3 it\u2019s now mandatory to specify a chart reference name e.g. my-amundsen : helm install my-amundsen ./templates/helm --values impl/helm/dev/values.yaml Other Notes \u00b6 For aws setup, you will also need to setup the external-dns plugin There is an existing helm chart for neo4j, but, it is missing some features necessary to for use such as: [stable/neo4j] make neo4j service definition more extensible ; without this, it is not possible to setup external load balancers, external-dns, etc [stable/neo4j] allow custom configuration of neo4j ; without this, custom configuration is not possible which includes setting configmap based settings, which also includes turning on apoc.","title":"K8S Installation"},{"location":"k8s_install/#amundsen-k8s-helm-charts","text":"Source code can be found here","title":"Amundsen K8s Helm Charts"},{"location":"k8s_install/#what-is-this","text":"This is setup templates for deploying amundsen on k8s (kubernetes) , using helm.","title":"What is this?"},{"location":"k8s_install/#how-do-i-get-started","text":"Make sure you have the following command line clients setup: k8s (kubectl) helm Build out a cloud based k8s cluster, such as Amazon EKS Ensure you can connect to your cluster with cli tools in step 1.","title":"How do I get started?"},{"location":"k8s_install/#prerequisites","text":"Helm 2.14+ Kubernetes 1.14+","title":"Prerequisites"},{"location":"k8s_install/#chart-requirements","text":"Repository Name Version https://charts.helm.sh/stable elasticsearch 1.32.0","title":"Chart Requirements"},{"location":"k8s_install/#chart-values","text":"Key Type Default Description LONG_RANDOM_STRING int 1234 A long random string. You should probably provide your own. This is needed for OIDC. affinity object {} amundsen application wide configuration of affinity. This applies to search, metadata, frontend and neo4j. Elasticsearch has it\u2019s own configuation properties for this. ref dnsZone string \"teamname.company.com\" DEPRECATED - its not standard to pre construct urls this way. The dns zone (e.g. group-qa.myaccount.company.com) the app is running in. Used to construct dns hostnames (on aws only). dockerhubImagePath string \"amundsendev\" DEPRECATED - this is not useful, it would be better to just allow the whole image to be swapped instead. The image path for dockerhub. elasticsearch.client.replicas int 1 only running amundsen on 1 client replica elasticsearch.cluster.env.EXPECTED_MASTER_NODES int 1 required to match master.replicas elasticsearch.cluster.env.MINIMUM_MASTER_NODES int 1 required to match master.replicas elasticsearch.cluster.env.RECOVER_AFTER_MASTER_NODES int 1 required to match master.replicas elasticsearch.data.replicas int 1 only running amundsen on 1 data replica elasticsearch.enabled bool true set this to false, if you want to provide your own ES instance. elasticsearch.master.replicas int 1 only running amundsen on 1 master replica environment string \"dev\" DEPRECATED - its not standard to pre construct urls this way. The environment the app is running in. Used to construct dns hostnames (on aws only) and ports. frontEnd.OIDC_AUTH_SERVER_ID string nil The authorization server id for OIDC. frontEnd.OIDC_CLIENT_ID string nil The client id for OIDC. frontEnd.OIDC_CLIENT_SECRET string \"\" The client secret for OIDC. frontEnd.OIDC_ORG_URL string nil The organization URL for OIDC. frontEnd.OVERWRITE_REDIRECT_URI string nil The redirect uri for OIDC. frontEnd.affinity object {} Frontend pod specific affinity. frontEnd.annotations object {} Frontend service specific tolerations. frontEnd.baseUrl string \"http://localhost\" used by notifications util to provide links to amundsen pages in emails. frontEnd.createOidcSecret bool false OIDC needs some configuration. If you want the chart to make your secrets, set this to true and set the next four values. If you don\u2019t want to configure your secrets via helm, you can still use the amundsen-oidc-config.yaml as a template frontEnd.image string \"amundsendev/amundsen-frontend\" The image of the frontend container. frontEnd.imagePullSecrets list [] Optional pod imagePullSecrets ref frontEnd.imageTag string \"2.3.0\" The image tag of the frontend container. frontEnd.nodeSelector object {} Frontend pod specific nodeSelector. frontEnd.oidcEnabled bool false To enable auth via OIDC, set this to true. frontEnd.podAnnotations object {} Frontend pod specific annotations. frontEnd.replicas int 1 How many replicas of the frontend service to run. frontEnd.resources object {} See pod resourcing ref frontEnd.serviceName string \"frontend\" The frontend service name. frontEnd.servicePort int 80 The port the frontend service will be exposed on via the loadbalancer. frontEnd.serviceType string \"ClusterIP\" The frontend service type. See service types ref frontEnd.tolerations list [] Frontend pod specific tolerations. metadata.affinity object {} Metadata pod specific affinity. metadata.annotations object {} Metadata service specific tolerations. metadata.image string \"amundsendev/amundsen-metadata\" The image of the metadata container. metadata.imagePullSecrets list [] Optional pod imagePullSecrets ref metadata.imageTag string \"2.5.5\" The image tag of the metadata container. metadata.neo4jEndpoint string nil The name of the service hosting neo4j on your cluster, if you bring your own. You should only need to change this, if you don\u2019t use the version in this chart. metadata.nodeSelector object {} Metadata pod specific nodeSelector. metadata.podAnnotations object {} Metadata pod specific annotations. metadata.replicas int 1 How many replicas of the metadata service to run. metadata.resources object {} See pod resourcing ref metadata.serviceName string \"metadata\" The metadata service name. metadata.serviceType string \"ClusterIP\" The metadata service type. See service types ref metadata.tolerations list [] Metadata pod specific tolerations. neo4j.affinity object {} neo4j specific affinity. neo4j.annotations object {} neo4j service specific tolerations. neo4j.backup object {\"enabled\":false,\"podAnnotations\":{},\"s3Path\":\"s3://dev/null\",\"schedule\":\"0 * * * *\"} If enabled is set to true, make sure and set the s3 path as well. neo4j.backup.s3Path string \"s3://dev/null\" The s3path to write to for backups. neo4j.backup.schedule string \"0 * * * *\" The schedule to run backups on. Defaults to hourly. neo4j.config object {\"dbms\":{\"heap_initial_size\":\"1G\",\"heap_max_size\":\"2G\",\"pagecache_size\":\"2G\"}} Neo4j application specific configuration. This type of configuration is why the charts/stable version is not used. See ref neo4j.config.dbms object {\"heap_initial_size\":\"1G\",\"heap_max_size\":\"2G\",\"pagecache_size\":\"2G\"} dbms config for neo4j neo4j.config.dbms.heap_initial_size string \"1G\" the initial java heap for neo4j neo4j.config.dbms.heap_max_size string \"2G\" the max java heap for neo4j neo4j.config.dbms.pagecache_size string \"2G\" the page cache size for neo4j neo4j.enabled bool true If neo4j is enabled as part of this chart, or not. Set this to false if you want to provide your own version. neo4j.nodeSelector object {} neo4j specific nodeSelector. neo4j.persistence object {} Neo4j persistence. Turn this on to keep your data between pod crashes, etc. This is also needed for backups. neo4j.podAnnotations object {} neo4j pod specific annotations. neo4j.resources object {} See pod resourcing ref neo4j.tolerations list [] neo4j specific tolerations. neo4j.version string \"3.3.0\" The neo4j application version used by amundsen. nodeSelector object {} amundsen application wide configuration of nodeSelector. This applies to search, metadata, frontend and neo4j. Elasticsearch has it\u2019s own configuation properties for this. ref podAnnotations object {} amundsen application wide configuration of podAnnotations. This applies to search, metadata, frontend and neo4j. Elasticsearch has it\u2019s own configuation properties for this. ref provider string \"aws\" The cloud provider the app is running in. Used to construct dns hostnames (on aws only). search.affinity object {} Search pod specific affinity. search.annotations object {} Search service specific tolerations. search.elasticSearchCredentials object {} The elasticsearch user and password. This should only be set if you bring your own elasticsearch cluster in which case you must also set elasticsearch.enabled to false search.elasticsearchEndpoint string nil The name of the service hosting elasticsearch on your cluster, if you bring your own. You should only need to change this, if you don\u2019t use the version in this chart. search.image string \"amundsendev/amundsen-search\" The image of the search container. search.imagePullSecrets list [] Optional pod imagePullSecrets ref search.imageTag string \"2.4.0\" The image tag of the search container. search.nodeSelector object {} Search pod specific nodeSelector. search.podAnnotations object {} Search pod specific annotations. search.replicas int 1 How many replicas of the search service to run. search.resources object {} See pod resourcing ref search.serviceName string \"search\" The search service name. search.serviceType string \"ClusterIP\" The search service type. See service types ref search.tolerations list [] Search pod specific tolerations. tolerations list [] amundsen application wide configuration of tolerations. This applies to search, metadata, frontend and neo4j. Elasticsearch has it\u2019s own configuation properties for this. ref Autogenerated from chart metadata using helm-docs v1.5.0","title":"Chart Values"},{"location":"k8s_install/#neo4j-dbms-config","text":"You may want to override the default memory usage for Neo4J. In particular, if you\u2019re just test-driving a deployment and your node exits with status 137, you should set the usage to smaller values: config : dbms : heap_initial_size : 1G heap_max_size : 2G pagecache_size : 2G With this values file, you can then install Amundsen using Helm 2 with: helm install ./templates/helm --values impl/helm/dev/values.yaml For Helm 3 it\u2019s now mandatory to specify a chart reference name e.g. my-amundsen : helm install my-amundsen ./templates/helm --values impl/helm/dev/values.yaml","title":"Neo4j DBMS Config?"},{"location":"k8s_install/#other-notes","text":"For aws setup, you will also need to setup the external-dns plugin There is an existing helm chart for neo4j, but, it is missing some features necessary to for use such as: [stable/neo4j] make neo4j service definition more extensible ; without this, it is not possible to setup external load balancers, external-dns, etc [stable/neo4j] allow custom configuration of neo4j ; without this, custom configuration is not possible which includes setting configmap based settings, which also includes turning on apoc.","title":"Other Notes"},{"location":"roadmap/","text":"Amundsen Roadmap \u00b6 The following roadmap gives an overview of what we are currently working on and what we want to tackle next. This helps potential contributors understand the current status of your project and where it\u2019s going next, as well as giving a chance to be part of the planning. Amundsen Mission \u00b6 To organize all information about data and make it universally actionable Vision for 2021 \u00b6 Centralize a comprehensive and actionable map of all our data resources that can be leveraged to solve a growing number of use cases and workflows Short Term - Our Current focus \u00b6 Native lineage integration \u00b6 What : We want to create a native lineage integration in Amundsen, to better surface how data assets interact with each other. Status : designs complete Integrate with Data Quality system \u00b6 What : Integrate with different data quality systems to provide quality score. Status : in progress Improve search ranking \u00b6 What : Overhaul search ranking to improve results. Status : planning Show distinct column values \u00b6 What : When a column has a limited set of possible values, we want to make then easily discoverable. Status : implementation started Neptune Databuilder support \u00b6 What : Supports Databuilder ingestion for Neptune ( FsNeo4jCSVLoader , FsNeputuneCSVLoader and various Neptune components). Detail could be found in RFC-13 . Status : implementation started RDS Proxy Support \u00b6 What : Support RDS as another type of proxy for both metadata and databuilder. Detail could be found in RFC-10 Status : Planning Mid Term - Our Next steps \u00b6 Curated navigation experience \u00b6 What : Currently Amundsen\u2019s experience is very focussed on search. However, especially for new users, an experience where they are able to navigate through the data hierarchy is very important. This item proposes to revamp the navigational experience in Amundsen (currently, barebones - based on tags) to do justice to the user need to browse through data sets when they don\u2019t know what to even search for. Status : planning Notifications when a table evolves \u00b6 What : Notify users in Amundsen (akin to Facebook notifications or similar) when a table evolves. Owners of data and consumers of data will likely need to be notified of different things. Status : planning has not started Commonly joined tables / browsing the data model \u00b6 What : As a data user, I would like to see commonly joined tables and how to join them. One option would be to show commonly joined tables and showing example join queries. Another option would be to provide a navigational experience for data model, showing foreign keys and which tables they come from. Status : planning has not started Push ingest API \u00b6 What : Possible through Kafka extractor , though Kafka topic schema is not well defined. And it requires client side SDK to support message pushing. Status : 50% Granular Access Control \u00b6 What : we want to have a more granular control of the access. For example, only certain types of people would be able to see certain types of metadata/functionality Status : not planned Versioning system \u00b6 What : We want to create a versioning system for our indexed resources, to be able to index different versions of the same resource. This is especially required for machine learning purposes. Status : not planned Index Processes \u00b6 What : we want to index ETLs and pipelines from our Machine Learning Engine Status : not planned Index Teams \u00b6 What : We want to add teams pages to enable users to see what are the important tables and dashboard a team uses Status : not planned Index Services \u00b6 What : With our microservices architecture, we want to index services and show how these services interact with data artifacts Status : not planned Index S3 buckets \u00b6 What : add these new resource types to our data map and create resource pages for them Status : not planned Index Pub/Sub systems \u00b6 What : We want to make our pub/sub systems discoverable Status : not planned How to Get Involved \u00b6 Let us know in the Slack channel if you are interested in taking a stab at leading the development of one of these features. You can also jump right in by tackling one of our issues labeled as \u2018help wanted\u2019 or, if you are new to Amundsen, try one of our \u2018good first issue\u2019 tickets.","title":"Roadmap"},{"location":"roadmap/#amundsen-roadmap","text":"The following roadmap gives an overview of what we are currently working on and what we want to tackle next. This helps potential contributors understand the current status of your project and where it\u2019s going next, as well as giving a chance to be part of the planning.","title":"Amundsen Roadmap"},{"location":"roadmap/#amundsen-mission","text":"To organize all information about data and make it universally actionable","title":"Amundsen Mission"},{"location":"roadmap/#vision-for-2021","text":"Centralize a comprehensive and actionable map of all our data resources that can be leveraged to solve a growing number of use cases and workflows","title":"Vision for 2021"},{"location":"roadmap/#short-term-our-current-focus","text":"","title":"Short Term - Our Current focus"},{"location":"roadmap/#native-lineage-integration","text":"What : We want to create a native lineage integration in Amundsen, to better surface how data assets interact with each other. Status : designs complete","title":"Native lineage integration"},{"location":"roadmap/#integrate-with-data-quality-system","text":"What : Integrate with different data quality systems to provide quality score. Status : in progress","title":"Integrate with Data Quality system"},{"location":"roadmap/#improve-search-ranking","text":"What : Overhaul search ranking to improve results. Status : planning","title":"Improve search ranking"},{"location":"roadmap/#show-distinct-column-values","text":"What : When a column has a limited set of possible values, we want to make then easily discoverable. Status : implementation started","title":"Show distinct column values"},{"location":"roadmap/#neptune-databuilder-support","text":"What : Supports Databuilder ingestion for Neptune ( FsNeo4jCSVLoader , FsNeputuneCSVLoader and various Neptune components). Detail could be found in RFC-13 . Status : implementation started","title":"Neptune Databuilder support"},{"location":"roadmap/#rds-proxy-support","text":"What : Support RDS as another type of proxy for both metadata and databuilder. Detail could be found in RFC-10 Status : Planning","title":"RDS Proxy Support"},{"location":"roadmap/#mid-term-our-next-steps","text":"","title":"Mid Term - Our Next steps"},{"location":"roadmap/#curated-navigation-experience","text":"What : Currently Amundsen\u2019s experience is very focussed on search. However, especially for new users, an experience where they are able to navigate through the data hierarchy is very important. This item proposes to revamp the navigational experience in Amundsen (currently, barebones - based on tags) to do justice to the user need to browse through data sets when they don\u2019t know what to even search for. Status : planning","title":"Curated navigation experience"},{"location":"roadmap/#notifications-when-a-table-evolves","text":"What : Notify users in Amundsen (akin to Facebook notifications or similar) when a table evolves. Owners of data and consumers of data will likely need to be notified of different things. Status : planning has not started","title":"Notifications when a table evolves"},{"location":"roadmap/#commonly-joined-tables-browsing-the-data-model","text":"What : As a data user, I would like to see commonly joined tables and how to join them. One option would be to show commonly joined tables and showing example join queries. Another option would be to provide a navigational experience for data model, showing foreign keys and which tables they come from. Status : planning has not started","title":"Commonly joined tables / browsing the data model"},{"location":"roadmap/#push-ingest-api","text":"What : Possible through Kafka extractor , though Kafka topic schema is not well defined. And it requires client side SDK to support message pushing. Status : 50%","title":"Push ingest API"},{"location":"roadmap/#granular-access-control","text":"What : we want to have a more granular control of the access. For example, only certain types of people would be able to see certain types of metadata/functionality Status : not planned","title":"Granular Access Control"},{"location":"roadmap/#versioning-system","text":"What : We want to create a versioning system for our indexed resources, to be able to index different versions of the same resource. This is especially required for machine learning purposes. Status : not planned","title":"Versioning system"},{"location":"roadmap/#index-processes","text":"What : we want to index ETLs and pipelines from our Machine Learning Engine Status : not planned","title":"Index Processes"},{"location":"roadmap/#index-teams","text":"What : We want to add teams pages to enable users to see what are the important tables and dashboard a team uses Status : not planned","title":"Index Teams"},{"location":"roadmap/#index-services","text":"What : With our microservices architecture, we want to index services and show how these services interact with data artifacts Status : not planned","title":"Index Services"},{"location":"roadmap/#index-s3-buckets","text":"What : add these new resource types to our data map and create resource pages for them Status : not planned","title":"Index S3 buckets"},{"location":"roadmap/#index-pubsub-systems","text":"What : We want to make our pub/sub systems discoverable Status : not planned","title":"Index Pub/Sub systems"},{"location":"roadmap/#how-to-get-involved","text":"Let us know in the Slack channel if you are interested in taking a stab at leading the development of one of these features. You can also jump right in by tackling one of our issues labeled as \u2018help wanted\u2019 or, if you are new to Amundsen, try one of our \u2018good first issue\u2019 tickets.","title":"How to Get Involved"},{"location":"authentication/oidc/","text":"OIDC (Keycloak) Authentication \u00b6 Setting up end-to-end authentication using OIDC is fairly simple and can be done using a Flask wrapper i.e., flaskoidc . flaskoidc leverages the Flask\u2019s before_request functionality to authenticate each request before passing that to the views. It also accepts headers on each request if available in order to validate bearer token from incoming requests. Installation \u00b6 Please refer to the flaskoidc documentation for the installation and the configurations. Note: You need to install and configure flaskoidc for each microservice of Amundsen i.e., for frontendlibrary, metadatalibrary and searchlibrary in order to secure each of them. Amundsen Configuration \u00b6 Once you have flaskoidc installed and configured for each microservice, please set the following environment variables: amundsenfrontendlibrary: APP_WRAPPER: flaskoidc APP_WRAPPER_CLASS: FlaskOIDC amundsenmetadatalibrary: FLASK_APP_MODULE_NAME: flaskoidc FLASK_APP_CLASS_NAME: FlaskOIDC amundsensearchlibrary: (Needs to be implemented) FLASK_APP_MODULE_NAME: flaskoidc FLASK_APP_CLASS_NAME: FlaskOIDC By default flaskoidc whitelist the healthcheck URLs, to not authenticate them. In case of metadatalibrary and searchlibrary we may want to whitelist the healthcheck APIs explicitly using following environment variable. FLASK_OIDC_WHITELISTED_ENDPOINTS: 'api.healthcheck' Setting Up Request Headers \u00b6 To communicate securely between the microservices, you need to pass the bearer token from frontend in each request to metadatalibrary and searchlibrary. This should be done using REQUEST_HEADERS_METHOD config variable in frontendlibrary. Define a function to add the bearer token in each request in your config.py: def get_access_headers ( app ): \"\"\" Function to retrieve and format the Authorization Headers that can be passed to various microservices who are expecting that. :param oidc: OIDC object having authorization information :return: A formatted dictionary containing access token as Authorization header. \"\"\" try : access_token = app . oidc . get_access_token () return { 'Authorization' : 'Bearer {} ' . format ( access_token )} except Exception : return None Set the method as the request header method in your config.py: REQUEST_HEADERS_METHOD = get_access_headers This function will be called using the current app instance to add the headers in each request when calling any endpoint of metadatalibrary and searchlibrary here Setting Up Auth User Method \u00b6 In order to get the current authenticated user (which is being used in Amundsen for many operations), we need to set AUTH_USER_METHOD config variable in frontendlibrary. This function should return email address, user id and any other required information. Define a function to fetch the user information in your config.py: def get_auth_user ( app ): \"\"\" Retrieves the user information from oidc token, and then makes a dictionary 'UserInfo' from the token information dictionary. We need to convert it to a class in order to use the information in the rest of the Amundsen application. :param app: The instance of the current app. :return: A class UserInfo \"\"\" from flask import g user_info = type ( 'UserInfo' , ( object ,), g . oidc_id_token ) # noinspection PyUnresolvedReferences user_info . user_id = user_info . preferred_username return user_info Set the method as the auth user method in your config.py: AUTH_USER_METHOD = get_auth_user Once done, you\u2019ll have the end-to-end authentication in Amundsen without any proxy or code changes. Using Okta with Amundsen on K8s \u00b6 Assumptions: You have access to okta (you can create a developer account for free!) You are using k8s to setup amundsen. See amundsen-kube-helm You need to have a stable DNS entry for amundsen-frontend that can be registered in okta. for example in AWS you can setup route53 I will assume for the rest of this tutorial that your stable uri is \u201c http://amundsen-frontend \u201c You need to register amundsen in okta as an app. More info here . But here are specific instructions for amundsen: At this time, I have only succesfully tested integration after ALL grants were checked. Set the Login redirect URIs to: http://amundsen-frontend/oidc_callback No need to set a logout redirect URI Set the Initiate login URI to: http://amundsen-frontend/ (This is where okta will take you if users click on amundsen via okta landing page) Copy the Client ID and Client secret as you will need this later. At present, there is no oidc build of the frontend. So you will need to build an oidc build yourself and upload it to, for example ECR, for use by k8s. You can then specify which image you want to use as a property override for your helm install like so: frontEndServiceImage : 123.dkr.ecr.us-west-2.amazonaws.com/edmunds/amundsen-frontend:oidc-test Please see further down in this doc for more instructions on how to build frontend. 4. When you start up helm you will need to provide some properties. Here are the properties that need to be overridden for oidc to work: 1 2 3 4 5 6 7 8 9 10 ```yaml oidcEnabled: true createOidcSecret: true OIDC_CLIENT_ID: YOUR_CLIENT_ID OIDC_CLIENT_SECRET: YOUR_SECRET_ID OIDC_ORG_URL: https://amundsen.okta.com OIDC_AUTH_SERVER_ID: default # You also will need a custom oidc frontend build too frontEndServiceImage: 123.dkr.ecr.us-west-2.amazonaws.com/edmunds/amundsen-frontend:oidc-test ``` Building frontend with OIDC \u00b6 Please look at this guide for instructions on how to build a custom frontend docker image. The only difference to above is that in your docker file you will want to add the following at the end. This will make sure its ready to go for oidc. You can take alook at the public.Dockerfile as a reference. RUN pip3 install . [ oidc ] ENV FRONTEND_SVC_CONFIG_MODULE_CLASS amundsen_application.oidc_config.OidcConfig ENV APP_WRAPPER flaskoidc ENV APP_WRAPPER_CLASS FlaskOIDC ENV FLASK_OIDC_WHITELISTED_ENDPOINTS status,healthcheck,health ENV SQLALCHEMY_DATABASE_URI sqlite:///sessions.db Please also take a look at this blog post for more detail.","title":"Authentication"},{"location":"authentication/oidc/#oidc-keycloak-authentication","text":"Setting up end-to-end authentication using OIDC is fairly simple and can be done using a Flask wrapper i.e., flaskoidc . flaskoidc leverages the Flask\u2019s before_request functionality to authenticate each request before passing that to the views. It also accepts headers on each request if available in order to validate bearer token from incoming requests.","title":"OIDC (Keycloak) Authentication"},{"location":"authentication/oidc/#installation","text":"Please refer to the flaskoidc documentation for the installation and the configurations. Note: You need to install and configure flaskoidc for each microservice of Amundsen i.e., for frontendlibrary, metadatalibrary and searchlibrary in order to secure each of them.","title":"Installation"},{"location":"authentication/oidc/#amundsen-configuration","text":"Once you have flaskoidc installed and configured for each microservice, please set the following environment variables: amundsenfrontendlibrary: APP_WRAPPER: flaskoidc APP_WRAPPER_CLASS: FlaskOIDC amundsenmetadatalibrary: FLASK_APP_MODULE_NAME: flaskoidc FLASK_APP_CLASS_NAME: FlaskOIDC amundsensearchlibrary: (Needs to be implemented) FLASK_APP_MODULE_NAME: flaskoidc FLASK_APP_CLASS_NAME: FlaskOIDC By default flaskoidc whitelist the healthcheck URLs, to not authenticate them. In case of metadatalibrary and searchlibrary we may want to whitelist the healthcheck APIs explicitly using following environment variable. FLASK_OIDC_WHITELISTED_ENDPOINTS: 'api.healthcheck'","title":"Amundsen Configuration"},{"location":"authentication/oidc/#setting-up-request-headers","text":"To communicate securely between the microservices, you need to pass the bearer token from frontend in each request to metadatalibrary and searchlibrary. This should be done using REQUEST_HEADERS_METHOD config variable in frontendlibrary. Define a function to add the bearer token in each request in your config.py: def get_access_headers ( app ): \"\"\" Function to retrieve and format the Authorization Headers that can be passed to various microservices who are expecting that. :param oidc: OIDC object having authorization information :return: A formatted dictionary containing access token as Authorization header. \"\"\" try : access_token = app . oidc . get_access_token () return { 'Authorization' : 'Bearer {} ' . format ( access_token )} except Exception : return None Set the method as the request header method in your config.py: REQUEST_HEADERS_METHOD = get_access_headers This function will be called using the current app instance to add the headers in each request when calling any endpoint of metadatalibrary and searchlibrary here","title":"Setting Up Request Headers"},{"location":"authentication/oidc/#setting-up-auth-user-method","text":"In order to get the current authenticated user (which is being used in Amundsen for many operations), we need to set AUTH_USER_METHOD config variable in frontendlibrary. This function should return email address, user id and any other required information. Define a function to fetch the user information in your config.py: def get_auth_user ( app ): \"\"\" Retrieves the user information from oidc token, and then makes a dictionary 'UserInfo' from the token information dictionary. We need to convert it to a class in order to use the information in the rest of the Amundsen application. :param app: The instance of the current app. :return: A class UserInfo \"\"\" from flask import g user_info = type ( 'UserInfo' , ( object ,), g . oidc_id_token ) # noinspection PyUnresolvedReferences user_info . user_id = user_info . preferred_username return user_info Set the method as the auth user method in your config.py: AUTH_USER_METHOD = get_auth_user Once done, you\u2019ll have the end-to-end authentication in Amundsen without any proxy or code changes.","title":"Setting Up Auth User Method"},{"location":"authentication/oidc/#using-okta-with-amundsen-on-k8s","text":"Assumptions: You have access to okta (you can create a developer account for free!) You are using k8s to setup amundsen. See amundsen-kube-helm You need to have a stable DNS entry for amundsen-frontend that can be registered in okta. for example in AWS you can setup route53 I will assume for the rest of this tutorial that your stable uri is \u201c http://amundsen-frontend \u201c You need to register amundsen in okta as an app. More info here . But here are specific instructions for amundsen: At this time, I have only succesfully tested integration after ALL grants were checked. Set the Login redirect URIs to: http://amundsen-frontend/oidc_callback No need to set a logout redirect URI Set the Initiate login URI to: http://amundsen-frontend/ (This is where okta will take you if users click on amundsen via okta landing page) Copy the Client ID and Client secret as you will need this later. At present, there is no oidc build of the frontend. So you will need to build an oidc build yourself and upload it to, for example ECR, for use by k8s. You can then specify which image you want to use as a property override for your helm install like so: frontEndServiceImage : 123.dkr.ecr.us-west-2.amazonaws.com/edmunds/amundsen-frontend:oidc-test Please see further down in this doc for more instructions on how to build frontend. 4. When you start up helm you will need to provide some properties. Here are the properties that need to be overridden for oidc to work: 1 2 3 4 5 6 7 8 9 10 ```yaml oidcEnabled: true createOidcSecret: true OIDC_CLIENT_ID: YOUR_CLIENT_ID OIDC_CLIENT_SECRET: YOUR_SECRET_ID OIDC_ORG_URL: https://amundsen.okta.com OIDC_AUTH_SERVER_ID: default # You also will need a custom oidc frontend build too frontEndServiceImage: 123.dkr.ecr.us-west-2.amazonaws.com/edmunds/amundsen-frontend:oidc-test ```","title":"Using Okta with Amundsen on K8s"},{"location":"authentication/oidc/#building-frontend-with-oidc","text":"Please look at this guide for instructions on how to build a custom frontend docker image. The only difference to above is that in your docker file you will want to add the following at the end. This will make sure its ready to go for oidc. You can take alook at the public.Dockerfile as a reference. RUN pip3 install . [ oidc ] ENV FRONTEND_SVC_CONFIG_MODULE_CLASS amundsen_application.oidc_config.OidcConfig ENV APP_WRAPPER flaskoidc ENV APP_WRAPPER_CLASS FlaskOIDC ENV FLASK_OIDC_WHITELISTED_ENDPOINTS status,healthcheck,health ENV SQLALCHEMY_DATABASE_URI sqlite:///sessions.db Please also take a look at this blog post for more detail.","title":"Building frontend with OIDC"},{"location":"common/","text":"Amundsen Common \u00b6 Amundsen Common library holds common codes among micro services in Amundsen. For information about Amundsen and our other services, visit the main repository . Please also see our instructions for a quick start setup of Amundsen with dummy data, and an overview of the architecture . Requirements \u00b6 Python >= 3.6 Doc \u00b6 https://www.amundsen.io/amundsen/","title":"Overview"},{"location":"common/#amundsen-common","text":"Amundsen Common library holds common codes among micro services in Amundsen. For information about Amundsen and our other services, visit the main repository . Please also see our instructions for a quick start setup of Amundsen with dummy data, and an overview of the architecture .","title":"Amundsen Common"},{"location":"common/#requirements","text":"Python >= 3.6","title":"Requirements"},{"location":"common/#doc","text":"https://www.amundsen.io/amundsen/","title":"Doc"},{"location":"common/CHANGELOG/","text":"Feature \u00b6 Added lineage item and lineage entities ( #90 ) ( f1c6011 ) Add chart into common ES index map ( #77 ) ( 4a7eea4 ) Add chart to dashboard model ( #73 ) ( 241f627 ) Added badges field (optional) to column in table model ( #68 ) ( 7bf5a84 ) Add marshmallow packages to setup.py ( #66 ) ( 7ff2fe1 ) Tweaks for gremlin support ( #60 ) ( 1a2733b ) Table model badges field update ( #56 ) ( 6a393d0 ) Added new badge model ( #55 ) ( 09897d9 ) Add github action for test and pypi publish ( #47 ) ( 1a466b1 ) Added resource_reports into Table model ( 60b1751 ) Fix \u00b6 Standardize requirements and fixes for marshmallow3+ ( #98 ) ( d185046 ) Moved version declaration ( #88 ) ( 19be687 ) Fix table index map bug ( #86 ) ( f250d6a ) Make column names searchable by lowercase ( #85 ) ( 0ead455 ) Changed marshmallow-annotation version, temp solution ( #81 ) ( ff9d2e2 ) Enable flake8 and mypy in CI ( #75 ) ( 32e317c ) Fix import ( #74 ) ( 2d1725b ) Add dashboard index map copied from amundsendatabuilder ( #65 ) ( 551834b ) Update elasticsearch mapping ( #64 ) ( b43a687 )","title":"CHANGELOG"},{"location":"common/CHANGELOG/#feature","text":"Added lineage item and lineage entities ( #90 ) ( f1c6011 ) Add chart into common ES index map ( #77 ) ( 4a7eea4 ) Add chart to dashboard model ( #73 ) ( 241f627 ) Added badges field (optional) to column in table model ( #68 ) ( 7bf5a84 ) Add marshmallow packages to setup.py ( #66 ) ( 7ff2fe1 ) Tweaks for gremlin support ( #60 ) ( 1a2733b ) Table model badges field update ( #56 ) ( 6a393d0 ) Added new badge model ( #55 ) ( 09897d9 ) Add github action for test and pypi publish ( #47 ) ( 1a466b1 ) Added resource_reports into Table model ( 60b1751 )","title":"Feature"},{"location":"common/CHANGELOG/#fix","text":"Standardize requirements and fixes for marshmallow3+ ( #98 ) ( d185046 ) Moved version declaration ( #88 ) ( 19be687 ) Fix table index map bug ( #86 ) ( f250d6a ) Make column names searchable by lowercase ( #85 ) ( 0ead455 ) Changed marshmallow-annotation version, temp solution ( #81 ) ( ff9d2e2 ) Enable flake8 and mypy in CI ( #75 ) ( 32e317c ) Fix import ( #74 ) ( 2d1725b ) Add dashboard index map copied from amundsendatabuilder ( #65 ) ( 551834b ) Update elasticsearch mapping ( #64 ) ( b43a687 )","title":"Fix"},{"location":"databuilder/","text":"Amundsen Databuilder \u00b6 Amundsen Databuilder is a data ingestion library, which is inspired by Apache Gobblin . It could be used in an orchestration framework(e.g. Apache Airflow) to build data from Amundsen. You could use the library either with an adhoc python script( example ) or inside an Apache Airflow DAG( example ). For information about Amundsen and our other services, visit the main repository README.md . Please also see our instructions for a quick start setup of Amundsen with dummy data, and an overview of the architecture . Requirements \u00b6 Python >= 3.6.x Doc \u00b6 https://www.amundsen.io/amundsen/ Concept \u00b6 ETL job consists of extraction of records from the source, transform records, if necessary, and load records into the sink. Amundsen Databuilder is a ETL framework for Amundsen and there are corresponding components for ETL called Extractor, Transformer, and Loader that deals with record level operation. A component called task controls all these three components. Job is the highest level component in Databuilder that controls task and publisher and is the one that client use to launch the ETL job. In Databuilder, each components are highly modularized and each components are using namespace based config, HOCON config, which makes it highly reusable and pluggable. (e.g: transformer can be reused within extractor, or extractor can be reused within extractor) (Note that concept on components are highly motivated by Apache Gobblin ) Extractor \u00b6 Extractor extracts record from the source. This does not neccessarily mean that it only supports pull pattern in ETL. For example, extracting record from messaging bus make it a push pattern in ETL. Transformer \u00b6 Transfomer takes record from either extractor or from transformer itself (via ChainedTransformer) to transform record. Loader \u00b6 A loader takes record from transformer or from extractor directly and load it to sink, or staging area. As loader is operated in record level, it\u2019s not capable of supporting atomicity. Task \u00b6 A task orchestrates extractor, transformer, and loader to perform record level operation. Record \u00b6 A record is represented by one of models . Publisher \u00b6 A publisher is an optional component. It\u2019s common usage is to support atomicity in job level and/or to easily support bulk load into the sink. Job \u00b6 Job is the highest level component in Databuilder, and it orchestrates task, and publisher. Model \u00b6 Models are abstractions representing the domain. List of extractors \u00b6 DBAPIExtractor \u00b6 An extractor that uses Python Database API interface. DBAPI requires three information, connection object that conforms DBAPI spec, a SELECT SQL statement, and a model class that correspond to the output of each row of SQL statement. job_config = ConfigFactory . from_dict ({ 'extractor.dbapi {} ' . format ( DBAPIExtractor . CONNECTION_CONFIG_KEY ): db_api_conn , 'extractor.dbapi. {} ' . format ( DBAPIExtractor . SQL_CONFIG_KEY ): select_sql_stmt , 'extractor.dbapi.model_class' : 'package.module_name.class_name' }) job = DefaultJob ( conf = job_config , task = DefaultTask ( extractor = DBAPIExtractor (), loader = AnyLoader ())) job . launch () GenericExtractor \u00b6 An extractor that takes list of dict from user through config. HiveTableLastUpdatedExtractor \u00b6 An extractor that extracts last updated time from Hive metastore and underlying file system. Although, hive metastore has a parameter called \u201clast_modified_time\u201d, but it cannot be used as it provides DDL timestamp not DML timestamp. For this reason, HiveTableLastUpdatedExtractor is utilizing underlying file of Hive to fetch latest updated date. However, it is not efficient to poke all files in Hive, and it only pokes underlying storage for non-partitioned table. For partitioned table, it will fetch partition created timestamp, and it\u2019s close enough for last updated timestamp. As getting metadata from files could be time consuming there\u2019re several features to increase performance. 1. Support of multithreading to parallelize metadata fetching. Although, cpython\u2019s multithreading is not true multithreading as it\u2019s bounded by single core, getting metadata of file is mostly IO bound operation. Note that number of threads should be less or equal to number of connections. 1. User can pass where clause to only include certain schema and also remove certain tables. For example, by adding something like TBL_NAME NOT REGEXP '(tmp|temp) would eliminate unncecessary computation. job_config = ConfigFactory . from_dict ({ 'extractor.hive_table_last_updated.partitioned_table_where_clause_suffix' : partitioned_table_where_clause , 'extractor.hive_table_last_updated.non_partitioned_table_where_clause_suffix' ): non_partitioned_table_where_clause , 'extractor.hive_table_last_updated.extractor.sqlalchemy. {} ' . format ( SQLAlchemyExtractor . CONN_STRING ): connection_string , 'extractor.hive_table_last_updated.extractor.fs_worker_pool_size' : pool_size , 'extractor.hive_table_last_updated.filesystem. {} ' . format ( FileSystem . DASK_FILE_SYSTEM ): s3fs . S3FileSystem ( anon = False , config_kwargs = { 'max_pool_connections' : pool_size })}) job = DefaultJob ( conf = job_config , task = DefaultTask ( extractor = HiveTableLastUpdatedExtractor (), loader = AnyLoader ())) job . launch () HiveTableMetadataExtractor \u00b6 An extractor that extracts table and column metadata including database, schema, table name, table description, column name and column description from Hive metastore database. job_config = ConfigFactory . from_dict ({ 'extractor.hive_table_metadata. {} ' . format ( HiveTableMetadataExtractor . WHERE_CLAUSE_SUFFIX_KEY ): where_clause_suffix , 'extractor.hive_table_metadata.extractor.sqlalchemy. {} ' . format ( SQLAlchemyExtractor . CONN_STRING ): connection_string ()}) job = DefaultJob ( conf = job_config , task = DefaultTask ( extractor = HiveTableMetadataExtractor (), loader = AnyLoader ())) job . launch () CassandraExtractor \u00b6 An extractor that extracts table and column metadata including keyspace, table name, column name and column type from Apache Cassandra databases job_config = ConfigFactory . from_dict ({ 'extractor.cassandra. {} ' . format ( CassandraExtractor . CLUSTER_KEY ): cluster_identifier_string , 'extractor.cassandra. {} ' . format ( CassandraExtractor . IPS_KEY ): [ 127.0.0.1 ], 'extractor.cassandra. {} ' . format ( CassandraExtractor . KWARGS_KEY ): {}, 'extractor.cassandra. {} ' . format ( CassandraExtractor . FILTER_FUNCTION_KEY ): my_filter_function , }) job = DefaultJob ( conf = job_config , task = DefaultTask ( extractor = CassandraExtractor (), loader = AnyLoader ())) job . launch () If using the function filter options here is the function description def filter ( keytab , table ): # return False if you don't want to add that table and True if you want to add return True If needed to define more args on the cassandra cluster you can pass through kwargs args config = ConfigFactory . from_dict ({ 'extractor.cassandra. {} ' . format ( CassandraExtractor . IPS_KEY ): [ 127.0.0.1 ], 'extractor.cassandra. {} ' . format ( CassandraExtractor . KWARGS_KEY ): { 'port' : 9042 } }) # it will call the cluster constructor like this Cluster ([ 127.0.0.1 ], ** kwargs ) GlueExtractor \u00b6 An extractor that extracts table and column metadata including database, schema, table name, table description, column name and column description from AWS Glue metastore. Before running make sure you have a working AWS profile configured and have access to search tables on Glue job_config = ConfigFactory . from_dict ({ 'extractor.glue. {} ' . format ( GlueExtractor . CLUSTER_KEY ): cluster_identifier_string , 'extractor.glue. {} ' . format ( GlueExtractor . FILTER_KEY ): []}) job = DefaultJob ( conf = job_config , task = DefaultTask ( extractor = GlueExtractor (), loader = AnyLoader ())) job . launch () If using the filters option here is the input format [ { \"Key\": \"string\", \"Value\": \"string\", \"Comparator\": \"EQUALS\"|\"GREATER_THAN\"|\"LESS_THAN\"|\"GREATER_THAN_EQUALS\"|\"LESS_THAN_EQUALS\" } ... ] Delta-Lake-MetadataExtractor \u00b6 An extractor that runs on a spark cluster and obtains delta-lake metadata using spark sql commands. This custom solution is currently necessary because the hive metastore does not contain all metadata information for delta-lake tables. For simplicity, this extractor can also be used for all hive tables as well. Because it must run on a spark cluster, it is required that you have an operator (for example a databricks submit run operator ) that calls the configuration code on a spark cluster. spark = SparkSession . builder . appName ( \"Amundsen Delta Lake Metadata Extraction\" ) . getOrCreate () job_config = create_delta_lake_job_config () dExtractor = DeltaLakeMetadataExtractor () dExtractor . set_spark ( spark ) job = DefaultJob ( conf = job_config , task = DefaultTask ( extractor = dExtractor , loader = FsNeo4jCSVLoader ()), publisher = Neo4jCsvPublisher ()) job . launch () You can check out the sample deltalake metadata script for a full example. DremioMetadataExtractor \u00b6 An extractor that extracts table and column metadata including database, schema, table name, table description, column name and column description from Dremio . Before running make sure that you have the Dremio ODBC driver installed. Default config values assume the default driver name for the MacBook install . job_config = ConfigFactory . from_dict ({ 'extractor.dremio. {} ' . format ( DremioMetadataExtractor . DREMIO_USER_KEY ): DREMIO_USER , 'extractor.dremio. {} ' . format ( DremioMetadataExtractor . DREMIO_PASSWORD_KEY ): DREMIO_PASSWORD , 'extractor.dremio. {} ' . format ( DremioMetadataExtractor . DREMIO_HOST_KEY ): DREMIO_HOST }) job = DefaultJob ( conf = job_config , task = DefaultTask ( extractor = DremioMetadataExtractor (), loader = AnyLoader ())) job . launch () DruidMetadataExtractor \u00b6 An extractor that extracts table and column metadata including database, schema, table name, table description, column name and column description from a Druid DB. The where_clause_suffix could be defined, normally you would like to filter out the in INFORMATION_SCHEMA . You could specify the following job config conn_string = \"druid+https:// {host} : {port} /druid/v2/sql/\" . format ( host = druid_broker_host , port = 443 ) job_config = ConfigFactory . from_dict ({ 'extractor.druid_metadata. {} ' . format ( PostgresMetadataExtractor . WHERE_CLAUSE_SUFFIX_KEY ): where_clause_suffix , 'extractor.druid_metadata.extractor.sqlalchemy. {} ' . format ( SQLAlchemyExtractor . CONN_STRING ): conn_string ()}) job = DefaultJob ( conf = job_config , task = DefaultTask ( extractor = DruidMetadataExtractor (), loader = AnyLoader ())) job . launch () PostgresMetadataExtractor \u00b6 An extractor that extracts table and column metadata including database, schema, table name, table description, column name and column description from a Postgres or Redshift database. By default, the Postgres/Redshift database name is used as the cluster name. To override this, set USE_CATALOG_AS_CLUSTER_NAME to False , and CLUSTER_KEY to what you wish to use as the cluster name. The where_clause_suffix below should define which schemas you\u2019d like to query (see the sample dag for an example). The SQL query driving the extraction is defined here job_config = ConfigFactory . from_dict ({ 'extractor.postgres_metadata. {} ' . format ( PostgresMetadataExtractor . WHERE_CLAUSE_SUFFIX_KEY ): where_clause_suffix , 'extractor.postgres_metadata. {} ' . format ( PostgresMetadataExtractor . USE_CATALOG_AS_CLUSTER_NAME ): True , 'extractor.postgres_metadata.extractor.sqlalchemy. {} ' . format ( SQLAlchemyExtractor . CONN_STRING ): connection_string ()}) job = DefaultJob ( conf = job_config , task = DefaultTask ( extractor = PostgresMetadataExtractor (), loader = AnyLoader ())) job . launch () MSSQLMetadataExtractor \u00b6 An extractor that extracts table and column metadata including database, schema, table name, table description, column name and column description from a Microsoft SQL database. By default, the Microsoft SQL Server Database name is used as the cluster name. To override this, set USE_CATALOG_AS_CLUSTER_NAME to False , and CLUSTER_KEY to what you wish to use as the cluster name. The where_clause_suffix below should define which schemas you\u2019d like to query ( \"('dbo','sys')\" ). The SQL query driving the extraction is defined here This extractor is highly derived from PostgresMetadataExtractor . job_config = ConfigFactory . from_dict ({ 'extractor.mssql_metadata. {} ' . format ( MSSQLMetadataExtractor . WHERE_CLAUSE_SUFFIX_KEY ): where_clause_suffix , 'extractor.mssql_metadata. {} ' . format ( MSSQLMetadataExtractor . USE_CATALOG_AS_CLUSTER_NAME ): True , 'extractor.mssql_metadata.extractor.sqlalchemy. {} ' . format ( SQLAlchemyExtractor . CONN_STRING ): connection_string ()}) job = DefaultJob ( conf = job_config , task = DefaultTask ( extractor = MSSQLMetadataExtractor (), loader = AnyLoader ())) job . launch () MysqlMetadataExtractor \u00b6 An extractor that extracts table and column metadata including database, schema, table name, table description, column name and column description from a MYSQL database. By default, the MYSQL database name is used as the cluster name. To override this, set USE_CATALOG_AS_CLUSTER_NAME to False , and CLUSTER_KEY to what you wish to use as the cluster name. The where_clause_suffix below should define which schemas you\u2019d like to query. The SQL query driving the extraction is defined here job_config = ConfigFactory . from_dict ({ 'extractor.mysql_metadata. {} ' . format ( MysqlMetadataExtractor . WHERE_CLAUSE_SUFFIX_KEY ): where_clause_suffix , 'extractor.mysql_metadata. {} ' . format ( MysqlMetadataExtractor . USE_CATALOG_AS_CLUSTER_NAME ): True , 'extractor.mysql_metadata.extractor.sqlalchemy. {} ' . format ( SQLAlchemyExtractor . CONN_STRING ): connection_string ()}) job = DefaultJob ( conf = job_config , task = DefaultTask ( extractor = MysqlMetadataExtractor (), loader = FsNeo4jCSVLoader ()), publisher = Neo4jCsvPublisher ()) job . launch () Db2MetadataExtractor \u00b6 An extractor that extracts table and column metadata including database, schema, table name, table description, column name and column description from a Unix, Windows or Linux Db2 database or BigSQL. The where_clause_suffix below should define which schemas you\u2019d like to query or those that you would not (see the sample data loader for an example). The SQL query driving the extraction is defined here job_config = ConfigFactory . from_dict ({ 'extractor.db2_metadata. {} ' . format ( Db2MetadataExtractor . WHERE_CLAUSE_SUFFIX_KEY ): where_clause_suffix , 'extractor.db2_metadata.extractor.sqlalchemy. {} ' . format ( SQLAlchemyExtractor . CONN_STRING ): connection_string ()}) job = DefaultJob ( conf = job_config , task = DefaultTask ( extractor = Db2MetadataExtractor (), loader = AnyLoader ())) job . launch () SnowflakeMetadataExtractor \u00b6 An extractor that extracts table and column metadata including database, schema, table name, table description, column name and column description from a Snowflake database. By default, the Snowflake database name is used as the cluster name. To override this, set USE_CATALOG_AS_CLUSTER_NAME to False , and CLUSTER_KEY to what you wish to use as the cluster name. By default, the Snowflake database is set to PROD . To override this, set DATABASE_KEY to WhateverNameOfYourDb . By default, the Snowflake schema is set to INFORMATION_SCHEMA . To override this, set SCHEMA_KEY to WhateverNameOfYourSchema . Note that ACCOUNT_USAGE is a separate schema which allows users to query a wider set of data at the cost of latency. Differences are defined here The where_clause_suffix should define which schemas you\u2019d like to query (see the sample dag for an example). The SQL query driving the extraction is defined here job_config = ConfigFactory . from_dict ({ 'extractor.snowflake. {} ' . format ( SnowflakeMetadataExtractor . SNOWFLAKE_DATABASE_KEY ): 'YourDbName' , 'extractor.snowflake. {} ' . format ( SnowflakeMetadataExtractor . WHERE_CLAUSE_SUFFIX_KEY ): where_clause_suffix , 'extractor.snowflake. {} ' . format ( SnowflakeMetadataExtractor . USE_CATALOG_AS_CLUSTER_NAME ): True , 'extractor.snowflake.extractor.sqlalchemy. {} ' . format ( SQLAlchemyExtractor . CONN_STRING ): connection_string ()}) job = DefaultJob ( conf = job_config , task = DefaultTask ( extractor = SnowflakeMetadataExtractor (), loader = AnyLoader ())) job . launch () GenericUsageExtractor \u00b6 An extractor that extracts table popularity metadata from a custom created Snowflake table (created by a script that may look like this scala script ). You can create a DAG using the Databricks Operator and run this script within Databricks or wherever you are able to run Scala. By default, snowflake is used as the database name. ColumnReader has the datasource as its database input, and database as its cluster input. The following inputs are related to where you create your Snowflake popularity table. By default, the Snowflake popularity database is set to PROD . To override this, set POPULARITY_TABLE_DATABASE to WhateverNameOfYourDb . By default, the Snowflake popularity schema is set to SCHEMA . To override this, set POPULARTIY_TABLE_SCHEMA to WhateverNameOfYourSchema . By default, the Snowflake popularity table is set to TABLE . To override this, set POPULARITY_TABLE_NAME to WhateverNameOfYourTable . The where_clause_suffix should define any filtering you\u2019d like to include in your query. For example, this may include user_email s that you don\u2019t want to include in your popularity definition. job_config = ConfigFactory . from_dict ({ f 'extractor.snowflake.extractor.sqlalchemy. { SQLAlchemyExtractor . CONN_STRING } ' : connection_string (), f 'extractor.snowflake. { GenericUsageExtractor . WHERE_CLAUSE_SUFFIX_KEY } ' : where_clause_suffix , f 'extractor.snowflake. { GenericUsageExtractor . POPULARITY_TABLE_DATABASE } ' : 'WhateverNameOfYourDb' , f 'extractor.snowflake. { GenericUsageExtractor . POPULARTIY_TABLE_SCHEMA } ' : 'WhateverNameOfYourSchema' , f 'extractor.snowflake. { GenericUsageExtractor . POPULARITY_TABLE_NAME } ' : 'WhateverNameOfYourTable' , job = DefaultJob ( conf = job_config , task = DefaultTask ( extractor = GenericUsageExtractor (), loader = AnyLoader ())) job . launch () SnowflakeTableLastUpdatedExtractor \u00b6 An extractor that extracts table last updated timestamp from a Snowflake database. It uses same configs as the SnowflakeMetadataExtractor described above. The SQL query driving the extraction is defined here job_config = ConfigFactory . from_dict ({ 'extractor.snowflake_table_last_updated. {} ' . format ( SnowflakeTableLastUpdatedExtractor . SNOWFLAKE_DATABASE_KEY ): 'YourDbName' , 'extractor.snowflake_table_last_updated. {} ' . format ( SnowflakeTableLastUpdatedExtractor . WHERE_CLAUSE_SUFFIX_KEY ): where_clause_suffix , 'extractor.snowflake_table_last_updated. {} ' . format ( SnowflakeTableLastUpdatedExtractor . USE_CATALOG_AS_CLUSTER_NAME ): True , 'extractor.snowflake_table_last_updated.extractor.sqlalchemy. {} ' . format ( SQLAlchemyExtractor . CONN_STRING ): connection_string ()}) job = DefaultJob ( conf = job_config , task = DefaultTask ( extractor = SnowflakeTableLastUpdatedExtractor (), loader = AnyLoader ())) job . launch () BigQueryMetadataExtractor \u00b6 An extractor that extracts table and column metadata including database, schema, table name, table description, column name and column description from a Bigquery database. The API calls driving the extraction is defined here You will need to create a service account for reading metadata and grant it \u201cBigQuery Metadata Viewer\u201d access to all of your datasets. This can all be done via the bigquery ui. Download the creditials file and store it securely. Set the GOOGLE_APPLICATION_CREDENTIALS environment varible to the location of your credtials files and your code should have access to everything it needs. You can configure bigquery like this. You can optionally set a label filter if you only want to pull tables with a certain label. job_config = { 'extractor.bigquery_table_metadata. {} ' . format ( BigQueryMetadataExtractor . PROJECT_ID_KEY ): gcloud_project } if label_filter : job_config [ 'extractor.bigquery_table_metadata. {} ' . format ( BigQueryMetadataExtractor . FILTER_KEY ) ] = label_filter task = DefaultTask ( extractor = BigQueryMetadataExtractor (), loader = csv_loader , transformer = NoopTransformer ()) job = DefaultJob ( conf = ConfigFactory . from_dict ( job_config ), task = task , publisher = Neo4jCsvPublisher ()) job . launch () Neo4jEsLastUpdatedExtractor \u00b6 An extractor that basically get current timestamp and passes it GenericExtractor. This extractor is basically being used to create timestamp for \u201cAmundsen was last indexed on \u2026\u201d in Amundsen web page\u2019s footer. Neo4jExtractor \u00b6 An extractor that extracts records from Neo4j based on provided Cypher query . One example is to extract data from Neo4j so that it can transform and publish to Elasticsearch. job_config = ConfigFactory . from_dict ({ 'extractor.neo4j. {} ' . format ( Neo4jExtractor . CYPHER_QUERY_CONFIG_KEY ): cypher_query , 'extractor.neo4j. {} ' . format ( Neo4jExtractor . GRAPH_URL_CONFIG_KEY ): neo4j_endpoint , 'extractor.neo4j. {} ' . format ( Neo4jExtractor . MODEL_CLASS_CONFIG_KEY ): 'package.module.class_name' , 'extractor.neo4j. {} ' . format ( Neo4jExtractor . NEO4J_AUTH_USER ): neo4j_user , 'extractor.neo4j. {} ' . format ( Neo4jExtractor . NEO4J_AUTH_PW ): neo4j_password }, 'extractor.neo4j. {} ' . format ( Neo4jExtractor . NEO4J_ENCRYPTED ): True }) job = DefaultJob ( conf = job_config , task = DefaultTask ( extractor = Neo4jExtractor (), loader = AnyLoader ())) job . launch () Neo4jSearchDataExtractor \u00b6 An extractor that is extracting Neo4j utilizing Neo4jExtractor where CYPHER query is already embedded in it. job_config = ConfigFactory . from_dict ({ 'extractor.search_data.extractor.neo4j. {} ' . format ( Neo4jExtractor . GRAPH_URL_CONFIG_KEY ): neo4j_endpoint , 'extractor.search_data.extractor.neo4j. {} ' . format ( Neo4jExtractor . MODEL_CLASS_CONFIG_KEY ): 'databuilder.models.neo4j_data.Neo4jDataResult' , 'extractor.search_data.extractor.neo4j. {} ' . format ( Neo4jExtractor . NEO4J_AUTH_USER ): neo4j_user , 'extractor.search_data.extractor.neo4j. {} ' . format ( Neo4jExtractor . NEO4J_AUTH_PW ): neo4j_password }, 'extractor.search_data.extractor.neo4j. {} ' . format ( Neo4jExtractor . NEO4J_ENCRYPTED ): False }) job = DefaultJob ( conf = job_config , task = DefaultTask ( extractor = Neo4jSearchDataExtractor (), loader = AnyLoader ())) job . launch () AtlasSearchDataExtractor \u00b6 An extractor that is extracting Atlas Data to index compatible with Elasticsearch Search Proxy. entity_type = 'Table' extracted_search_data_path = f '/tmp/ { entity_type . lower () } _search_data.json' process_pool_size = 5 # atlas config atlas_url = 'localhost' atlas_port = 21000 atlas_protocol = 'http' atlas_verify_ssl = False atlas_username = 'admin' atlas_password = 'admin' atlas_search_chunk_size = 200 atlas_details_chunk_size = 10 # elastic config es = Elasticsearch ([ { 'host' : 'localhost' }, ]) elasticsearch_client = es elasticsearch_new_index_key = f ' { entity_type . lower () } -' + str ( uuid . uuid4 ()) elasticsearch_new_index_key_type = '_doc' elasticsearch_index_alias = f ' { entity_type . lower () } _search_index' job_config = ConfigFactory . from_dict ({ 'extractor.atlas_search_data. {} ' . format ( AtlasSearchDataExtractor . ATLAS_URL_CONFIG_KEY ): atlas_url , 'extractor.atlas_search_data. {} ' . format ( AtlasSearchDataExtractor . ATLAS_PORT_CONFIG_KEY ): atlas_port , 'extractor.atlas_search_data. {} ' . format ( AtlasSearchDataExtractor . ATLAS_PROTOCOL_CONFIG_KEY ): atlas_protocol , 'extractor.atlas_search_data. {} ' . format ( AtlasSearchDataExtractor . ATLAS_VALIDATE_SSL_CONFIG_KEY ): atlas_verify_ssl , 'extractor.atlas_search_data. {} ' . format ( AtlasSearchDataExtractor . ATLAS_USERNAME_CONFIG_KEY ): atlas_username , 'extractor.atlas_search_data. {} ' . format ( AtlasSearchDataExtractor . ATLAS_PASSWORD_CONFIG_KEY ): atlas_password , 'extractor.atlas_search_data. {} ' . format ( AtlasSearchDataExtractor . ATLAS_SEARCH_CHUNK_SIZE_KEY ): atlas_search_chunk_size , 'extractor.atlas_search_data. {} ' . format ( AtlasSearchDataExtractor . ATLAS_DETAILS_CHUNK_SIZE_KEY ): atlas_details_chunk_size , 'extractor.atlas_search_data. {} ' . format ( AtlasSearchDataExtractor . PROCESS_POOL_SIZE_KEY ): process_pool_size , 'extractor.atlas_search_data. {} ' . format ( AtlasSearchDataExtractor . ENTITY_TYPE_KEY ): entity_type , 'loader.filesystem.elasticsearch. {} ' . format ( FSElasticsearchJSONLoader . FILE_PATH_CONFIG_KEY ): extracted_search_data_path , 'loader.filesystem.elasticsearch. {} ' . format ( FSElasticsearchJSONLoader . FILE_MODE_CONFIG_KEY ): 'w' , 'publisher.elasticsearch. {} ' . format ( ElasticsearchPublisher . FILE_PATH_CONFIG_KEY ): extracted_search_data_path , 'publisher.elasticsearch. {} ' . format ( ElasticsearchPublisher . FILE_MODE_CONFIG_KEY ): 'r' , 'publisher.elasticsearch. {} ' . format ( ElasticsearchPublisher . ELASTICSEARCH_CLIENT_CONFIG_KEY ): elasticsearch_client , 'publisher.elasticsearch. {} ' . format ( ElasticsearchPublisher . ELASTICSEARCH_NEW_INDEX_CONFIG_KEY ): elasticsearch_new_index_key , 'publisher.elasticsearch. {} ' . format ( ElasticsearchPublisher . ELASTICSEARCH_DOC_TYPE_CONFIG_KEY ): elasticsearch_new_index_key_type , 'publisher.elasticsearch. {} ' . format ( ElasticsearchPublisher . ELASTICSEARCH_ALIAS_CONFIG_KEY ): elasticsearch_index_alias }) if __name__ == \"__main__\" : task = DefaultTask ( extractor = AtlasSearchDataExtractor (), transformer = NoopTransformer (), loader = FSElasticsearchJSONLoader ()) job = DefaultJob ( conf = job_config , task = task , publisher = ElasticsearchPublisher ()) job . launch () VerticaMetadataExtractor \u00b6 An extractor that extracts table and column metadata including database, schema, table name, column name and column datatype from a Vertica database. A sample loading script for Vertica is provided here By default, the Vertica database name is used as the cluster name. The where_clause_suffix in the example can be used to define which schemas you would like to query. SQLAlchemyExtractor \u00b6 An extractor utilizes SQLAlchemy to extract record from any database that support SQL Alchemy. job_config = ConfigFactory . from_dict ({ 'extractor.sqlalchemy. {} ' . format ( SQLAlchemyExtractor . CONN_STRING ): connection_string (), 'extractor.sqlalchemy. {} ' . format ( SQLAlchemyExtractor . EXTRACT_SQL ): sql , 'extractor.sqlalchemy.model_class' : 'package.module.class_name' }) job = DefaultJob ( conf = job_config , task = DefaultTask ( extractor = SQLAlchemyExtractor (), loader = AnyLoader ())) job . launch () DbtExtractor \u00b6 This extractor utilizes the dbt output files catalog.json and manifest.json to extract metadata and ingest it into Amundsen. The catalog.json and manifest.json can both be generated by running dbt docs generate in your dbt project. Visit the dbt artifacts page for more information. The DbtExtractor can currently create the following: Tables and their definitions Columns and their definitions Table level lineage dbt tags (as Amundsen badges or tags) Table Sources (e.g. link to GitHib where the dbt template resides) job_config = ConfigFactory . from_dict ({ # Required args f 'extractor.dbt. { DbtExtractor . DATABASE_NAME } ' : 'snowflake' , f 'extractor.dbt. { DbtExtractor . MANIFEST_JSON } ' : catalog_file_loc , # File location f 'extractor.dbt. { DbtExtractor . DATABASE_NAME } ' : json . dumps ( manifest_data ), # JSON Dumped object # Optional args f 'extractor.dbt. { DbtExtractor . SOURCE_URL } ' : 'https://github.com/your-company/your-repo/tree/main' , f 'extractor.dbt. { DbtExtractor . EXTRACT_TABLES } ' : True , f 'extractor.dbt. { DbtExtractor . EXTRACT_DESCRIPTIONS } ' : True , f 'extractor.dbt. { DbtExtractor . EXTRACT_TAGS } ' : True , f 'extractor.dbt. { DbtExtractor . IMPORT_TAGS_AS } ' : 'badges' , f 'extractor.dbt. { DbtExtractor . EXTRACT_LINEAGE } ' : True , }) job = DefaultJob ( conf = job_config , task = DefaultTask ( extractor = DbtExtractor (), loader = AnyLoader ())) job . launch () RestAPIExtractor \u00b6 A extractor that utilizes RestAPIQuery to extract data. RestAPIQuery needs to be constructed ( example ) and needs to be injected to RestAPIExtractor. Mode Dashboard Extractor \u00b6 Here are extractors that extracts metadata information from Mode via Mode\u2019s REST API. Prerequisite: You will need to create API access token that has admin privilege. You will need organization code. This is something you can easily get by looking at one of Mode report\u2019s URL. https://app.mode.com/<organization code>/reports/report_token ModeDashboardExtractor \u00b6 A Extractor that extracts core metadata on Mode dashboard. https://app.mode.com/ It extracts list of reports that consists of: Dashboard group name (Space name) Dashboard group id (Space token) Dashboard group description (Space description) Dashboard name (Report name) Dashboard id (Report token) Dashboard description (Report description) Other information such as report run, owner, chart name, query name is in separate extractor. It calls two APIs ( spaces API and reports API ) joining together. You can create Databuilder job config like this. task = DefaultTask ( extractor = ModeDashboardExtractor (), loader = FsNeo4jCSVLoader (), ) tmp_folder = '/var/tmp/amundsen/mode_dashboard_metadata' node_files_folder = ' {tmp_folder} /nodes' . format ( tmp_folder = tmp_folder ) relationship_files_folder = ' {tmp_folder} /relationships' . format ( tmp_folder = tmp_folder ) job_config = ConfigFactory . from_dict ({ 'extractor.mode_dashboard. {} ' . format ( ORGANIZATION ): organization , 'extractor.mode_dashboard. {} ' . format ( MODE_BEARER_TOKEN ): mode_bearer_token , 'extractor.mode_dashboard. {} ' . format ( DASHBOARD_GROUP_IDS_TO_SKIP ): [ space_token_1 , space_token_2 , ... ], 'loader.filesystem_csv_neo4j. {} ' . format ( FsNeo4jCSVLoader . NODE_DIR_PATH ): node_files_folder , 'loader.filesystem_csv_neo4j. {} ' . format ( FsNeo4jCSVLoader . RELATION_DIR_PATH ): relationship_files_folder , 'loader.filesystem_csv_neo4j. {} ' . format ( FsNeo4jCSVLoader . SHOULD_DELETE_CREATED_DIR ): True , 'task.progress_report_frequency' : 100 , 'publisher.neo4j. {} ' . format ( neo4j_csv_publisher . NODE_FILES_DIR ): node_files_folder , 'publisher.neo4j. {} ' . format ( neo4j_csv_publisher . RELATION_FILES_DIR ): relationship_files_folder , 'publisher.neo4j. {} ' . format ( neo4j_csv_publisher . NEO4J_END_POINT_KEY ): neo4j_endpoint , 'publisher.neo4j. {} ' . format ( neo4j_csv_publisher . NEO4J_USER ): neo4j_user , 'publisher.neo4j. {} ' . format ( neo4j_csv_publisher . NEO4J_PASSWORD ): neo4j_password , 'publisher.neo4j. {} ' . format ( neo4j_csv_publisher . NEO4J_ENCRYPTED ): True , 'publisher.neo4j. {} ' . format ( neo4j_csv_publisher . NEO4J_CREATE_ONLY_NODES ): [ DESCRIPTION_NODE_LABEL ], 'publisher.neo4j. {} ' . format ( neo4j_csv_publisher . JOB_PUBLISH_TAG ): job_publish_tag }) job = DefaultJob ( conf = job_config , task = task , publisher = Neo4jCsvPublisher ()) job . launch () ModeDashboardOwnerExtractor \u00b6 An Extractor that extracts Dashboard owner. Mode itself does not have concept of owner and it will use creator as owner. Note that if user left the organization, it would skip the dashboard. You can create Databuilder job config like this. (configuration related to loader and publisher is omitted as it is mostly the same. Please take a look at this example for the configuration that holds loader and publisher. extractor = ModeDashboardOwnerExtractor () task = DefaultTask ( extractor = extractor , loader = FsNeo4jCSVLoader (), ) job_config = ConfigFactory . from_dict ({ ' {} . {} ' . format ( extractor . get_scope (), ORGANIZATION ): organization , ' {} . {} ' . format ( extractor . get_scope (), MODE_BEARER_TOKEN ): mode_bearer_token , }) job = DefaultJob ( conf = job_config , task = task , publisher = Neo4jCsvPublisher ()) job . launch () ModeDashboardLastSuccessfulExecutionExtractor \u00b6 A Extractor that extracts Mode dashboard\u2019s last successful run (execution) timestamp. You can create Databuilder job config like this. (configuration related to loader and publisher is omitted as it is mostly the same. Please take a look at this example for the configuration that holds loader and publisher. extractor = ModeDashboardLastSuccessfulExecutionExtractor () task = DefaultTask ( extractor = extractor , loader = FsNeo4jCSVLoader ()) job_config = ConfigFactory . from_dict ({ ' {} . {} ' . format ( extractor . get_scope (), ORGANIZATION ): organization , ' {} . {} ' . format ( extractor . get_scope (), MODE_BEARER_TOKEN ): mode_bearer_token , }) job = DefaultJob ( conf = job_config , task = task , publisher = Neo4jCsvPublisher ()) job . launch () ModeDashboardExecutionsExtractor \u00b6 A Extractor that extracts last run (execution) status and timestamp. You can create Databuilder job config like this. (configuration related to loader and publisher is omitted as it is mostly the same. Please take a look at this example for the configuration that holds loader and publisher. extractor = ModeDashboardExecutionsExtractor () task = DefaultTask ( extractor = extractor , loader = FsNeo4jCSVLoader ()) job_config = ConfigFactory . from_dict ({ ' {} . {} ' . format ( extractor . get_scope (), ORGANIZATION ): organization , ' {} . {} ' . format ( extractor . get_scope (), MODE_BEARER_TOKEN ): mode_bearer_token , }) job = DefaultJob ( conf = job_config , task = task , publisher = Neo4jCsvPublisher ()) job . launch () ModeDashboardLastModifiedTimestampExtractor \u00b6 A Extractor that extracts Mode dashboard\u2019s last modified timestamp. You can create Databuilder job config like this. (configuration related to loader and publisher is omitted as it is mostly the same. Please take a look at this example for the configuration that holds loader and publisher. extractor = ModeDashboardLastModifiedTimestampExtractor () task = DefaultTask ( extractor = extractor , loader = FsNeo4jCSVLoader ()) job_config = ConfigFactory . from_dict ({ ' {} . {} ' . format ( extractor . get_scope (), ORGANIZATION ): organization , ' {} . {} ' . format ( extractor . get_scope (), MODE_BEARER_TOKEN ): mode_bearer_token , }) job = DefaultJob ( conf = job_config , task = task , publisher = Neo4jCsvPublisher ()) job . launch () ModeDashboardQueriesExtractor \u00b6 A Extractor that extracts Mode\u2019s query information. You can create Databuilder job config like this. (configuration related to loader and publisher is omitted as it is mostly the same. Please take a look at this example for the configuration that holds loader and publisher. extractor = ModeDashboardQueriesExtractor () task = DefaultTask ( extractor = extractor , loader = FsNeo4jCSVLoader ()) job_config = ConfigFactory . from_dict ({ ' {} . {} ' . format ( extractor . get_scope (), ORGANIZATION ): organization , ' {} . {} ' . format ( extractor . get_scope (), MODE_BEARER_TOKEN ): mode_bearer_token , }) job = DefaultJob ( conf = job_config , task = task , publisher = Neo4jCsvPublisher ()) job . launch () ModeDashboardChartsBatchExtractor \u00b6 A Extractor that extracts Mode Dashboard charts metadata. You can create Databuilder job config like this. (configuration related to loader and publisher is omitted as it is mostly the same. Please take a look at this example for the configuration that holds loader and publisher. extractor = ModeDashboardChartsBatchExtractor () task = DefaultTask ( extractor = extractor , loader = FsNeo4jCSVLoader ()) job_config = ConfigFactory . from_dict ({ ' {} . {} ' . format ( extractor . get_scope (), ORGANIZATION ): organization , ' {} . {} ' . format ( extractor . get_scope (), MODE_BEARER_TOKEN ): mode_bearer_token , }) job = DefaultJob ( conf = job_config , task = task , publisher = Neo4jCsvPublisher ()) job . launch () ModeDashboardUserExtractor \u00b6 A Extractor that extracts Mode user_id and then update User node. You can create Databuilder job config like this. (configuration related to loader and publisher is omitted as it is mostly the same. Please take a look at this example for the configuration that holds loader and publisher. extractor = ModeDashboardUserExtractor () task = DefaultTask ( extractor = extractor , loader = FsNeo4jCSVLoader ()) job_config = ConfigFactory . from_dict ({ ' {} . {} ' . format ( extractor . get_scope (), ORGANIZATION ): organization , ' {} . {} ' . format ( extractor . get_scope (), MODE_ACCESS_TOKEN ): mode_token , ' {} . {} ' . format ( extractor . get_scope (), MODE_PASSWORD_TOKEN ): mode_password , }) job = DefaultJob ( conf = job_config , task = task , publisher = Neo4jCsvPublisher ()) job . launch () ModeDashboardUsageExtractor \u00b6 A Extractor that extracts Mode dashboard\u2019s accumulated view count. Note that this provides accumulated view count which does not effectively show relevancy . Thus, fields from this extractor is not directly compatible with DashboardUsage model. If you are fine with accumulated usage , you could use TemplateVariableSubstitutionTransformer to transform Dict payload from ModeDashboardUsageExtractor to fit DashboardUsage and transform Dict to DashboardUsage by TemplateVariableSubstitutionTransformer , and DictToModel transformers. ( Example on how to combining these two transformers) RedashDashboardExtractor \u00b6 The included RedashDashboardExtractor provides support for extracting basic metadata for Redash dashboards (dashboard name, owner, URL, created/updated timestamps, and a generated description) and their associated queries (query name, URL, and raw query). It can be extended with a configurable table parser function to also support extraction of DashboardTable metadata. (See below for example usage.) Note: DashboardUsage and DashboardExecution metadata are not supported in this extractor, as these concepts are not supported by the Redash API. The RedashDashboardExtractor depends on the following Redash API endpoints: GET /api/dashboards , GET /api/dashboards/<dashboard-slug> . It has been tested against Redash 8 and is also expected to work with Redash 9. extractor = RedashDashboardExtractor () task = DefaultTask ( extractor = extractor , loader = FsNeo4jCSVLoader ()) job_config = ConfigFactory . from_dict ({ 'extractor.redash_dashboard.redash_base_url' : redash_base_url , # ex: https://redash.example.org 'extractor.redash_dashboard.api_base_url' : api_base_url , # ex: https://redash.example.org/api 'extractor.redash_dashboard.api_key' : api_key , # ex: abc1234 'extractor.redash_dashboard.table_parser' : table_parser , # ex: my_library.module.parse_tables 'extractor.redash_dashboard.redash_version' : redash_version # ex: 8. optional, default=9 }) job = DefaultJob ( conf = job_config , task = task , publisher = Neo4jCsvPublisher ()) job . launch () RedashDashboardExtractor: table_parser \u00b6 The RedashDashboardExtractor extracts raw queries from each dashboard. You may optionally use these queries to parse out relations to tables in Amundsen. A table parser can be provided in the configuration for the RedashDashboardExtractor , as seen above. This function should have type signature (RedashVisualizationWidget) -> Iterator[TableRelationData] . For example: def parse_tables ( viz_widget : RedashVisualizationWidget ) -> Iterator [ TableRelationData ]: # Each viz_widget corresponds to one query. # viz_widget.data_source_id is the ID of the target DB in Redash. # viz_widget.raw_query is the raw query (e.g., SQL). if viz_widget . data_source_id == 123 : table_names = some_sql_parser ( viz_widget . raw_query ) return [ TableRelationData ( 'some_db' , 'prod' , 'some_schema' , tbl ) for tbl in table_names ] return [] TableauDashboardExtractor \u00b6 The included TableauDashboardExtractor provides support for extracting basic metadata for Tableau workbooks. All Tableau extractors including this one use the Tableau Metadata GraphQL API to gather the metadata. Tableau \u201cworkbooks\u201d are mapped to Amundsen dashboards, and the top-level project in which these workbooks preside is the dashboard group. The metadata it gathers is as follows: - Dashboard name (Workbook name) - Dashboard description (Workbook description) - Dashboard creation timestamp (Workbook creation timestamp) - Dashboard group name (Workbook top-level folder name) - Dashboard and dashboard group URL If you wish to exclude top-level projects from being loaded, specify their names in the tableau_excluded_projects list and workbooks from any of those projects will not be indexed. Tableau\u2019s concept of \u201cowners\u201d does not map cleanly into Amundsen\u2019s understanding of owners, as the owner of a Tableau workbook is simply whoever updated it last, even if they made a very small change. This can prove problematic in determining the true point of contact for a workbook, so it\u2019s simply omitted for now. Similarly, the hierachy of dashboard/query/chart in Amundsen does not map into Tableau, where charts have only an optional relation to queries and vice versa. For these reasons, there are not extractors for either entity. The Tableau Metadata API also does not support usage or execution statistics, so there are no extractors for these entities either. Sample job config: extractor = TableauDashboardExtractor () task = DefaultTask ( extractor = extractor , loader = FsNeo4jCSVLoader ()) job_config = ConfigFactory . from_dict ({ 'extractor.tableau_dashboard_metadata.tableau_host' : tableau_host , 'extractor.tableau_dashboard_metadata.api_version' : tableau_api_version , 'extractor.tableau_dashboard_metadata.site_name' : tableau_site_name , 'extractor.tableau_dashboard_metadata.tableau_personal_access_token_name' : tableau_personal_access_token_name , 'extractor.tableau_dashboard_metadata.tableau_personal_access_token_secret' : tableau_personal_access_token_secret , 'extractor.tableau_dashboard_metadata.excluded_projects' : tableau_excluded_projects , 'extractor.tableau_dashboard_metadata.cluster' : tableau_dashboard_cluster , 'extractor.tableau_dashboard_metadata.database' : tableau_dashboard_database , 'extractor.tableau_dashboard_metadata.transformer.timestamp_str_to_epoch.timestamp_format' : \"%Y-%m- %d T%H:%M:%SZ\" , }) job = DefaultJob ( conf = job_config , task = task , publisher = Neo4jCsvPublisher ()) job . launch () TableauDashboardTableExtractor \u00b6 The included TableauDashboardTableExtractor provides support for extracting table metadata from Tableau workbooks. The extractor assumes all the table entities have already been created; if you are interested in using the provided TableauExternalTableExtractor , make sure that job runs before this one, as it will create the tables required by this job. It also assumes that the dashboards are using their names as the primary ID. A sample job config is shown below. Configuration related to the loader and publisher is omitted as it is mostly the same. Please take a look at this example for the configuration that holds loader and publisher. extractor = TableauDashboardTableExtractor () task = DefaultTask ( extractor = extractor , loader = FsNeo4jCSVLoader ()) job_config = ConfigFactory . from_dict ({ 'extractor.tableau_dashboard_table.tableau_host' : tableau_host , 'extractor.tableau_dashboard_table.api_version' : tableau_api_version , 'extractor.tableau_dashboard_table.site_name' : tableau_site_name , 'extractor.tableau_dashboard_table.tableau_personal_access_token_name' : tableau_personal_access_token_name , 'extractor.tableau_dashboard_table.tableau_personal_access_token_secret' : tableau_personal_access_token_secret , 'extractor.tableau_dashboard_table.excluded_projects' : tableau_excluded_projects , 'extractor.tableau_dashboard_table.cluster' : tableau_dashboard_cluster , 'extractor.tableau_dashboard_table.database' : tableau_dashboard_database , 'extractor.tableau_dashboard_table.external_cluster_name' : tableau_external_table_cluster , 'extractor.tableau_dashboard_table.external_schema_name' : tableau_external_table_schema , }) job = DefaultJob ( conf = job_config , task = task , publisher = Neo4jCsvPublisher ()) job . launch () TableauDashboardQueryExtractor \u00b6 The included TableauDashboardQueryExtractor provides support for extracting query metadata from Tableau workbooks. It retrives the name and query text for each custom SQL query. A sample job config is shown below. Configuration related to the loader and publisher is omitted as it is mostly the same. Please take a look at this example for the configuration that holds loader and publisher. extractor = TableauDashboardQueryExtractor () task = DefaultTask ( extractor = extractor , loader = FsNeo4jCSVLoader ()) job_config = ConfigFactory . from_dict ({ 'extractor.tableau_dashboard_query.tableau_host' : tableau_host , 'extractor.tableau_dashboard_query.api_version' : tableau_api_version , 'extractor.tableau_dashboard_query.site_name' : tableau_site_name , 'extractor.tableau_dashboard_query.tableau_personal_access_token_name' : tableau_personal_access_token_name , 'extractor.tableau_dashboard_query.tableau_personal_access_token_secret' : tableau_personal_access_token_secret , 'extractor.tableau_dashboard_query.excluded_projects' : tableau_excluded_projects , 'extractor.tableau_dashboard_query.cluster' : tableau_dashboard_cluster , 'extractor.tableau_dashboard_query.database' : tableau_dashboard_database , }) job = DefaultJob ( conf = job_config , task = task , publisher = Neo4jCsvPublisher ()) job . launch () TableauDashboardLastModifiedExtractor \u00b6 The included TableauDashboardLastModifiedExtractor provides support for extracting the last updated timestamp for Tableau workbooks. A sample job config is shown below. Configuration related to the loader and publisher is omitted as it is mostly the same. Please take a look at this example for the configuration that holds loader and publisher. extractor = TableauDashboardQueryExtractor () task = DefaultTask ( extractor = extractor , loader = FsNeo4jCSVLoader ()) job_config = ConfigFactory . from_dict ({ 'extractor.tableau_dashboard_last_modified.tableau_host' : tableau_host , 'extractor.tableau_dashboard_last_modified.api_version' : tableau_api_version , 'extractor.tableau_dashboard_last_modified.site_name' : tableau_site_name , 'extractor.tableau_dashboard_last_modified.tableau_personal_access_token_name' : tableau_personal_access_token_name , 'extractor.tableau_dashboard_last_modified.tableau_personal_access_token_secret' : tableau_personal_access_token_secret , 'extractor.tableau_dashboard_last_modified.excluded_projects' : tableau_excluded_projects , 'extractor.tableau_dashboard_last_modified.cluster' : tableau_dashboard_cluster , 'extractor.tableau_dashboard_last_modified.database' : tableau_dashboard_database , 'extractor.tableau_dashboard_last_modified.transformer.timestamp_str_to_epoch.timestamp_format' : \"%Y-%m- %d T%H:%M:%SZ\" , }) job = DefaultJob ( conf = job_config , task = task , publisher = Neo4jCsvPublisher ()) job . launch () TableauExternalTableExtractor \u00b6 The included TableauExternalTableExtractor provides support for extracting external table entities referenced by Tableau workbooks. In this context, \u201cexternal\u201d tables are \u201ctables\u201d that are not from a typical database, and are loaded using some other data format, like CSV files. This extractor has been tested with the following types of external tables; feel free to add others, but it\u2019s recommended to test them in a non-production instance first to be safe. - Excel spreadsheets - Text files (including CSV files) - Salesforce connections - Google Sheets connections Use the external_table_types list config option to specify which external connection types you would like to index; refer to your Tableau instance for the exact formatting of each connection type string. Excel spreadsheets, Salesforce connections, and Google Sheets connections are all classified as \u201cdatabases\u201d in terms of Tableau\u2019s Metadata API, with their \u201csubsheets\u201d forming their \u201ctables\u201d when present. However, these tables are not assigned a schema, this extractor chooses to use the name of the parent sheet as the schema, and assign a new table to each subsheet. The connection type is always used as the database, and for text files, the schema is set using the external_schema_name config option. Since these external tables are usually named for human consumption only and often contain a wider range of characters, all inputs are sanitized to remove any problematic occurences before they are inserted: see the sanitize methods TableauDashboardUtils for specifics. A more concrete example: if one had a Google Sheet titled \u201cGrowth by Region\u201d with 2 subsheets called \u201cFY19 Report\u201d and \u201cFY20 Report\u201d, two tables would be generated with the following keys: googlesheets://external.growth_by_region/FY_19_Report googlesheets://external.growth_by_region/FY_20_Report A sample job config is shown below. Configuration related to the loader and publisher is omitted as it is mostly the same. Please take a look at this example for the configuration that holds loader and publisher. extractor = TableauExternalTableExtractor () task = DefaultTask ( extractor = extractor , loader = FsNeo4jCSVLoader ()) job_config = ConfigFactory . from_dict ({ 'extractor.tableau_external_table.tableau_host' : tableau_host , 'extractor.tableau_external_table.api_version' : tableau_api_version , 'extractor.tableau_external_table.site_name' : tableau_site_name , 'extractor.tableau_external_table.tableau_personal_access_token_name' : tableau_personal_access_token_name , 'extractor.tableau_external_table.tableau_personal_access_token_secret' : tableau_personal_access_token_secret , 'extractor.tableau_external_table.excluded_projects' : tableau_excluded_projects , 'extractor.tableau_external_table.cluster' : tableau_dashboard_cluster , 'extractor.tableau_external_table.database' : tableau_dashboard_database , 'extractor.tableau_external_table.external_cluster_name' : tableau_external_table_cluster , 'extractor.tableau_external_table.external_schema_name' : tableau_external_table_schema , 'extractor.tableau_external_table.external_table_types' : tableau_external_table_types }) job = DefaultJob ( conf = job_config , task = task , publisher = Neo4jCsvPublisher ()) job . launch () ApacheSupersetMetadataExtractor \u00b6 The included ApacheSupersetMetadataExtractor provides support for extracting basic metadata for Apache Superset dashboards. All Apache Superset extractors including this one use Apache Superset REST API ( /api/v1 ) and were developed based on Apache Superset version 1.1 . Caution! \u00b6 Apache Superset does not contain metadata fulfilling the concept of DashboardGroup . For that reasons, when configuring extractor following parameters must be provided: - dashboard_group_id (required) - dashboard_group_name (required) - cluster (required) - dashboard_group_description (optional) DashboardMetadata \u00b6 ApacheSupersetMetadataExtractor extracts metadata into DashboardMetadata model. Metadata available in REST API \u00b6 Dashboard id (id) Dashboard name (dashboard_title) Dashboard URL (url) Metadata not available in Apache Superset REST API \u00b6 Dashboard description Dashboard creation timestamp DashboardLastModifiedTimestamp \u00b6 ApacheSupersetLastModifiedTimestampExtractor extracts metadata into DashboardLastModifiedTimestamp model. Available in REST API \u00b6 Dashboard last modified timestamp (changed_on property of dashboard) Caution! \u00b6 changed_on value does not provide timezone info so we assume it\u2019s UTC. Sample job config \u00b6 tmp_folder = f '/tmp/amundsen/dashboard' dict_config = { f 'loader.filesystem_csv_neo4j. { FsNeo4jCSVLoader . NODE_DIR_PATH } ' : f ' { tmp_folder } /nodes' , f 'loader.filesystem_csv_neo4j. { FsNeo4jCSVLoader . RELATION_DIR_PATH } ' : f ' { tmp_folder } /relationships' , f 'loader.filesystem_csv_neo4j. { FsNeo4jCSVLoader . SHOULD_DELETE_CREATED_DIR } ' : True , f 'extractor.apache_superset. { ApacheSupersetBaseExtractor . DASHBOARD_GROUP_ID } ' : '1' , f 'extractor.apache_superset. { ApacheSupersetBaseExtractor . DASHBOARD_GROUP_NAME } ' : 'dashboard group' , f 'extractor.apache_superset. { ApacheSupersetBaseExtractor . DASHBOARD_GROUP_DESCRIPTION } ' : 'dashboard group description' , f 'extractor.apache_superset. { ApacheSupersetBaseExtractor . CLUSTER } ' : 'gold' , f 'extractor.apache_superset. { ApacheSupersetBaseExtractor . APACHE_SUPERSET_SECURITY_SETTINGS_DICT } ' : dict ( username = 'admin' , password = 'admin' , provider = 'db' ) } job_config = ConfigFactory . from_dict ( dict_config ) task = DefaultTask ( extractor = ApacheSupersetMetadataExtractor (), loader = FsNeo4jCSVLoader ()) job = DefaultJob ( conf = job_config , task = task ) job . launch () ApacheSupersetTableExtractor \u00b6 The included ApacheSupersetTableExtractor provides support for extracting relationships between dashboards and tables. All Apache Superset extractors including this one use Apache Superset REST API ( api/v1 ). Caution! \u00b6 As table information in Apache Superset is minimal, following configuration options enable parametrization required to achieve proper relationship information: - driver_to_database_mapping - mapping between sqlalchemy drivername and actual database property of TableMetadata model. - database_to_cluster_mapping - mapping between Apache Superset Database ID and cluster from TableMedata model (defaults to cluster config of extractor.apache_superset ) DashboardTable \u00b6 Metadata available in REST API \u00b6 Table keys Sample job config \u00b6 tmp_folder = f '/tmp/amundsen/dashboard' dict_config = { f 'loader.filesystem_csv_neo4j. { FsNeo4jCSVLoader . NODE_DIR_PATH } ' : f ' { tmp_folder } /nodes' , f 'loader.filesystem_csv_neo4j. { FsNeo4jCSVLoader . RELATION_DIR_PATH } ' : f ' { tmp_folder } /relationships' , f 'loader.filesystem_csv_neo4j. { FsNeo4jCSVLoader . SHOULD_DELETE_CREATED_DIR } ' : True , f 'extractor.apache_superset. { ApacheSupersetBaseExtractor . DASHBOARD_GROUP_ID } ' : '1' , f 'extractor.apache_superset. { ApacheSupersetBaseExtractor . DASHBOARD_GROUP_NAME } ' : 'dashboard group' , f 'extractor.apache_superset. { ApacheSupersetBaseExtractor . DASHBOARD_GROUP_DESCRIPTION } ' : 'dashboard group description' , f 'extractor.apache_superset. { ApacheSupersetBaseExtractor . CLUSTER } ' : 'gold' , f 'extractor.apache_superset. { ApacheSupersetBaseExtractor . APACHE_SUPERSET_SECURITY_SETTINGS_DICT } ' : dict ( username = 'admin' , password = 'admin' , provider = 'db' ) } job_config = ConfigFactory . from_dict ( dict_config ) task = DefaultTask ( extractor = ApacheSupersetTableExtractor (), loader = FsNeo4jCSVLoader ()) job = DefaultJob ( conf = job_config , task = task ) job . launch () ApacheSupersetChartExtractor \u00b6 The included ApacheSupersetChartExtractor provides support for extracting information on charts connected to given dashboard. Caution! \u00b6 Currently there is no way to connect Apache Superset Query model to neither Chart nor Dashboard model. For that reason, to comply with Amundsen Databuilder data model, we register single DashboardQuery node serving as a bridge to which all the DashboardChart nodes are connected. DashboardChart \u00b6 Metadata available in REST API \u00b6 Chart id (id) Chart name (chart_name) Chart type (viz_type) Metadata not available in REST API \u00b6 Chart url Sample job config \u00b6 tmp_folder = f '/tmp/amundsen/dashboard' dict_config = { f 'loader.filesystem_csv_neo4j. { FsNeo4jCSVLoader . NODE_DIR_PATH } ' : f ' { tmp_folder } /nodes' , f 'loader.filesystem_csv_neo4j. { FsNeo4jCSVLoader . RELATION_DIR_PATH } ' : f ' { tmp_folder } /relationships' , f 'loader.filesystem_csv_neo4j. { FsNeo4jCSVLoader . SHOULD_DELETE_CREATED_DIR } ' : True , f 'extractor.apache_superset. { ApacheSupersetBaseExtractor . DASHBOARD_GROUP_ID } ' : '1' , f 'extractor.apache_superset. { ApacheSupersetBaseExtractor . DASHBOARD_GROUP_NAME } ' : 'dashboard group' , f 'extractor.apache_superset. { ApacheSupersetBaseExtractor . DASHBOARD_GROUP_DESCRIPTION } ' : 'dashboard group description' , f 'extractor.apache_superset. { ApacheSupersetBaseExtractor . CLUSTER } ' : 'gold' , f 'extractor.apache_superset. { ApacheSupersetBaseExtractor . APACHE_SUPERSET_SECURITY_SETTINGS_DICT } ' : dict ( username = 'admin' , password = 'admin' , provider = 'db' ) } job_config = ConfigFactory . from_dict ( dict_config ) task = DefaultTask ( extractor = ApacheSupersetChartExtractor (), loader = FsNeo4jCSVLoader ()) job = DefaultJob ( conf = job_config , task = task ) job . launch () PandasProfilingColumnStatsExtractor \u00b6 Pandas profiling is a library commonly used by Data Engineer and Scientists to calculate advanced data profiles on data. It is run on pandas dataframe and results in json file containing (amongst other things) descriptive and quantile statistics on columns. Required input parameters \u00b6 FILE_PATH - file path to pandas-profiling json report TABLE_NAME - name of the table for which report was calculated SCHEMA_NAME - name of the schema from which table originates DATABASE_NAME - name of database technology from which table originates CLUSTER_NAME - name of the cluster from which table originates Optional input parameters \u00b6 PRECISION - precision for metrics of float type. Defaults to 3 meaning up to 3 digits after decimal point. STAT_MAPPINGS - if you wish to collect only selected set of metrics configure this option with dictionary of following format: key - raw name of the stat in pandas-profiling value - tuple of 2 elements: first value of the tuple - full name of the stat (this influences what will be rendered for user in UI) second value of the tuple - function modifying the stat (by default we just do type casting) Such dictionary should in that case contain only keys of metrics you wish to collect. For example - if you want only min and max value of a column, provide extractor with configuration option: PandasProfilingColumnStatsExtractor . STAT_MAPPINGS = { 'max' : ( 'Maximum' , float ), 'min' : ( 'Minimum' , float )} Complete set of available metrics is defined as DEFAULT_STAT_MAPPINGS attribute of PandasProfilingColumnStatsExtractor. Common usage patterns \u00b6 As pandas profiling is executed on top of pandas dataframe, it is up to the user to populate the dataframe before running the report calculation (and subsequently the extractor). While doing so remember that it might not be a good idea to run the report on a complete set of rows if your tables are very sparse. In such case it is recommended to dump a subset of rows to pandas dataframe beforehand and calculate the report on just a sample of original data. Spark support \u00b6 Support for native execution of pandas-profiling on Spark Dataframe is currently worked on and should come in the future. Sample job config \u00b6 import pandas as pd import pandas_profiling from pyhocon import ConfigFactory from sqlalchemy import create_engine from databuilder.extractor.pandas_profiling_column_stats_extractor import PandasProfilingColumnStatsExtractor from databuilder.job.job import DefaultJob from databuilder.loader.file_system_neo4j_csv_loader import FsNeo4jCSVLoader from databuilder.task.task import DefaultTask table_name = 'video_game_sales' schema_name = 'superset' # Load table contents to pandas dataframe db_uri = f 'postgresql://superset:superset@localhost:5432/ { schema_name } ' engine = create_engine ( db_uri , echo = True ) df = pd . read_sql_table ( table_name , con = engine ) # Calculate pandas-profiling report on a table report_file = '/tmp/table_report.json' report = df . profile_report ( sort = None ) report . to_file ( report_file ) # Run PandasProfilingColumnStatsExtractor on calculated report tmp_folder = f '/tmp/amundsen/column_stats' dict_config = { f 'loader.filesystem_csv_neo4j. { FsNeo4jCSVLoader . NODE_DIR_PATH } ' : f ' { tmp_folder } /nodes' , f 'loader.filesystem_csv_neo4j. { FsNeo4jCSVLoader . RELATION_DIR_PATH } ' : f ' { tmp_folder } /relationships' , f 'loader.filesystem_csv_neo4j. { FsNeo4jCSVLoader . SHOULD_DELETE_CREATED_DIR } ' : False , 'extractor.pandas_profiling.table_name' : table_name , 'extractor.pandas_profiling.schema_name' : schema_name , 'extractor.pandas_profiling.database_name' : 'postgres' , 'extractor.pandas_profiling.cluster_name' : 'dev' , 'extractor.pandas_profiling.file_path' : report_file } job_config = ConfigFactory . from_dict ( dict_config ) task = DefaultTask ( extractor = PandasProfilingColumnStatsExtractor (), loader = FsNeo4jCSVLoader ()) job = DefaultJob ( conf = job_config , task = task ) job . launch () BamboohrUserExtractor \u00b6 The included BamboohrUserExtractor provides support for extracting basic user metadata from BambooHR . For companies and organizations that use BambooHR to store employee information such as email addresses, first names, last names, titles, and departments, use the BamboohrUserExtractor to populate Amundsen user data. A sample job config is shown below. extractor = BamboohrUserExtractor () task = DefaultTask ( extractor = extractor , loader = FsNeo4jCSVLoader ()) job_config = ConfigFactory . from_dict ({ 'extractor.bamboohr_user.api_key' : api_key , 'extractor.bamboohr_user.subdomain' : subdomain , }) job = DefaultJob ( conf = job_config , task = task , publisher = Neo4jCsvPublisher ()) job . launch () List of transformers \u00b6 Transformers are implemented by subclassing Transformer and implementing transform(self, record) . A transformer can: Modify a record and return it, Return None to filter a record out, Yield multiple records. This is useful for e.g. inferring metadata (such as ownership) from table descriptions. ChainedTransformer \u00b6 A chanined transformer that can take multiple transformers, passing each record through the chain. RegexStrReplaceTransformer \u00b6 Generic string replacement transformer using REGEX. User can pass list of tuples where tuple contains regex and replacement pair. job_config = ConfigFactory . from_dict ({ 'transformer.regex_str_replace. {} ' . format ( REGEX_REPLACE_TUPLE_LIST ): [( ',' , ' ' ), ( '\"' , '' )], 'transformer.regex_str_replace. {} ' . format ( ATTRIBUTE_NAME ): 'instance_field_name' ,}) job = DefaultJob ( conf = job_config , task = DefaultTask ( extractor = AnyExtractor (), transformer = RegexStrReplaceTransformer (), loader = AnyLoader ())) job . launch () TemplateVariableSubstitutionTransformer \u00b6 Adds or replaces field in Dict by string.format based on given template and provide record Dict as a template parameter. DictToModel \u00b6 Transforms dictionary into model. TimestampStringToEpoch \u00b6 Transforms string timestamp into int epoch. RemoveFieldTransformer \u00b6 Remove fields from the Dict. TableTagTransformer \u00b6 Adds the same set of tags to all tables produced by the job. GenericTransformer \u00b6 Transforms dictionary based on callback function that user provides. List of loader \u00b6 FsNeo4jCSVLoader \u00b6 Write node and relationship CSV file(s) that can be consumed by Neo4jCsvPublisher. It assumes that the record it consumes is instance of Neo4jCsvSerializable. job_config = ConfigFactory . from_dict ({ 'loader.filesystem_csv_neo4j. {} ' . format ( FsNeo4jCSVLoader . NODE_DIR_PATH ): node_files_folder , 'loader.filesystem_csv_neo4j. {} ' . format ( FsNeo4jCSVLoader . RELATION_DIR_PATH ): relationship_files_folder },) job = DefaultJob ( conf = job_config , task = DefaultTask ( extractor = AnyExtractor (), loader = FsNeo4jCSVLoader ()), publisher = Neo4jCsvPublisher ()) job . launch () GenericLoader \u00b6 Loader class that calls user provided callback function with record as a parameter Example that pushes Mode Dashboard accumulated usage via GenericLoader where callback_function expected to insert record to data warehouse. extractor = ModeDashboardUsageExtractor () task = DefaultTask ( extractor = extractor , loader = GenericLoader (), ) job_config = ConfigFactory . from_dict ({ ' {} . {} ' . format ( extractor . get_scope (), ORGANIZATION ): organization , ' {} . {} ' . format ( extractor . get_scope (), MODE_BEARER_TOKEN ): mode_bearer_token , 'loader.generic.callback_function' : callback_function }) job = DefaultJob ( conf = job_config , task = task ) job . launch () FSElasticsearchJSONLoader \u00b6 Write Elasticsearch document in JSON format which can be consumed by ElasticsearchPublisher. It assumes that the record it consumes is instance of ElasticsearchDocument. data_file_path = '/var/tmp/amundsen/search_data.json' job_config = ConfigFactory . from_dict ({ 'loader.filesystem.elasticsearch. {} ' . format ( FSElasticsearchJSONLoader . FILE_PATH_CONFIG_KEY ): data_file_path , 'loader.filesystem.elasticsearch. {} ' . format ( FSElasticsearchJSONLoader . FILE_MODE_CONFIG_KEY ): 'w' ,}) job = DefaultJob ( conf = job_config , task = DefaultTask ( extractor = AnyExtractor (), loader = FSElasticsearchJSONLoader ()), publisher = ElasticsearchPublisher ()) job . launch () FsAtlasCSVLoader \u00b6 Write node and relationship CSV file(s) that can be consumed by AtlasCsvPublisher. It assumes that the record it consumes is instance of AtlasSerializable. from pyhocon import ConfigFactory from databuilder.job.job import DefaultJob from databuilder.loader.file_system_atlas_csv_loader import FsAtlasCSVLoader from databuilder.task.task import DefaultTask tmp_folder = f '/tmp/amundsen/dashboard' job_config = ConfigFactory . from_dict ({ f 'loader.filesystem_csv_atlas. { FsAtlasCSVLoader . ENTITY_DIR_PATH } ' : f ' { tmp_folder } /entities' , f 'loader.filesystem_csv_atlas. { FsAtlasCSVLoader . RELATIONSHIP_DIR_PATH } ' : f ' { tmp_folder } /relationships' }) job = DefaultJob ( conf = job_config , task = DefaultTask ( extractor = AnyExtractor (), loader = FsAtlasCSVLoader ()), publisher = AnyPublisher ()) job . launch () List of publisher \u00b6 Neo4jCsvPublisher \u00b6 A Publisher takes two folders for input and publishes to Neo4j. One folder will contain CSV file(s) for Node where the other folder will contain CSV file(s) for Relationship. Neo4j follows Label Node properties Graph and refer to here for more information node_files_folder = ' {tmp_folder} /nodes/' . format ( tmp_folder = tmp_folder ) relationship_files_folder = ' {tmp_folder} /relationships/' . format ( tmp_folder = tmp_folder ) job_config = ConfigFactory . from_dict ({ 'loader.filesystem_csv_neo4j. {} ' . format ( FsNeo4jCSVLoader . NODE_DIR_PATH ): node_files_folder , 'loader.filesystem_csv_neo4j. {} ' . format ( FsNeo4jCSVLoader . RELATION_DIR_PATH ): relationship_files_folder , 'publisher.neo4j. {} ' . format ( neo4j_csv_publisher . NODE_FILES_DIR ): node_files_folder , 'publisher.neo4j. {} ' . format ( neo4j_csv_publisher . RELATION_FILES_DIR ): relationship_files_folder , 'publisher.neo4j. {} ' . format ( neo4j_csv_publisher . NEO4J_END_POINT_KEY ): neo4j_endpoint , 'publisher.neo4j. {} ' . format ( neo4j_csv_publisher . NEO4J_USER ): neo4j_user , 'publisher.neo4j. {} ' . format ( neo4j_csv_publisher . NEO4J_PASSWORD ): neo4j_password , 'publisher.neo4j. {} ' . format ( neo4j_csv_publisher . NEO4J_ENCRYPTED ): True , }) job = DefaultJob ( conf = job_config , task = DefaultTask ( extractor = AnyExtractor (), loader = FsNeo4jCSVLoader ()), publisher = Neo4jCsvPublisher ()) job . launch () ElasticsearchPublisher \u00b6 Elasticsearch Publisher uses Bulk API to load data from JSON file. Elasticsearch publisher supports atomic operation by utilizing alias in Elasticsearch. A new index is created and data is uploaded into it. After the upload is complete, index alias is swapped to point to new index from old index and traffic is routed to new index. data_file_path = '/var/tmp/amundsen/search_data.json' job_config = ConfigFactory . from_dict ({ 'loader.filesystem.elasticsearch. {} ' . format ( FSElasticsearchJSONLoader . FILE_PATH_CONFIG_KEY ): data_file_path , 'loader.filesystem.elasticsearch. {} ' . format ( FSElasticsearchJSONLoader . FILE_MODE_CONFIG_KEY ): 'w' , 'publisher.elasticsearch. {} ' . format ( ElasticsearchPublisher . FILE_PATH_CONFIG_KEY ): data_file_path , 'publisher.elasticsearch. {} ' . format ( ElasticsearchPublisher . FILE_MODE_CONFIG_KEY ): 'r' , 'publisher.elasticsearch {} ' . format ( ElasticsearchPublisher . ELASTICSEARCH_CLIENT_CONFIG_KEY ): elasticsearch_client , 'publisher.elasticsearch. {} ' . format ( ElasticsearchPublisher . ELASTICSEARCH_NEW_INDEX_CONFIG_KEY ): elasticsearch_new_index , 'publisher.elasticsearch. {} ' . format ( ElasticsearchPublisher . ELASTICSEARCH_DOC_TYPE_CONFIG_KEY ): elasticsearch_doc_type , 'publisher.elasticsearch. {} ' . format ( ElasticsearchPublisher . ELASTICSEARCH_ALIAS_CONFIG_KEY ): elasticsearch_index_alias , }) job = DefaultJob ( conf = job_config , task = DefaultTask ( extractor = AnyExtractor (), loader = FSElasticsearchJSONLoader ()), publisher = ElasticsearchPublisher ()) job . launch () AtlasCsvPublisher \u00b6 A Publisher takes two folders for input and publishes to Atlas. One folder will contain CSV file(s) for Entity where the other folder will contain CSV file(s) for Relationship. Caution!!! \u00b6 Publisher assumes that all necessary data types are already defined in atlas, otherwise publishing will fail. from apache_atlas.client.base_client import AtlasClient from pyhocon import ConfigFactory from databuilder.job.job import DefaultJob from databuilder.loader.file_system_atlas_csv_loader import FsAtlasCSVLoader from databuilder.publisher.atlas_csv_publisher import AtlasCSVPublisher from databuilder.task.task import DefaultTask tmp_folder = f '/tmp/amundsen/dashboard' job_config = ConfigFactory . from_dict ({ f 'loader.filesystem_csv_atlas. { FsAtlasCSVLoader . ENTITY_DIR_PATH } ' : f ' { tmp_folder } /entities' , f 'loader.filesystem_csv_atlas. { FsAtlasCSVLoader . RELATIONSHIP_DIR_PATH } ' : f ' { tmp_folder } /relationships' , f 'publisher.atlas_csv_publisher. { AtlasCSVPublisher . ATLAS_CLIENT } ' : AtlasClient ( 'http://localhost:21000' , ( 'admin' , 'admin' )) , f 'publisher.atlas_csv_publisher. { AtlasCSVPublisher . ENTITY_DIR_PATH } ' : f ' { tmp_folder } /entities' , f 'publisher.atlas_csv_publisher. { AtlasCSVPublisher . RELATIONSHIP_DIR_PATH } ' : f ' { tmp_folder } /relationships' , f 'publisher.atlas_csv_publisher. { AtlasCSVPublisher . ATLAS_ENTITY_CREATE_BATCH_SIZE } ' : 10 , }) job = DefaultJob ( conf = job_config , task = DefaultTask ( extractor = AnyExtractor (), loader = FsAtlasCSVLoader ()), publisher = AtlasCSVPublisher ()) job . launch () Callback \u00b6 Callback interface is built upon a Observer pattern where the participant want to take any action when target\u2019s state changes. Publisher is the first one adopting Callback where registered Callback will be called either when publish succeeded or when publish failed. In order to register callback, Publisher provides register_call_back method. One use case is for Extractor that needs to commit when job is finished (e.g: Kafka). Having Extractor register a callback to Publisher to commit when publish is successful, extractor can safely commit by implementing commit logic into on_success method. REST API Query \u00b6 Databuilder now has a generic REST API Query capability that can be joined each other. Most of the cases of extraction is currently from Database or Datawarehouse that is queryable via SQL. However, not all metadata sources provide our access to its Database and they mostly provide REST API to consume their metadata. The challenges come with REST API is that: there\u2019s no explicit standard in REST API. Here, we need to conform to majority of cases (HTTP call with JSON payload & response) but open for extension for different authentication scheme, and different way of pagination, etc. It is hardly the case that you would get what you want from one REST API call. It is usually the case that you need to snitch (JOIN) multiple REST API calls together to get the information you want. To solve this challenges, we introduce RestApiQuery RestAPIQuery is: 1. Assuming that REST API is using HTTP(S) call with GET method \u2013 RestAPIQuery intention\u2019s is read , not write \u2013 where basic HTTP auth is supported out of the box. There\u2019s extension point on other authentication scheme such as Oauth, and pagination, etc. (See ModePaginatedRestApiQuery for pagination) 2. Usually, you want the subset of the response you get from the REST API call \u2013 value extraction. To extract the value you want, RestApiQuery uses JSONPath which is similar product as XPATH of XML. 3. You can JOIN multiple RestApiQuery together. More detail on JOIN operation in RestApiQuery: 1. It joins multiple RestApiQuery together by accepting prior RestApiQuery as a constructor \u2013 a Decorator pattern 2. In REST API, URL is the one that locates the resource we want. Here, JOIN simply means we need to find resource based on the identifier that other query\u2019s result has . In other words, when RestApiQuery forms URL, it uses previous query\u2019s result to compute the URL e.g: Previous record: {\"dashboard_id\": \"foo\"}, URL before: http://foo.bar/dashboard/{dashboard_id} URL after compute: http://foo.bar/dashboard/foo With this pattern RestApiQuery supports 1:1 and 1:N JOIN relationship. (GROUP BY or any other aggregation, sub-query join is not supported) To see in action, take a peek at ModeDashboardExtractor Also, take a look at how it extends to support pagination at ModePaginatedRestApiQuery . Removing stale data in Neo4j \u2013 Neo4jStalenessRemovalTask : \u00b6 As Databuilder ingestion mostly consists of either INSERT OR UPDATE, there could be some stale data that has been removed from metadata source but still remains in Neo4j database. Neo4jStalenessRemovalTask basically detects staleness and removes it. In Neo4jCsvPublisher , it adds attributes \u201cpublished_tag\u201d and \u201cpublisher_last_updated_epoch_ms\u201d on every nodes and relations. You can use either of these two attributes to detect staleness and remove those stale node or relation from the database. Using \u201cpublished_tag\u201d to remove stale data \u00b6 Use published_tag to remove stale data, when it is certain that non-matching tag is stale once all the ingestion is completed. For example, suppose that you use current date (or execution date in Airflow) as a published_tag , \u201c2020-03-31\u201d. Once Databuilder ingests all tables and all columns, all table nodes and column nodes should have published_tag as \u201c2020-03-31\u201d. It is safe to assume that table nodes and column nodes whose published_tag is different \u2013 such as \u201c2020-03-30\u201d or \u201c2020-02-10\u201d \u2013 means that it is deleted from the source metadata. You can use Neo4jStalenessRemovalTask to delete those stale data. 1 2 3 4 5 6 7 8 9 10 11 12 13 task = Neo4jStalenessRemovalTask() job_config_dict = { 'job.identifier': 'remove_stale_data_job', 'task.remove_stale_data.neo4j_endpoint': neo4j_endpoint, 'task.remove_stale_data.neo4j_user': neo4j_user, 'task.remove_stale_data.neo4j_password': neo4j_password, 'task.remove_stale_data.staleness_max_pct': 10, 'task.remove_stale_data.target_nodes': ['Table', 'Column'], 'task.remove_stale_data.job_publish_tag': '2020-03-31' } job_config = ConfigFactory.from_dict(job_config_dict) job = DefaultJob(conf=job_config, task=task) job.launch() Note that there\u2019s protection mechanism, staleness_max_pct , that protect your data being wiped out when something is clearly wrong. \u201c staleness_max_pct \u201d basically first measure the proportion of elements that will be deleted and if it exceeds threshold per type ( 10% on the configuration above ), the deletion won\u2019t be executed and the task aborts. Using \u201cpublisher_last_updated_epoch_ms\u201d to remove stale data \u00b6 You can think this approach as TTL based eviction. This is particularly useful when there are multiple ingestion pipelines and you cannot be sure when all ingestion is done. In this case, you might still can say that if specific node or relation has not been published past 3 days, it\u2019s stale data. 1 2 3 4 5 6 7 8 9 10 11 12 13 task = Neo4jStalenessRemovalTask() job_config_dict = { 'job.identifier': 'remove_stale_data_job', 'task.remove_stale_data.neo4j_endpoint': neo4j_endpoint, 'task.remove_stale_data.neo4j_user': neo4j_user, 'task.remove_stale_data.neo4j_password': neo4j_password, 'task.remove_stale_data.staleness_max_pct': 10, 'task.remove_stale_data.target_relations': ['READ', 'READ_BY'], 'task.remove_stale_data.milliseconds_to_expire': 86400000 * 3 } job_config = ConfigFactory.from_dict(job_config_dict) job = DefaultJob(conf=job_config, task=task) job.launch() Above configuration is trying to delete stale usage relation (READ, READ_BY), by deleting READ or READ_BY relation that has not been published past 3 days. If number of elements to be removed is more than 10% per type, this task will be aborted without executing any deletion. Dry run \u00b6 Deletion is always scary and it\u2019s better to perform dryrun before put this into action. You can use Dry run to see what sort of Cypher query will be executed. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 task = Neo4jStalenessRemovalTask() job_config_dict = { 'job.identifier': 'remove_stale_data_job', 'task.remove_stale_data.neo4j_endpoint': neo4j_endpoint, 'task.remove_stale_data.neo4j_user': neo4j_user, 'task.remove_stale_data.neo4j_password': neo4j_password, 'task.remove_stale_data.staleness_max_pct': 10, 'task.remove_stale_data.target_relations': ['READ', 'READ_BY'], 'task.remove_stale_data.milliseconds_to_expire': 86400000 * 3 'task.remove_stale_data.dry_run': True } job_config = ConfigFactory.from_dict(job_config_dict) job = DefaultJob(conf=job_config, task=task) job.launch()","title":"Overview"},{"location":"databuilder/#amundsen-databuilder","text":"Amundsen Databuilder is a data ingestion library, which is inspired by Apache Gobblin . It could be used in an orchestration framework(e.g. Apache Airflow) to build data from Amundsen. You could use the library either with an adhoc python script( example ) or inside an Apache Airflow DAG( example ). For information about Amundsen and our other services, visit the main repository README.md . Please also see our instructions for a quick start setup of Amundsen with dummy data, and an overview of the architecture .","title":"Amundsen Databuilder"},{"location":"databuilder/#requirements","text":"Python >= 3.6.x","title":"Requirements"},{"location":"databuilder/#doc","text":"https://www.amundsen.io/amundsen/","title":"Doc"},{"location":"databuilder/#concept","text":"ETL job consists of extraction of records from the source, transform records, if necessary, and load records into the sink. Amundsen Databuilder is a ETL framework for Amundsen and there are corresponding components for ETL called Extractor, Transformer, and Loader that deals with record level operation. A component called task controls all these three components. Job is the highest level component in Databuilder that controls task and publisher and is the one that client use to launch the ETL job. In Databuilder, each components are highly modularized and each components are using namespace based config, HOCON config, which makes it highly reusable and pluggable. (e.g: transformer can be reused within extractor, or extractor can be reused within extractor) (Note that concept on components are highly motivated by Apache Gobblin )","title":"Concept"},{"location":"databuilder/#extractor","text":"Extractor extracts record from the source. This does not neccessarily mean that it only supports pull pattern in ETL. For example, extracting record from messaging bus make it a push pattern in ETL.","title":"Extractor"},{"location":"databuilder/#transformer","text":"Transfomer takes record from either extractor or from transformer itself (via ChainedTransformer) to transform record.","title":"Transformer"},{"location":"databuilder/#loader","text":"A loader takes record from transformer or from extractor directly and load it to sink, or staging area. As loader is operated in record level, it\u2019s not capable of supporting atomicity.","title":"Loader"},{"location":"databuilder/#task","text":"A task orchestrates extractor, transformer, and loader to perform record level operation.","title":"Task"},{"location":"databuilder/#record","text":"A record is represented by one of models .","title":"Record"},{"location":"databuilder/#publisher","text":"A publisher is an optional component. It\u2019s common usage is to support atomicity in job level and/or to easily support bulk load into the sink.","title":"Publisher"},{"location":"databuilder/#job","text":"Job is the highest level component in Databuilder, and it orchestrates task, and publisher.","title":"Job"},{"location":"databuilder/#model","text":"Models are abstractions representing the domain.","title":"Model"},{"location":"databuilder/#list-of-extractors","text":"","title":"List of extractors"},{"location":"databuilder/#dbapiextractor","text":"An extractor that uses Python Database API interface. DBAPI requires three information, connection object that conforms DBAPI spec, a SELECT SQL statement, and a model class that correspond to the output of each row of SQL statement. job_config = ConfigFactory . from_dict ({ 'extractor.dbapi {} ' . format ( DBAPIExtractor . CONNECTION_CONFIG_KEY ): db_api_conn , 'extractor.dbapi. {} ' . format ( DBAPIExtractor . SQL_CONFIG_KEY ): select_sql_stmt , 'extractor.dbapi.model_class' : 'package.module_name.class_name' }) job = DefaultJob ( conf = job_config , task = DefaultTask ( extractor = DBAPIExtractor (), loader = AnyLoader ())) job . launch ()","title":"DBAPIExtractor"},{"location":"databuilder/#genericextractor","text":"An extractor that takes list of dict from user through config.","title":"GenericExtractor"},{"location":"databuilder/#hivetablelastupdatedextractor","text":"An extractor that extracts last updated time from Hive metastore and underlying file system. Although, hive metastore has a parameter called \u201clast_modified_time\u201d, but it cannot be used as it provides DDL timestamp not DML timestamp. For this reason, HiveTableLastUpdatedExtractor is utilizing underlying file of Hive to fetch latest updated date. However, it is not efficient to poke all files in Hive, and it only pokes underlying storage for non-partitioned table. For partitioned table, it will fetch partition created timestamp, and it\u2019s close enough for last updated timestamp. As getting metadata from files could be time consuming there\u2019re several features to increase performance. 1. Support of multithreading to parallelize metadata fetching. Although, cpython\u2019s multithreading is not true multithreading as it\u2019s bounded by single core, getting metadata of file is mostly IO bound operation. Note that number of threads should be less or equal to number of connections. 1. User can pass where clause to only include certain schema and also remove certain tables. For example, by adding something like TBL_NAME NOT REGEXP '(tmp|temp) would eliminate unncecessary computation. job_config = ConfigFactory . from_dict ({ 'extractor.hive_table_last_updated.partitioned_table_where_clause_suffix' : partitioned_table_where_clause , 'extractor.hive_table_last_updated.non_partitioned_table_where_clause_suffix' ): non_partitioned_table_where_clause , 'extractor.hive_table_last_updated.extractor.sqlalchemy. {} ' . format ( SQLAlchemyExtractor . CONN_STRING ): connection_string , 'extractor.hive_table_last_updated.extractor.fs_worker_pool_size' : pool_size , 'extractor.hive_table_last_updated.filesystem. {} ' . format ( FileSystem . DASK_FILE_SYSTEM ): s3fs . S3FileSystem ( anon = False , config_kwargs = { 'max_pool_connections' : pool_size })}) job = DefaultJob ( conf = job_config , task = DefaultTask ( extractor = HiveTableLastUpdatedExtractor (), loader = AnyLoader ())) job . launch ()","title":"HiveTableLastUpdatedExtractor"},{"location":"databuilder/#hivetablemetadataextractor","text":"An extractor that extracts table and column metadata including database, schema, table name, table description, column name and column description from Hive metastore database. job_config = ConfigFactory . from_dict ({ 'extractor.hive_table_metadata. {} ' . format ( HiveTableMetadataExtractor . WHERE_CLAUSE_SUFFIX_KEY ): where_clause_suffix , 'extractor.hive_table_metadata.extractor.sqlalchemy. {} ' . format ( SQLAlchemyExtractor . CONN_STRING ): connection_string ()}) job = DefaultJob ( conf = job_config , task = DefaultTask ( extractor = HiveTableMetadataExtractor (), loader = AnyLoader ())) job . launch ()","title":"HiveTableMetadataExtractor"},{"location":"databuilder/#cassandraextractor","text":"An extractor that extracts table and column metadata including keyspace, table name, column name and column type from Apache Cassandra databases job_config = ConfigFactory . from_dict ({ 'extractor.cassandra. {} ' . format ( CassandraExtractor . CLUSTER_KEY ): cluster_identifier_string , 'extractor.cassandra. {} ' . format ( CassandraExtractor . IPS_KEY ): [ 127.0.0.1 ], 'extractor.cassandra. {} ' . format ( CassandraExtractor . KWARGS_KEY ): {}, 'extractor.cassandra. {} ' . format ( CassandraExtractor . FILTER_FUNCTION_KEY ): my_filter_function , }) job = DefaultJob ( conf = job_config , task = DefaultTask ( extractor = CassandraExtractor (), loader = AnyLoader ())) job . launch () If using the function filter options here is the function description def filter ( keytab , table ): # return False if you don't want to add that table and True if you want to add return True If needed to define more args on the cassandra cluster you can pass through kwargs args config = ConfigFactory . from_dict ({ 'extractor.cassandra. {} ' . format ( CassandraExtractor . IPS_KEY ): [ 127.0.0.1 ], 'extractor.cassandra. {} ' . format ( CassandraExtractor . KWARGS_KEY ): { 'port' : 9042 } }) # it will call the cluster constructor like this Cluster ([ 127.0.0.1 ], ** kwargs )","title":"CassandraExtractor"},{"location":"databuilder/#glueextractor","text":"An extractor that extracts table and column metadata including database, schema, table name, table description, column name and column description from AWS Glue metastore. Before running make sure you have a working AWS profile configured and have access to search tables on Glue job_config = ConfigFactory . from_dict ({ 'extractor.glue. {} ' . format ( GlueExtractor . CLUSTER_KEY ): cluster_identifier_string , 'extractor.glue. {} ' . format ( GlueExtractor . FILTER_KEY ): []}) job = DefaultJob ( conf = job_config , task = DefaultTask ( extractor = GlueExtractor (), loader = AnyLoader ())) job . launch () If using the filters option here is the input format [ { \"Key\": \"string\", \"Value\": \"string\", \"Comparator\": \"EQUALS\"|\"GREATER_THAN\"|\"LESS_THAN\"|\"GREATER_THAN_EQUALS\"|\"LESS_THAN_EQUALS\" } ... ]","title":"GlueExtractor"},{"location":"databuilder/#delta-lake-metadataextractor","text":"An extractor that runs on a spark cluster and obtains delta-lake metadata using spark sql commands. This custom solution is currently necessary because the hive metastore does not contain all metadata information for delta-lake tables. For simplicity, this extractor can also be used for all hive tables as well. Because it must run on a spark cluster, it is required that you have an operator (for example a databricks submit run operator ) that calls the configuration code on a spark cluster. spark = SparkSession . builder . appName ( \"Amundsen Delta Lake Metadata Extraction\" ) . getOrCreate () job_config = create_delta_lake_job_config () dExtractor = DeltaLakeMetadataExtractor () dExtractor . set_spark ( spark ) job = DefaultJob ( conf = job_config , task = DefaultTask ( extractor = dExtractor , loader = FsNeo4jCSVLoader ()), publisher = Neo4jCsvPublisher ()) job . launch () You can check out the sample deltalake metadata script for a full example.","title":"Delta-Lake-MetadataExtractor"},{"location":"databuilder/#dremiometadataextractor","text":"An extractor that extracts table and column metadata including database, schema, table name, table description, column name and column description from Dremio . Before running make sure that you have the Dremio ODBC driver installed. Default config values assume the default driver name for the MacBook install . job_config = ConfigFactory . from_dict ({ 'extractor.dremio. {} ' . format ( DremioMetadataExtractor . DREMIO_USER_KEY ): DREMIO_USER , 'extractor.dremio. {} ' . format ( DremioMetadataExtractor . DREMIO_PASSWORD_KEY ): DREMIO_PASSWORD , 'extractor.dremio. {} ' . format ( DremioMetadataExtractor . DREMIO_HOST_KEY ): DREMIO_HOST }) job = DefaultJob ( conf = job_config , task = DefaultTask ( extractor = DremioMetadataExtractor (), loader = AnyLoader ())) job . launch ()","title":"DremioMetadataExtractor"},{"location":"databuilder/#druidmetadataextractor","text":"An extractor that extracts table and column metadata including database, schema, table name, table description, column name and column description from a Druid DB. The where_clause_suffix could be defined, normally you would like to filter out the in INFORMATION_SCHEMA . You could specify the following job config conn_string = \"druid+https:// {host} : {port} /druid/v2/sql/\" . format ( host = druid_broker_host , port = 443 ) job_config = ConfigFactory . from_dict ({ 'extractor.druid_metadata. {} ' . format ( PostgresMetadataExtractor . WHERE_CLAUSE_SUFFIX_KEY ): where_clause_suffix , 'extractor.druid_metadata.extractor.sqlalchemy. {} ' . format ( SQLAlchemyExtractor . CONN_STRING ): conn_string ()}) job = DefaultJob ( conf = job_config , task = DefaultTask ( extractor = DruidMetadataExtractor (), loader = AnyLoader ())) job . launch ()","title":"DruidMetadataExtractor"},{"location":"databuilder/#postgresmetadataextractor","text":"An extractor that extracts table and column metadata including database, schema, table name, table description, column name and column description from a Postgres or Redshift database. By default, the Postgres/Redshift database name is used as the cluster name. To override this, set USE_CATALOG_AS_CLUSTER_NAME to False , and CLUSTER_KEY to what you wish to use as the cluster name. The where_clause_suffix below should define which schemas you\u2019d like to query (see the sample dag for an example). The SQL query driving the extraction is defined here job_config = ConfigFactory . from_dict ({ 'extractor.postgres_metadata. {} ' . format ( PostgresMetadataExtractor . WHERE_CLAUSE_SUFFIX_KEY ): where_clause_suffix , 'extractor.postgres_metadata. {} ' . format ( PostgresMetadataExtractor . USE_CATALOG_AS_CLUSTER_NAME ): True , 'extractor.postgres_metadata.extractor.sqlalchemy. {} ' . format ( SQLAlchemyExtractor . CONN_STRING ): connection_string ()}) job = DefaultJob ( conf = job_config , task = DefaultTask ( extractor = PostgresMetadataExtractor (), loader = AnyLoader ())) job . launch ()","title":"PostgresMetadataExtractor"},{"location":"databuilder/#mssqlmetadataextractor","text":"An extractor that extracts table and column metadata including database, schema, table name, table description, column name and column description from a Microsoft SQL database. By default, the Microsoft SQL Server Database name is used as the cluster name. To override this, set USE_CATALOG_AS_CLUSTER_NAME to False , and CLUSTER_KEY to what you wish to use as the cluster name. The where_clause_suffix below should define which schemas you\u2019d like to query ( \"('dbo','sys')\" ). The SQL query driving the extraction is defined here This extractor is highly derived from PostgresMetadataExtractor . job_config = ConfigFactory . from_dict ({ 'extractor.mssql_metadata. {} ' . format ( MSSQLMetadataExtractor . WHERE_CLAUSE_SUFFIX_KEY ): where_clause_suffix , 'extractor.mssql_metadata. {} ' . format ( MSSQLMetadataExtractor . USE_CATALOG_AS_CLUSTER_NAME ): True , 'extractor.mssql_metadata.extractor.sqlalchemy. {} ' . format ( SQLAlchemyExtractor . CONN_STRING ): connection_string ()}) job = DefaultJob ( conf = job_config , task = DefaultTask ( extractor = MSSQLMetadataExtractor (), loader = AnyLoader ())) job . launch ()","title":"MSSQLMetadataExtractor"},{"location":"databuilder/#mysqlmetadataextractor","text":"An extractor that extracts table and column metadata including database, schema, table name, table description, column name and column description from a MYSQL database. By default, the MYSQL database name is used as the cluster name. To override this, set USE_CATALOG_AS_CLUSTER_NAME to False , and CLUSTER_KEY to what you wish to use as the cluster name. The where_clause_suffix below should define which schemas you\u2019d like to query. The SQL query driving the extraction is defined here job_config = ConfigFactory . from_dict ({ 'extractor.mysql_metadata. {} ' . format ( MysqlMetadataExtractor . WHERE_CLAUSE_SUFFIX_KEY ): where_clause_suffix , 'extractor.mysql_metadata. {} ' . format ( MysqlMetadataExtractor . USE_CATALOG_AS_CLUSTER_NAME ): True , 'extractor.mysql_metadata.extractor.sqlalchemy. {} ' . format ( SQLAlchemyExtractor . CONN_STRING ): connection_string ()}) job = DefaultJob ( conf = job_config , task = DefaultTask ( extractor = MysqlMetadataExtractor (), loader = FsNeo4jCSVLoader ()), publisher = Neo4jCsvPublisher ()) job . launch ()","title":"MysqlMetadataExtractor"},{"location":"databuilder/#db2metadataextractor","text":"An extractor that extracts table and column metadata including database, schema, table name, table description, column name and column description from a Unix, Windows or Linux Db2 database or BigSQL. The where_clause_suffix below should define which schemas you\u2019d like to query or those that you would not (see the sample data loader for an example). The SQL query driving the extraction is defined here job_config = ConfigFactory . from_dict ({ 'extractor.db2_metadata. {} ' . format ( Db2MetadataExtractor . WHERE_CLAUSE_SUFFIX_KEY ): where_clause_suffix , 'extractor.db2_metadata.extractor.sqlalchemy. {} ' . format ( SQLAlchemyExtractor . CONN_STRING ): connection_string ()}) job = DefaultJob ( conf = job_config , task = DefaultTask ( extractor = Db2MetadataExtractor (), loader = AnyLoader ())) job . launch ()","title":"Db2MetadataExtractor"},{"location":"databuilder/#snowflakemetadataextractor","text":"An extractor that extracts table and column metadata including database, schema, table name, table description, column name and column description from a Snowflake database. By default, the Snowflake database name is used as the cluster name. To override this, set USE_CATALOG_AS_CLUSTER_NAME to False , and CLUSTER_KEY to what you wish to use as the cluster name. By default, the Snowflake database is set to PROD . To override this, set DATABASE_KEY to WhateverNameOfYourDb . By default, the Snowflake schema is set to INFORMATION_SCHEMA . To override this, set SCHEMA_KEY to WhateverNameOfYourSchema . Note that ACCOUNT_USAGE is a separate schema which allows users to query a wider set of data at the cost of latency. Differences are defined here The where_clause_suffix should define which schemas you\u2019d like to query (see the sample dag for an example). The SQL query driving the extraction is defined here job_config = ConfigFactory . from_dict ({ 'extractor.snowflake. {} ' . format ( SnowflakeMetadataExtractor . SNOWFLAKE_DATABASE_KEY ): 'YourDbName' , 'extractor.snowflake. {} ' . format ( SnowflakeMetadataExtractor . WHERE_CLAUSE_SUFFIX_KEY ): where_clause_suffix , 'extractor.snowflake. {} ' . format ( SnowflakeMetadataExtractor . USE_CATALOG_AS_CLUSTER_NAME ): True , 'extractor.snowflake.extractor.sqlalchemy. {} ' . format ( SQLAlchemyExtractor . CONN_STRING ): connection_string ()}) job = DefaultJob ( conf = job_config , task = DefaultTask ( extractor = SnowflakeMetadataExtractor (), loader = AnyLoader ())) job . launch ()","title":"SnowflakeMetadataExtractor"},{"location":"databuilder/#genericusageextractor","text":"An extractor that extracts table popularity metadata from a custom created Snowflake table (created by a script that may look like this scala script ). You can create a DAG using the Databricks Operator and run this script within Databricks or wherever you are able to run Scala. By default, snowflake is used as the database name. ColumnReader has the datasource as its database input, and database as its cluster input. The following inputs are related to where you create your Snowflake popularity table. By default, the Snowflake popularity database is set to PROD . To override this, set POPULARITY_TABLE_DATABASE to WhateverNameOfYourDb . By default, the Snowflake popularity schema is set to SCHEMA . To override this, set POPULARTIY_TABLE_SCHEMA to WhateverNameOfYourSchema . By default, the Snowflake popularity table is set to TABLE . To override this, set POPULARITY_TABLE_NAME to WhateverNameOfYourTable . The where_clause_suffix should define any filtering you\u2019d like to include in your query. For example, this may include user_email s that you don\u2019t want to include in your popularity definition. job_config = ConfigFactory . from_dict ({ f 'extractor.snowflake.extractor.sqlalchemy. { SQLAlchemyExtractor . CONN_STRING } ' : connection_string (), f 'extractor.snowflake. { GenericUsageExtractor . WHERE_CLAUSE_SUFFIX_KEY } ' : where_clause_suffix , f 'extractor.snowflake. { GenericUsageExtractor . POPULARITY_TABLE_DATABASE } ' : 'WhateverNameOfYourDb' , f 'extractor.snowflake. { GenericUsageExtractor . POPULARTIY_TABLE_SCHEMA } ' : 'WhateverNameOfYourSchema' , f 'extractor.snowflake. { GenericUsageExtractor . POPULARITY_TABLE_NAME } ' : 'WhateverNameOfYourTable' , job = DefaultJob ( conf = job_config , task = DefaultTask ( extractor = GenericUsageExtractor (), loader = AnyLoader ())) job . launch ()","title":"GenericUsageExtractor"},{"location":"databuilder/#snowflaketablelastupdatedextractor","text":"An extractor that extracts table last updated timestamp from a Snowflake database. It uses same configs as the SnowflakeMetadataExtractor described above. The SQL query driving the extraction is defined here job_config = ConfigFactory . from_dict ({ 'extractor.snowflake_table_last_updated. {} ' . format ( SnowflakeTableLastUpdatedExtractor . SNOWFLAKE_DATABASE_KEY ): 'YourDbName' , 'extractor.snowflake_table_last_updated. {} ' . format ( SnowflakeTableLastUpdatedExtractor . WHERE_CLAUSE_SUFFIX_KEY ): where_clause_suffix , 'extractor.snowflake_table_last_updated. {} ' . format ( SnowflakeTableLastUpdatedExtractor . USE_CATALOG_AS_CLUSTER_NAME ): True , 'extractor.snowflake_table_last_updated.extractor.sqlalchemy. {} ' . format ( SQLAlchemyExtractor . CONN_STRING ): connection_string ()}) job = DefaultJob ( conf = job_config , task = DefaultTask ( extractor = SnowflakeTableLastUpdatedExtractor (), loader = AnyLoader ())) job . launch ()","title":"SnowflakeTableLastUpdatedExtractor"},{"location":"databuilder/#bigquerymetadataextractor","text":"An extractor that extracts table and column metadata including database, schema, table name, table description, column name and column description from a Bigquery database. The API calls driving the extraction is defined here You will need to create a service account for reading metadata and grant it \u201cBigQuery Metadata Viewer\u201d access to all of your datasets. This can all be done via the bigquery ui. Download the creditials file and store it securely. Set the GOOGLE_APPLICATION_CREDENTIALS environment varible to the location of your credtials files and your code should have access to everything it needs. You can configure bigquery like this. You can optionally set a label filter if you only want to pull tables with a certain label. job_config = { 'extractor.bigquery_table_metadata. {} ' . format ( BigQueryMetadataExtractor . PROJECT_ID_KEY ): gcloud_project } if label_filter : job_config [ 'extractor.bigquery_table_metadata. {} ' . format ( BigQueryMetadataExtractor . FILTER_KEY ) ] = label_filter task = DefaultTask ( extractor = BigQueryMetadataExtractor (), loader = csv_loader , transformer = NoopTransformer ()) job = DefaultJob ( conf = ConfigFactory . from_dict ( job_config ), task = task , publisher = Neo4jCsvPublisher ()) job . launch ()","title":"BigQueryMetadataExtractor"},{"location":"databuilder/#neo4jeslastupdatedextractor","text":"An extractor that basically get current timestamp and passes it GenericExtractor. This extractor is basically being used to create timestamp for \u201cAmundsen was last indexed on \u2026\u201d in Amundsen web page\u2019s footer.","title":"Neo4jEsLastUpdatedExtractor"},{"location":"databuilder/#neo4jextractor","text":"An extractor that extracts records from Neo4j based on provided Cypher query . One example is to extract data from Neo4j so that it can transform and publish to Elasticsearch. job_config = ConfigFactory . from_dict ({ 'extractor.neo4j. {} ' . format ( Neo4jExtractor . CYPHER_QUERY_CONFIG_KEY ): cypher_query , 'extractor.neo4j. {} ' . format ( Neo4jExtractor . GRAPH_URL_CONFIG_KEY ): neo4j_endpoint , 'extractor.neo4j. {} ' . format ( Neo4jExtractor . MODEL_CLASS_CONFIG_KEY ): 'package.module.class_name' , 'extractor.neo4j. {} ' . format ( Neo4jExtractor . NEO4J_AUTH_USER ): neo4j_user , 'extractor.neo4j. {} ' . format ( Neo4jExtractor . NEO4J_AUTH_PW ): neo4j_password }, 'extractor.neo4j. {} ' . format ( Neo4jExtractor . NEO4J_ENCRYPTED ): True }) job = DefaultJob ( conf = job_config , task = DefaultTask ( extractor = Neo4jExtractor (), loader = AnyLoader ())) job . launch ()","title":"Neo4jExtractor"},{"location":"databuilder/#neo4jsearchdataextractor","text":"An extractor that is extracting Neo4j utilizing Neo4jExtractor where CYPHER query is already embedded in it. job_config = ConfigFactory . from_dict ({ 'extractor.search_data.extractor.neo4j. {} ' . format ( Neo4jExtractor . GRAPH_URL_CONFIG_KEY ): neo4j_endpoint , 'extractor.search_data.extractor.neo4j. {} ' . format ( Neo4jExtractor . MODEL_CLASS_CONFIG_KEY ): 'databuilder.models.neo4j_data.Neo4jDataResult' , 'extractor.search_data.extractor.neo4j. {} ' . format ( Neo4jExtractor . NEO4J_AUTH_USER ): neo4j_user , 'extractor.search_data.extractor.neo4j. {} ' . format ( Neo4jExtractor . NEO4J_AUTH_PW ): neo4j_password }, 'extractor.search_data.extractor.neo4j. {} ' . format ( Neo4jExtractor . NEO4J_ENCRYPTED ): False }) job = DefaultJob ( conf = job_config , task = DefaultTask ( extractor = Neo4jSearchDataExtractor (), loader = AnyLoader ())) job . launch ()","title":"Neo4jSearchDataExtractor"},{"location":"databuilder/#atlassearchdataextractor","text":"An extractor that is extracting Atlas Data to index compatible with Elasticsearch Search Proxy. entity_type = 'Table' extracted_search_data_path = f '/tmp/ { entity_type . lower () } _search_data.json' process_pool_size = 5 # atlas config atlas_url = 'localhost' atlas_port = 21000 atlas_protocol = 'http' atlas_verify_ssl = False atlas_username = 'admin' atlas_password = 'admin' atlas_search_chunk_size = 200 atlas_details_chunk_size = 10 # elastic config es = Elasticsearch ([ { 'host' : 'localhost' }, ]) elasticsearch_client = es elasticsearch_new_index_key = f ' { entity_type . lower () } -' + str ( uuid . uuid4 ()) elasticsearch_new_index_key_type = '_doc' elasticsearch_index_alias = f ' { entity_type . lower () } _search_index' job_config = ConfigFactory . from_dict ({ 'extractor.atlas_search_data. {} ' . format ( AtlasSearchDataExtractor . ATLAS_URL_CONFIG_KEY ): atlas_url , 'extractor.atlas_search_data. {} ' . format ( AtlasSearchDataExtractor . ATLAS_PORT_CONFIG_KEY ): atlas_port , 'extractor.atlas_search_data. {} ' . format ( AtlasSearchDataExtractor . ATLAS_PROTOCOL_CONFIG_KEY ): atlas_protocol , 'extractor.atlas_search_data. {} ' . format ( AtlasSearchDataExtractor . ATLAS_VALIDATE_SSL_CONFIG_KEY ): atlas_verify_ssl , 'extractor.atlas_search_data. {} ' . format ( AtlasSearchDataExtractor . ATLAS_USERNAME_CONFIG_KEY ): atlas_username , 'extractor.atlas_search_data. {} ' . format ( AtlasSearchDataExtractor . ATLAS_PASSWORD_CONFIG_KEY ): atlas_password , 'extractor.atlas_search_data. {} ' . format ( AtlasSearchDataExtractor . ATLAS_SEARCH_CHUNK_SIZE_KEY ): atlas_search_chunk_size , 'extractor.atlas_search_data. {} ' . format ( AtlasSearchDataExtractor . ATLAS_DETAILS_CHUNK_SIZE_KEY ): atlas_details_chunk_size , 'extractor.atlas_search_data. {} ' . format ( AtlasSearchDataExtractor . PROCESS_POOL_SIZE_KEY ): process_pool_size , 'extractor.atlas_search_data. {} ' . format ( AtlasSearchDataExtractor . ENTITY_TYPE_KEY ): entity_type , 'loader.filesystem.elasticsearch. {} ' . format ( FSElasticsearchJSONLoader . FILE_PATH_CONFIG_KEY ): extracted_search_data_path , 'loader.filesystem.elasticsearch. {} ' . format ( FSElasticsearchJSONLoader . FILE_MODE_CONFIG_KEY ): 'w' , 'publisher.elasticsearch. {} ' . format ( ElasticsearchPublisher . FILE_PATH_CONFIG_KEY ): extracted_search_data_path , 'publisher.elasticsearch. {} ' . format ( ElasticsearchPublisher . FILE_MODE_CONFIG_KEY ): 'r' , 'publisher.elasticsearch. {} ' . format ( ElasticsearchPublisher . ELASTICSEARCH_CLIENT_CONFIG_KEY ): elasticsearch_client , 'publisher.elasticsearch. {} ' . format ( ElasticsearchPublisher . ELASTICSEARCH_NEW_INDEX_CONFIG_KEY ): elasticsearch_new_index_key , 'publisher.elasticsearch. {} ' . format ( ElasticsearchPublisher . ELASTICSEARCH_DOC_TYPE_CONFIG_KEY ): elasticsearch_new_index_key_type , 'publisher.elasticsearch. {} ' . format ( ElasticsearchPublisher . ELASTICSEARCH_ALIAS_CONFIG_KEY ): elasticsearch_index_alias }) if __name__ == \"__main__\" : task = DefaultTask ( extractor = AtlasSearchDataExtractor (), transformer = NoopTransformer (), loader = FSElasticsearchJSONLoader ()) job = DefaultJob ( conf = job_config , task = task , publisher = ElasticsearchPublisher ()) job . launch ()","title":"AtlasSearchDataExtractor"},{"location":"databuilder/#verticametadataextractor","text":"An extractor that extracts table and column metadata including database, schema, table name, column name and column datatype from a Vertica database. A sample loading script for Vertica is provided here By default, the Vertica database name is used as the cluster name. The where_clause_suffix in the example can be used to define which schemas you would like to query.","title":"VerticaMetadataExtractor"},{"location":"databuilder/#sqlalchemyextractor","text":"An extractor utilizes SQLAlchemy to extract record from any database that support SQL Alchemy. job_config = ConfigFactory . from_dict ({ 'extractor.sqlalchemy. {} ' . format ( SQLAlchemyExtractor . CONN_STRING ): connection_string (), 'extractor.sqlalchemy. {} ' . format ( SQLAlchemyExtractor . EXTRACT_SQL ): sql , 'extractor.sqlalchemy.model_class' : 'package.module.class_name' }) job = DefaultJob ( conf = job_config , task = DefaultTask ( extractor = SQLAlchemyExtractor (), loader = AnyLoader ())) job . launch ()","title":"SQLAlchemyExtractor"},{"location":"databuilder/#dbtextractor","text":"This extractor utilizes the dbt output files catalog.json and manifest.json to extract metadata and ingest it into Amundsen. The catalog.json and manifest.json can both be generated by running dbt docs generate in your dbt project. Visit the dbt artifacts page for more information. The DbtExtractor can currently create the following: Tables and their definitions Columns and their definitions Table level lineage dbt tags (as Amundsen badges or tags) Table Sources (e.g. link to GitHib where the dbt template resides) job_config = ConfigFactory . from_dict ({ # Required args f 'extractor.dbt. { DbtExtractor . DATABASE_NAME } ' : 'snowflake' , f 'extractor.dbt. { DbtExtractor . MANIFEST_JSON } ' : catalog_file_loc , # File location f 'extractor.dbt. { DbtExtractor . DATABASE_NAME } ' : json . dumps ( manifest_data ), # JSON Dumped object # Optional args f 'extractor.dbt. { DbtExtractor . SOURCE_URL } ' : 'https://github.com/your-company/your-repo/tree/main' , f 'extractor.dbt. { DbtExtractor . EXTRACT_TABLES } ' : True , f 'extractor.dbt. { DbtExtractor . EXTRACT_DESCRIPTIONS } ' : True , f 'extractor.dbt. { DbtExtractor . EXTRACT_TAGS } ' : True , f 'extractor.dbt. { DbtExtractor . IMPORT_TAGS_AS } ' : 'badges' , f 'extractor.dbt. { DbtExtractor . EXTRACT_LINEAGE } ' : True , }) job = DefaultJob ( conf = job_config , task = DefaultTask ( extractor = DbtExtractor (), loader = AnyLoader ())) job . launch ()","title":"DbtExtractor"},{"location":"databuilder/#restapiextractor","text":"A extractor that utilizes RestAPIQuery to extract data. RestAPIQuery needs to be constructed ( example ) and needs to be injected to RestAPIExtractor.","title":"RestAPIExtractor"},{"location":"databuilder/#mode-dashboard-extractor","text":"Here are extractors that extracts metadata information from Mode via Mode\u2019s REST API. Prerequisite: You will need to create API access token that has admin privilege. You will need organization code. This is something you can easily get by looking at one of Mode report\u2019s URL. https://app.mode.com/<organization code>/reports/report_token","title":"Mode Dashboard Extractor"},{"location":"databuilder/#modedashboardextractor","text":"A Extractor that extracts core metadata on Mode dashboard. https://app.mode.com/ It extracts list of reports that consists of: Dashboard group name (Space name) Dashboard group id (Space token) Dashboard group description (Space description) Dashboard name (Report name) Dashboard id (Report token) Dashboard description (Report description) Other information such as report run, owner, chart name, query name is in separate extractor. It calls two APIs ( spaces API and reports API ) joining together. You can create Databuilder job config like this. task = DefaultTask ( extractor = ModeDashboardExtractor (), loader = FsNeo4jCSVLoader (), ) tmp_folder = '/var/tmp/amundsen/mode_dashboard_metadata' node_files_folder = ' {tmp_folder} /nodes' . format ( tmp_folder = tmp_folder ) relationship_files_folder = ' {tmp_folder} /relationships' . format ( tmp_folder = tmp_folder ) job_config = ConfigFactory . from_dict ({ 'extractor.mode_dashboard. {} ' . format ( ORGANIZATION ): organization , 'extractor.mode_dashboard. {} ' . format ( MODE_BEARER_TOKEN ): mode_bearer_token , 'extractor.mode_dashboard. {} ' . format ( DASHBOARD_GROUP_IDS_TO_SKIP ): [ space_token_1 , space_token_2 , ... ], 'loader.filesystem_csv_neo4j. {} ' . format ( FsNeo4jCSVLoader . NODE_DIR_PATH ): node_files_folder , 'loader.filesystem_csv_neo4j. {} ' . format ( FsNeo4jCSVLoader . RELATION_DIR_PATH ): relationship_files_folder , 'loader.filesystem_csv_neo4j. {} ' . format ( FsNeo4jCSVLoader . SHOULD_DELETE_CREATED_DIR ): True , 'task.progress_report_frequency' : 100 , 'publisher.neo4j. {} ' . format ( neo4j_csv_publisher . NODE_FILES_DIR ): node_files_folder , 'publisher.neo4j. {} ' . format ( neo4j_csv_publisher . RELATION_FILES_DIR ): relationship_files_folder , 'publisher.neo4j. {} ' . format ( neo4j_csv_publisher . NEO4J_END_POINT_KEY ): neo4j_endpoint , 'publisher.neo4j. {} ' . format ( neo4j_csv_publisher . NEO4J_USER ): neo4j_user , 'publisher.neo4j. {} ' . format ( neo4j_csv_publisher . NEO4J_PASSWORD ): neo4j_password , 'publisher.neo4j. {} ' . format ( neo4j_csv_publisher . NEO4J_ENCRYPTED ): True , 'publisher.neo4j. {} ' . format ( neo4j_csv_publisher . NEO4J_CREATE_ONLY_NODES ): [ DESCRIPTION_NODE_LABEL ], 'publisher.neo4j. {} ' . format ( neo4j_csv_publisher . JOB_PUBLISH_TAG ): job_publish_tag }) job = DefaultJob ( conf = job_config , task = task , publisher = Neo4jCsvPublisher ()) job . launch ()","title":"ModeDashboardExtractor"},{"location":"databuilder/#modedashboardownerextractor","text":"An Extractor that extracts Dashboard owner. Mode itself does not have concept of owner and it will use creator as owner. Note that if user left the organization, it would skip the dashboard. You can create Databuilder job config like this. (configuration related to loader and publisher is omitted as it is mostly the same. Please take a look at this example for the configuration that holds loader and publisher. extractor = ModeDashboardOwnerExtractor () task = DefaultTask ( extractor = extractor , loader = FsNeo4jCSVLoader (), ) job_config = ConfigFactory . from_dict ({ ' {} . {} ' . format ( extractor . get_scope (), ORGANIZATION ): organization , ' {} . {} ' . format ( extractor . get_scope (), MODE_BEARER_TOKEN ): mode_bearer_token , }) job = DefaultJob ( conf = job_config , task = task , publisher = Neo4jCsvPublisher ()) job . launch ()","title":"ModeDashboardOwnerExtractor"},{"location":"databuilder/#modedashboardlastsuccessfulexecutionextractor","text":"A Extractor that extracts Mode dashboard\u2019s last successful run (execution) timestamp. You can create Databuilder job config like this. (configuration related to loader and publisher is omitted as it is mostly the same. Please take a look at this example for the configuration that holds loader and publisher. extractor = ModeDashboardLastSuccessfulExecutionExtractor () task = DefaultTask ( extractor = extractor , loader = FsNeo4jCSVLoader ()) job_config = ConfigFactory . from_dict ({ ' {} . {} ' . format ( extractor . get_scope (), ORGANIZATION ): organization , ' {} . {} ' . format ( extractor . get_scope (), MODE_BEARER_TOKEN ): mode_bearer_token , }) job = DefaultJob ( conf = job_config , task = task , publisher = Neo4jCsvPublisher ()) job . launch ()","title":"ModeDashboardLastSuccessfulExecutionExtractor"},{"location":"databuilder/#modedashboardexecutionsextractor","text":"A Extractor that extracts last run (execution) status and timestamp. You can create Databuilder job config like this. (configuration related to loader and publisher is omitted as it is mostly the same. Please take a look at this example for the configuration that holds loader and publisher. extractor = ModeDashboardExecutionsExtractor () task = DefaultTask ( extractor = extractor , loader = FsNeo4jCSVLoader ()) job_config = ConfigFactory . from_dict ({ ' {} . {} ' . format ( extractor . get_scope (), ORGANIZATION ): organization , ' {} . {} ' . format ( extractor . get_scope (), MODE_BEARER_TOKEN ): mode_bearer_token , }) job = DefaultJob ( conf = job_config , task = task , publisher = Neo4jCsvPublisher ()) job . launch ()","title":"ModeDashboardExecutionsExtractor"},{"location":"databuilder/#modedashboardlastmodifiedtimestampextractor","text":"A Extractor that extracts Mode dashboard\u2019s last modified timestamp. You can create Databuilder job config like this. (configuration related to loader and publisher is omitted as it is mostly the same. Please take a look at this example for the configuration that holds loader and publisher. extractor = ModeDashboardLastModifiedTimestampExtractor () task = DefaultTask ( extractor = extractor , loader = FsNeo4jCSVLoader ()) job_config = ConfigFactory . from_dict ({ ' {} . {} ' . format ( extractor . get_scope (), ORGANIZATION ): organization , ' {} . {} ' . format ( extractor . get_scope (), MODE_BEARER_TOKEN ): mode_bearer_token , }) job = DefaultJob ( conf = job_config , task = task , publisher = Neo4jCsvPublisher ()) job . launch ()","title":"ModeDashboardLastModifiedTimestampExtractor"},{"location":"databuilder/#modedashboardqueriesextractor","text":"A Extractor that extracts Mode\u2019s query information. You can create Databuilder job config like this. (configuration related to loader and publisher is omitted as it is mostly the same. Please take a look at this example for the configuration that holds loader and publisher. extractor = ModeDashboardQueriesExtractor () task = DefaultTask ( extractor = extractor , loader = FsNeo4jCSVLoader ()) job_config = ConfigFactory . from_dict ({ ' {} . {} ' . format ( extractor . get_scope (), ORGANIZATION ): organization , ' {} . {} ' . format ( extractor . get_scope (), MODE_BEARER_TOKEN ): mode_bearer_token , }) job = DefaultJob ( conf = job_config , task = task , publisher = Neo4jCsvPublisher ()) job . launch ()","title":"ModeDashboardQueriesExtractor"},{"location":"databuilder/#modedashboardchartsbatchextractor","text":"A Extractor that extracts Mode Dashboard charts metadata. You can create Databuilder job config like this. (configuration related to loader and publisher is omitted as it is mostly the same. Please take a look at this example for the configuration that holds loader and publisher. extractor = ModeDashboardChartsBatchExtractor () task = DefaultTask ( extractor = extractor , loader = FsNeo4jCSVLoader ()) job_config = ConfigFactory . from_dict ({ ' {} . {} ' . format ( extractor . get_scope (), ORGANIZATION ): organization , ' {} . {} ' . format ( extractor . get_scope (), MODE_BEARER_TOKEN ): mode_bearer_token , }) job = DefaultJob ( conf = job_config , task = task , publisher = Neo4jCsvPublisher ()) job . launch ()","title":"ModeDashboardChartsBatchExtractor"},{"location":"databuilder/#modedashboarduserextractor","text":"A Extractor that extracts Mode user_id and then update User node. You can create Databuilder job config like this. (configuration related to loader and publisher is omitted as it is mostly the same. Please take a look at this example for the configuration that holds loader and publisher. extractor = ModeDashboardUserExtractor () task = DefaultTask ( extractor = extractor , loader = FsNeo4jCSVLoader ()) job_config = ConfigFactory . from_dict ({ ' {} . {} ' . format ( extractor . get_scope (), ORGANIZATION ): organization , ' {} . {} ' . format ( extractor . get_scope (), MODE_ACCESS_TOKEN ): mode_token , ' {} . {} ' . format ( extractor . get_scope (), MODE_PASSWORD_TOKEN ): mode_password , }) job = DefaultJob ( conf = job_config , task = task , publisher = Neo4jCsvPublisher ()) job . launch ()","title":"ModeDashboardUserExtractor"},{"location":"databuilder/#modedashboardusageextractor","text":"A Extractor that extracts Mode dashboard\u2019s accumulated view count. Note that this provides accumulated view count which does not effectively show relevancy . Thus, fields from this extractor is not directly compatible with DashboardUsage model. If you are fine with accumulated usage , you could use TemplateVariableSubstitutionTransformer to transform Dict payload from ModeDashboardUsageExtractor to fit DashboardUsage and transform Dict to DashboardUsage by TemplateVariableSubstitutionTransformer , and DictToModel transformers. ( Example on how to combining these two transformers)","title":"ModeDashboardUsageExtractor"},{"location":"databuilder/#redashdashboardextractor","text":"The included RedashDashboardExtractor provides support for extracting basic metadata for Redash dashboards (dashboard name, owner, URL, created/updated timestamps, and a generated description) and their associated queries (query name, URL, and raw query). It can be extended with a configurable table parser function to also support extraction of DashboardTable metadata. (See below for example usage.) Note: DashboardUsage and DashboardExecution metadata are not supported in this extractor, as these concepts are not supported by the Redash API. The RedashDashboardExtractor depends on the following Redash API endpoints: GET /api/dashboards , GET /api/dashboards/<dashboard-slug> . It has been tested against Redash 8 and is also expected to work with Redash 9. extractor = RedashDashboardExtractor () task = DefaultTask ( extractor = extractor , loader = FsNeo4jCSVLoader ()) job_config = ConfigFactory . from_dict ({ 'extractor.redash_dashboard.redash_base_url' : redash_base_url , # ex: https://redash.example.org 'extractor.redash_dashboard.api_base_url' : api_base_url , # ex: https://redash.example.org/api 'extractor.redash_dashboard.api_key' : api_key , # ex: abc1234 'extractor.redash_dashboard.table_parser' : table_parser , # ex: my_library.module.parse_tables 'extractor.redash_dashboard.redash_version' : redash_version # ex: 8. optional, default=9 }) job = DefaultJob ( conf = job_config , task = task , publisher = Neo4jCsvPublisher ()) job . launch ()","title":"RedashDashboardExtractor"},{"location":"databuilder/#redashdashboardextractor-table_parser","text":"The RedashDashboardExtractor extracts raw queries from each dashboard. You may optionally use these queries to parse out relations to tables in Amundsen. A table parser can be provided in the configuration for the RedashDashboardExtractor , as seen above. This function should have type signature (RedashVisualizationWidget) -> Iterator[TableRelationData] . For example: def parse_tables ( viz_widget : RedashVisualizationWidget ) -> Iterator [ TableRelationData ]: # Each viz_widget corresponds to one query. # viz_widget.data_source_id is the ID of the target DB in Redash. # viz_widget.raw_query is the raw query (e.g., SQL). if viz_widget . data_source_id == 123 : table_names = some_sql_parser ( viz_widget . raw_query ) return [ TableRelationData ( 'some_db' , 'prod' , 'some_schema' , tbl ) for tbl in table_names ] return []","title":"RedashDashboardExtractor: table_parser"},{"location":"databuilder/#tableaudashboardextractor","text":"The included TableauDashboardExtractor provides support for extracting basic metadata for Tableau workbooks. All Tableau extractors including this one use the Tableau Metadata GraphQL API to gather the metadata. Tableau \u201cworkbooks\u201d are mapped to Amundsen dashboards, and the top-level project in which these workbooks preside is the dashboard group. The metadata it gathers is as follows: - Dashboard name (Workbook name) - Dashboard description (Workbook description) - Dashboard creation timestamp (Workbook creation timestamp) - Dashboard group name (Workbook top-level folder name) - Dashboard and dashboard group URL If you wish to exclude top-level projects from being loaded, specify their names in the tableau_excluded_projects list and workbooks from any of those projects will not be indexed. Tableau\u2019s concept of \u201cowners\u201d does not map cleanly into Amundsen\u2019s understanding of owners, as the owner of a Tableau workbook is simply whoever updated it last, even if they made a very small change. This can prove problematic in determining the true point of contact for a workbook, so it\u2019s simply omitted for now. Similarly, the hierachy of dashboard/query/chart in Amundsen does not map into Tableau, where charts have only an optional relation to queries and vice versa. For these reasons, there are not extractors for either entity. The Tableau Metadata API also does not support usage or execution statistics, so there are no extractors for these entities either. Sample job config: extractor = TableauDashboardExtractor () task = DefaultTask ( extractor = extractor , loader = FsNeo4jCSVLoader ()) job_config = ConfigFactory . from_dict ({ 'extractor.tableau_dashboard_metadata.tableau_host' : tableau_host , 'extractor.tableau_dashboard_metadata.api_version' : tableau_api_version , 'extractor.tableau_dashboard_metadata.site_name' : tableau_site_name , 'extractor.tableau_dashboard_metadata.tableau_personal_access_token_name' : tableau_personal_access_token_name , 'extractor.tableau_dashboard_metadata.tableau_personal_access_token_secret' : tableau_personal_access_token_secret , 'extractor.tableau_dashboard_metadata.excluded_projects' : tableau_excluded_projects , 'extractor.tableau_dashboard_metadata.cluster' : tableau_dashboard_cluster , 'extractor.tableau_dashboard_metadata.database' : tableau_dashboard_database , 'extractor.tableau_dashboard_metadata.transformer.timestamp_str_to_epoch.timestamp_format' : \"%Y-%m- %d T%H:%M:%SZ\" , }) job = DefaultJob ( conf = job_config , task = task , publisher = Neo4jCsvPublisher ()) job . launch ()","title":"TableauDashboardExtractor"},{"location":"databuilder/#tableaudashboardtableextractor","text":"The included TableauDashboardTableExtractor provides support for extracting table metadata from Tableau workbooks. The extractor assumes all the table entities have already been created; if you are interested in using the provided TableauExternalTableExtractor , make sure that job runs before this one, as it will create the tables required by this job. It also assumes that the dashboards are using their names as the primary ID. A sample job config is shown below. Configuration related to the loader and publisher is omitted as it is mostly the same. Please take a look at this example for the configuration that holds loader and publisher. extractor = TableauDashboardTableExtractor () task = DefaultTask ( extractor = extractor , loader = FsNeo4jCSVLoader ()) job_config = ConfigFactory . from_dict ({ 'extractor.tableau_dashboard_table.tableau_host' : tableau_host , 'extractor.tableau_dashboard_table.api_version' : tableau_api_version , 'extractor.tableau_dashboard_table.site_name' : tableau_site_name , 'extractor.tableau_dashboard_table.tableau_personal_access_token_name' : tableau_personal_access_token_name , 'extractor.tableau_dashboard_table.tableau_personal_access_token_secret' : tableau_personal_access_token_secret , 'extractor.tableau_dashboard_table.excluded_projects' : tableau_excluded_projects , 'extractor.tableau_dashboard_table.cluster' : tableau_dashboard_cluster , 'extractor.tableau_dashboard_table.database' : tableau_dashboard_database , 'extractor.tableau_dashboard_table.external_cluster_name' : tableau_external_table_cluster , 'extractor.tableau_dashboard_table.external_schema_name' : tableau_external_table_schema , }) job = DefaultJob ( conf = job_config , task = task , publisher = Neo4jCsvPublisher ()) job . launch ()","title":"TableauDashboardTableExtractor"},{"location":"databuilder/#tableaudashboardqueryextractor","text":"The included TableauDashboardQueryExtractor provides support for extracting query metadata from Tableau workbooks. It retrives the name and query text for each custom SQL query. A sample job config is shown below. Configuration related to the loader and publisher is omitted as it is mostly the same. Please take a look at this example for the configuration that holds loader and publisher. extractor = TableauDashboardQueryExtractor () task = DefaultTask ( extractor = extractor , loader = FsNeo4jCSVLoader ()) job_config = ConfigFactory . from_dict ({ 'extractor.tableau_dashboard_query.tableau_host' : tableau_host , 'extractor.tableau_dashboard_query.api_version' : tableau_api_version , 'extractor.tableau_dashboard_query.site_name' : tableau_site_name , 'extractor.tableau_dashboard_query.tableau_personal_access_token_name' : tableau_personal_access_token_name , 'extractor.tableau_dashboard_query.tableau_personal_access_token_secret' : tableau_personal_access_token_secret , 'extractor.tableau_dashboard_query.excluded_projects' : tableau_excluded_projects , 'extractor.tableau_dashboard_query.cluster' : tableau_dashboard_cluster , 'extractor.tableau_dashboard_query.database' : tableau_dashboard_database , }) job = DefaultJob ( conf = job_config , task = task , publisher = Neo4jCsvPublisher ()) job . launch ()","title":"TableauDashboardQueryExtractor"},{"location":"databuilder/#tableaudashboardlastmodifiedextractor","text":"The included TableauDashboardLastModifiedExtractor provides support for extracting the last updated timestamp for Tableau workbooks. A sample job config is shown below. Configuration related to the loader and publisher is omitted as it is mostly the same. Please take a look at this example for the configuration that holds loader and publisher. extractor = TableauDashboardQueryExtractor () task = DefaultTask ( extractor = extractor , loader = FsNeo4jCSVLoader ()) job_config = ConfigFactory . from_dict ({ 'extractor.tableau_dashboard_last_modified.tableau_host' : tableau_host , 'extractor.tableau_dashboard_last_modified.api_version' : tableau_api_version , 'extractor.tableau_dashboard_last_modified.site_name' : tableau_site_name , 'extractor.tableau_dashboard_last_modified.tableau_personal_access_token_name' : tableau_personal_access_token_name , 'extractor.tableau_dashboard_last_modified.tableau_personal_access_token_secret' : tableau_personal_access_token_secret , 'extractor.tableau_dashboard_last_modified.excluded_projects' : tableau_excluded_projects , 'extractor.tableau_dashboard_last_modified.cluster' : tableau_dashboard_cluster , 'extractor.tableau_dashboard_last_modified.database' : tableau_dashboard_database , 'extractor.tableau_dashboard_last_modified.transformer.timestamp_str_to_epoch.timestamp_format' : \"%Y-%m- %d T%H:%M:%SZ\" , }) job = DefaultJob ( conf = job_config , task = task , publisher = Neo4jCsvPublisher ()) job . launch ()","title":"TableauDashboardLastModifiedExtractor"},{"location":"databuilder/#tableauexternaltableextractor","text":"The included TableauExternalTableExtractor provides support for extracting external table entities referenced by Tableau workbooks. In this context, \u201cexternal\u201d tables are \u201ctables\u201d that are not from a typical database, and are loaded using some other data format, like CSV files. This extractor has been tested with the following types of external tables; feel free to add others, but it\u2019s recommended to test them in a non-production instance first to be safe. - Excel spreadsheets - Text files (including CSV files) - Salesforce connections - Google Sheets connections Use the external_table_types list config option to specify which external connection types you would like to index; refer to your Tableau instance for the exact formatting of each connection type string. Excel spreadsheets, Salesforce connections, and Google Sheets connections are all classified as \u201cdatabases\u201d in terms of Tableau\u2019s Metadata API, with their \u201csubsheets\u201d forming their \u201ctables\u201d when present. However, these tables are not assigned a schema, this extractor chooses to use the name of the parent sheet as the schema, and assign a new table to each subsheet. The connection type is always used as the database, and for text files, the schema is set using the external_schema_name config option. Since these external tables are usually named for human consumption only and often contain a wider range of characters, all inputs are sanitized to remove any problematic occurences before they are inserted: see the sanitize methods TableauDashboardUtils for specifics. A more concrete example: if one had a Google Sheet titled \u201cGrowth by Region\u201d with 2 subsheets called \u201cFY19 Report\u201d and \u201cFY20 Report\u201d, two tables would be generated with the following keys: googlesheets://external.growth_by_region/FY_19_Report googlesheets://external.growth_by_region/FY_20_Report A sample job config is shown below. Configuration related to the loader and publisher is omitted as it is mostly the same. Please take a look at this example for the configuration that holds loader and publisher. extractor = TableauExternalTableExtractor () task = DefaultTask ( extractor = extractor , loader = FsNeo4jCSVLoader ()) job_config = ConfigFactory . from_dict ({ 'extractor.tableau_external_table.tableau_host' : tableau_host , 'extractor.tableau_external_table.api_version' : tableau_api_version , 'extractor.tableau_external_table.site_name' : tableau_site_name , 'extractor.tableau_external_table.tableau_personal_access_token_name' : tableau_personal_access_token_name , 'extractor.tableau_external_table.tableau_personal_access_token_secret' : tableau_personal_access_token_secret , 'extractor.tableau_external_table.excluded_projects' : tableau_excluded_projects , 'extractor.tableau_external_table.cluster' : tableau_dashboard_cluster , 'extractor.tableau_external_table.database' : tableau_dashboard_database , 'extractor.tableau_external_table.external_cluster_name' : tableau_external_table_cluster , 'extractor.tableau_external_table.external_schema_name' : tableau_external_table_schema , 'extractor.tableau_external_table.external_table_types' : tableau_external_table_types }) job = DefaultJob ( conf = job_config , task = task , publisher = Neo4jCsvPublisher ()) job . launch ()","title":"TableauExternalTableExtractor"},{"location":"databuilder/#apachesupersetmetadataextractor","text":"The included ApacheSupersetMetadataExtractor provides support for extracting basic metadata for Apache Superset dashboards. All Apache Superset extractors including this one use Apache Superset REST API ( /api/v1 ) and were developed based on Apache Superset version 1.1 .","title":"ApacheSupersetMetadataExtractor"},{"location":"databuilder/#caution","text":"Apache Superset does not contain metadata fulfilling the concept of DashboardGroup . For that reasons, when configuring extractor following parameters must be provided: - dashboard_group_id (required) - dashboard_group_name (required) - cluster (required) - dashboard_group_description (optional)","title":"Caution!"},{"location":"databuilder/#dashboardmetadata","text":"ApacheSupersetMetadataExtractor extracts metadata into DashboardMetadata model.","title":"DashboardMetadata"},{"location":"databuilder/#metadata-available-in-rest-api","text":"Dashboard id (id) Dashboard name (dashboard_title) Dashboard URL (url)","title":"Metadata available in REST API"},{"location":"databuilder/#metadata-not-available-in-apache-superset-rest-api","text":"Dashboard description Dashboard creation timestamp","title":"Metadata not available in Apache Superset REST API"},{"location":"databuilder/#dashboardlastmodifiedtimestamp","text":"ApacheSupersetLastModifiedTimestampExtractor extracts metadata into DashboardLastModifiedTimestamp model.","title":"DashboardLastModifiedTimestamp"},{"location":"databuilder/#available-in-rest-api","text":"Dashboard last modified timestamp (changed_on property of dashboard)","title":"Available in REST API"},{"location":"databuilder/#caution_1","text":"changed_on value does not provide timezone info so we assume it\u2019s UTC.","title":"Caution!"},{"location":"databuilder/#sample-job-config","text":"tmp_folder = f '/tmp/amundsen/dashboard' dict_config = { f 'loader.filesystem_csv_neo4j. { FsNeo4jCSVLoader . NODE_DIR_PATH } ' : f ' { tmp_folder } /nodes' , f 'loader.filesystem_csv_neo4j. { FsNeo4jCSVLoader . RELATION_DIR_PATH } ' : f ' { tmp_folder } /relationships' , f 'loader.filesystem_csv_neo4j. { FsNeo4jCSVLoader . SHOULD_DELETE_CREATED_DIR } ' : True , f 'extractor.apache_superset. { ApacheSupersetBaseExtractor . DASHBOARD_GROUP_ID } ' : '1' , f 'extractor.apache_superset. { ApacheSupersetBaseExtractor . DASHBOARD_GROUP_NAME } ' : 'dashboard group' , f 'extractor.apache_superset. { ApacheSupersetBaseExtractor . DASHBOARD_GROUP_DESCRIPTION } ' : 'dashboard group description' , f 'extractor.apache_superset. { ApacheSupersetBaseExtractor . CLUSTER } ' : 'gold' , f 'extractor.apache_superset. { ApacheSupersetBaseExtractor . APACHE_SUPERSET_SECURITY_SETTINGS_DICT } ' : dict ( username = 'admin' , password = 'admin' , provider = 'db' ) } job_config = ConfigFactory . from_dict ( dict_config ) task = DefaultTask ( extractor = ApacheSupersetMetadataExtractor (), loader = FsNeo4jCSVLoader ()) job = DefaultJob ( conf = job_config , task = task ) job . launch ()","title":"Sample job config"},{"location":"databuilder/#apachesupersettableextractor","text":"The included ApacheSupersetTableExtractor provides support for extracting relationships between dashboards and tables. All Apache Superset extractors including this one use Apache Superset REST API ( api/v1 ).","title":"ApacheSupersetTableExtractor"},{"location":"databuilder/#caution_2","text":"As table information in Apache Superset is minimal, following configuration options enable parametrization required to achieve proper relationship information: - driver_to_database_mapping - mapping between sqlalchemy drivername and actual database property of TableMetadata model. - database_to_cluster_mapping - mapping between Apache Superset Database ID and cluster from TableMedata model (defaults to cluster config of extractor.apache_superset )","title":"Caution!"},{"location":"databuilder/#dashboardtable","text":"","title":"DashboardTable"},{"location":"databuilder/#metadata-available-in-rest-api_1","text":"Table keys","title":"Metadata available in REST API"},{"location":"databuilder/#sample-job-config_1","text":"tmp_folder = f '/tmp/amundsen/dashboard' dict_config = { f 'loader.filesystem_csv_neo4j. { FsNeo4jCSVLoader . NODE_DIR_PATH } ' : f ' { tmp_folder } /nodes' , f 'loader.filesystem_csv_neo4j. { FsNeo4jCSVLoader . RELATION_DIR_PATH } ' : f ' { tmp_folder } /relationships' , f 'loader.filesystem_csv_neo4j. { FsNeo4jCSVLoader . SHOULD_DELETE_CREATED_DIR } ' : True , f 'extractor.apache_superset. { ApacheSupersetBaseExtractor . DASHBOARD_GROUP_ID } ' : '1' , f 'extractor.apache_superset. { ApacheSupersetBaseExtractor . DASHBOARD_GROUP_NAME } ' : 'dashboard group' , f 'extractor.apache_superset. { ApacheSupersetBaseExtractor . DASHBOARD_GROUP_DESCRIPTION } ' : 'dashboard group description' , f 'extractor.apache_superset. { ApacheSupersetBaseExtractor . CLUSTER } ' : 'gold' , f 'extractor.apache_superset. { ApacheSupersetBaseExtractor . APACHE_SUPERSET_SECURITY_SETTINGS_DICT } ' : dict ( username = 'admin' , password = 'admin' , provider = 'db' ) } job_config = ConfigFactory . from_dict ( dict_config ) task = DefaultTask ( extractor = ApacheSupersetTableExtractor (), loader = FsNeo4jCSVLoader ()) job = DefaultJob ( conf = job_config , task = task ) job . launch ()","title":"Sample job config"},{"location":"databuilder/#apachesupersetchartextractor","text":"The included ApacheSupersetChartExtractor provides support for extracting information on charts connected to given dashboard.","title":"ApacheSupersetChartExtractor"},{"location":"databuilder/#caution_3","text":"Currently there is no way to connect Apache Superset Query model to neither Chart nor Dashboard model. For that reason, to comply with Amundsen Databuilder data model, we register single DashboardQuery node serving as a bridge to which all the DashboardChart nodes are connected.","title":"Caution!"},{"location":"databuilder/#dashboardchart","text":"","title":"DashboardChart"},{"location":"databuilder/#metadata-available-in-rest-api_2","text":"Chart id (id) Chart name (chart_name) Chart type (viz_type)","title":"Metadata available in REST API"},{"location":"databuilder/#metadata-not-available-in-rest-api","text":"Chart url","title":"Metadata not available in REST API"},{"location":"databuilder/#sample-job-config_2","text":"tmp_folder = f '/tmp/amundsen/dashboard' dict_config = { f 'loader.filesystem_csv_neo4j. { FsNeo4jCSVLoader . NODE_DIR_PATH } ' : f ' { tmp_folder } /nodes' , f 'loader.filesystem_csv_neo4j. { FsNeo4jCSVLoader . RELATION_DIR_PATH } ' : f ' { tmp_folder } /relationships' , f 'loader.filesystem_csv_neo4j. { FsNeo4jCSVLoader . SHOULD_DELETE_CREATED_DIR } ' : True , f 'extractor.apache_superset. { ApacheSupersetBaseExtractor . DASHBOARD_GROUP_ID } ' : '1' , f 'extractor.apache_superset. { ApacheSupersetBaseExtractor . DASHBOARD_GROUP_NAME } ' : 'dashboard group' , f 'extractor.apache_superset. { ApacheSupersetBaseExtractor . DASHBOARD_GROUP_DESCRIPTION } ' : 'dashboard group description' , f 'extractor.apache_superset. { ApacheSupersetBaseExtractor . CLUSTER } ' : 'gold' , f 'extractor.apache_superset. { ApacheSupersetBaseExtractor . APACHE_SUPERSET_SECURITY_SETTINGS_DICT } ' : dict ( username = 'admin' , password = 'admin' , provider = 'db' ) } job_config = ConfigFactory . from_dict ( dict_config ) task = DefaultTask ( extractor = ApacheSupersetChartExtractor (), loader = FsNeo4jCSVLoader ()) job = DefaultJob ( conf = job_config , task = task ) job . launch ()","title":"Sample job config"},{"location":"databuilder/#pandasprofilingcolumnstatsextractor","text":"Pandas profiling is a library commonly used by Data Engineer and Scientists to calculate advanced data profiles on data. It is run on pandas dataframe and results in json file containing (amongst other things) descriptive and quantile statistics on columns.","title":"PandasProfilingColumnStatsExtractor"},{"location":"databuilder/#required-input-parameters","text":"FILE_PATH - file path to pandas-profiling json report TABLE_NAME - name of the table for which report was calculated SCHEMA_NAME - name of the schema from which table originates DATABASE_NAME - name of database technology from which table originates CLUSTER_NAME - name of the cluster from which table originates","title":"Required input parameters"},{"location":"databuilder/#optional-input-parameters","text":"PRECISION - precision for metrics of float type. Defaults to 3 meaning up to 3 digits after decimal point. STAT_MAPPINGS - if you wish to collect only selected set of metrics configure this option with dictionary of following format: key - raw name of the stat in pandas-profiling value - tuple of 2 elements: first value of the tuple - full name of the stat (this influences what will be rendered for user in UI) second value of the tuple - function modifying the stat (by default we just do type casting) Such dictionary should in that case contain only keys of metrics you wish to collect. For example - if you want only min and max value of a column, provide extractor with configuration option: PandasProfilingColumnStatsExtractor . STAT_MAPPINGS = { 'max' : ( 'Maximum' , float ), 'min' : ( 'Minimum' , float )} Complete set of available metrics is defined as DEFAULT_STAT_MAPPINGS attribute of PandasProfilingColumnStatsExtractor.","title":"Optional input parameters"},{"location":"databuilder/#common-usage-patterns","text":"As pandas profiling is executed on top of pandas dataframe, it is up to the user to populate the dataframe before running the report calculation (and subsequently the extractor). While doing so remember that it might not be a good idea to run the report on a complete set of rows if your tables are very sparse. In such case it is recommended to dump a subset of rows to pandas dataframe beforehand and calculate the report on just a sample of original data.","title":"Common usage patterns"},{"location":"databuilder/#spark-support","text":"Support for native execution of pandas-profiling on Spark Dataframe is currently worked on and should come in the future.","title":"Spark support"},{"location":"databuilder/#sample-job-config_3","text":"import pandas as pd import pandas_profiling from pyhocon import ConfigFactory from sqlalchemy import create_engine from databuilder.extractor.pandas_profiling_column_stats_extractor import PandasProfilingColumnStatsExtractor from databuilder.job.job import DefaultJob from databuilder.loader.file_system_neo4j_csv_loader import FsNeo4jCSVLoader from databuilder.task.task import DefaultTask table_name = 'video_game_sales' schema_name = 'superset' # Load table contents to pandas dataframe db_uri = f 'postgresql://superset:superset@localhost:5432/ { schema_name } ' engine = create_engine ( db_uri , echo = True ) df = pd . read_sql_table ( table_name , con = engine ) # Calculate pandas-profiling report on a table report_file = '/tmp/table_report.json' report = df . profile_report ( sort = None ) report . to_file ( report_file ) # Run PandasProfilingColumnStatsExtractor on calculated report tmp_folder = f '/tmp/amundsen/column_stats' dict_config = { f 'loader.filesystem_csv_neo4j. { FsNeo4jCSVLoader . NODE_DIR_PATH } ' : f ' { tmp_folder } /nodes' , f 'loader.filesystem_csv_neo4j. { FsNeo4jCSVLoader . RELATION_DIR_PATH } ' : f ' { tmp_folder } /relationships' , f 'loader.filesystem_csv_neo4j. { FsNeo4jCSVLoader . SHOULD_DELETE_CREATED_DIR } ' : False , 'extractor.pandas_profiling.table_name' : table_name , 'extractor.pandas_profiling.schema_name' : schema_name , 'extractor.pandas_profiling.database_name' : 'postgres' , 'extractor.pandas_profiling.cluster_name' : 'dev' , 'extractor.pandas_profiling.file_path' : report_file } job_config = ConfigFactory . from_dict ( dict_config ) task = DefaultTask ( extractor = PandasProfilingColumnStatsExtractor (), loader = FsNeo4jCSVLoader ()) job = DefaultJob ( conf = job_config , task = task ) job . launch ()","title":"Sample job config"},{"location":"databuilder/#bamboohruserextractor","text":"The included BamboohrUserExtractor provides support for extracting basic user metadata from BambooHR . For companies and organizations that use BambooHR to store employee information such as email addresses, first names, last names, titles, and departments, use the BamboohrUserExtractor to populate Amundsen user data. A sample job config is shown below. extractor = BamboohrUserExtractor () task = DefaultTask ( extractor = extractor , loader = FsNeo4jCSVLoader ()) job_config = ConfigFactory . from_dict ({ 'extractor.bamboohr_user.api_key' : api_key , 'extractor.bamboohr_user.subdomain' : subdomain , }) job = DefaultJob ( conf = job_config , task = task , publisher = Neo4jCsvPublisher ()) job . launch ()","title":"BamboohrUserExtractor"},{"location":"databuilder/#list-of-transformers","text":"Transformers are implemented by subclassing Transformer and implementing transform(self, record) . A transformer can: Modify a record and return it, Return None to filter a record out, Yield multiple records. This is useful for e.g. inferring metadata (such as ownership) from table descriptions.","title":"List of transformers"},{"location":"databuilder/#chainedtransformer","text":"A chanined transformer that can take multiple transformers, passing each record through the chain.","title":"ChainedTransformer"},{"location":"databuilder/#regexstrreplacetransformer","text":"Generic string replacement transformer using REGEX. User can pass list of tuples where tuple contains regex and replacement pair. job_config = ConfigFactory . from_dict ({ 'transformer.regex_str_replace. {} ' . format ( REGEX_REPLACE_TUPLE_LIST ): [( ',' , ' ' ), ( '\"' , '' )], 'transformer.regex_str_replace. {} ' . format ( ATTRIBUTE_NAME ): 'instance_field_name' ,}) job = DefaultJob ( conf = job_config , task = DefaultTask ( extractor = AnyExtractor (), transformer = RegexStrReplaceTransformer (), loader = AnyLoader ())) job . launch ()","title":"RegexStrReplaceTransformer"},{"location":"databuilder/#templatevariablesubstitutiontransformer","text":"Adds or replaces field in Dict by string.format based on given template and provide record Dict as a template parameter.","title":"TemplateVariableSubstitutionTransformer"},{"location":"databuilder/#dicttomodel","text":"Transforms dictionary into model.","title":"DictToModel"},{"location":"databuilder/#timestampstringtoepoch","text":"Transforms string timestamp into int epoch.","title":"TimestampStringToEpoch"},{"location":"databuilder/#removefieldtransformer","text":"Remove fields from the Dict.","title":"RemoveFieldTransformer"},{"location":"databuilder/#tabletagtransformer","text":"Adds the same set of tags to all tables produced by the job.","title":"TableTagTransformer"},{"location":"databuilder/#generictransformer","text":"Transforms dictionary based on callback function that user provides.","title":"GenericTransformer"},{"location":"databuilder/#list-of-loader","text":"","title":"List of loader"},{"location":"databuilder/#fsneo4jcsvloader","text":"Write node and relationship CSV file(s) that can be consumed by Neo4jCsvPublisher. It assumes that the record it consumes is instance of Neo4jCsvSerializable. job_config = ConfigFactory . from_dict ({ 'loader.filesystem_csv_neo4j. {} ' . format ( FsNeo4jCSVLoader . NODE_DIR_PATH ): node_files_folder , 'loader.filesystem_csv_neo4j. {} ' . format ( FsNeo4jCSVLoader . RELATION_DIR_PATH ): relationship_files_folder },) job = DefaultJob ( conf = job_config , task = DefaultTask ( extractor = AnyExtractor (), loader = FsNeo4jCSVLoader ()), publisher = Neo4jCsvPublisher ()) job . launch ()","title":"FsNeo4jCSVLoader"},{"location":"databuilder/#genericloader","text":"Loader class that calls user provided callback function with record as a parameter Example that pushes Mode Dashboard accumulated usage via GenericLoader where callback_function expected to insert record to data warehouse. extractor = ModeDashboardUsageExtractor () task = DefaultTask ( extractor = extractor , loader = GenericLoader (), ) job_config = ConfigFactory . from_dict ({ ' {} . {} ' . format ( extractor . get_scope (), ORGANIZATION ): organization , ' {} . {} ' . format ( extractor . get_scope (), MODE_BEARER_TOKEN ): mode_bearer_token , 'loader.generic.callback_function' : callback_function }) job = DefaultJob ( conf = job_config , task = task ) job . launch ()","title":"GenericLoader"},{"location":"databuilder/#fselasticsearchjsonloader","text":"Write Elasticsearch document in JSON format which can be consumed by ElasticsearchPublisher. It assumes that the record it consumes is instance of ElasticsearchDocument. data_file_path = '/var/tmp/amundsen/search_data.json' job_config = ConfigFactory . from_dict ({ 'loader.filesystem.elasticsearch. {} ' . format ( FSElasticsearchJSONLoader . FILE_PATH_CONFIG_KEY ): data_file_path , 'loader.filesystem.elasticsearch. {} ' . format ( FSElasticsearchJSONLoader . FILE_MODE_CONFIG_KEY ): 'w' ,}) job = DefaultJob ( conf = job_config , task = DefaultTask ( extractor = AnyExtractor (), loader = FSElasticsearchJSONLoader ()), publisher = ElasticsearchPublisher ()) job . launch ()","title":"FSElasticsearchJSONLoader"},{"location":"databuilder/#fsatlascsvloader","text":"Write node and relationship CSV file(s) that can be consumed by AtlasCsvPublisher. It assumes that the record it consumes is instance of AtlasSerializable. from pyhocon import ConfigFactory from databuilder.job.job import DefaultJob from databuilder.loader.file_system_atlas_csv_loader import FsAtlasCSVLoader from databuilder.task.task import DefaultTask tmp_folder = f '/tmp/amundsen/dashboard' job_config = ConfigFactory . from_dict ({ f 'loader.filesystem_csv_atlas. { FsAtlasCSVLoader . ENTITY_DIR_PATH } ' : f ' { tmp_folder } /entities' , f 'loader.filesystem_csv_atlas. { FsAtlasCSVLoader . RELATIONSHIP_DIR_PATH } ' : f ' { tmp_folder } /relationships' }) job = DefaultJob ( conf = job_config , task = DefaultTask ( extractor = AnyExtractor (), loader = FsAtlasCSVLoader ()), publisher = AnyPublisher ()) job . launch ()","title":"FsAtlasCSVLoader"},{"location":"databuilder/#list-of-publisher","text":"","title":"List of publisher"},{"location":"databuilder/#neo4jcsvpublisher","text":"A Publisher takes two folders for input and publishes to Neo4j. One folder will contain CSV file(s) for Node where the other folder will contain CSV file(s) for Relationship. Neo4j follows Label Node properties Graph and refer to here for more information node_files_folder = ' {tmp_folder} /nodes/' . format ( tmp_folder = tmp_folder ) relationship_files_folder = ' {tmp_folder} /relationships/' . format ( tmp_folder = tmp_folder ) job_config = ConfigFactory . from_dict ({ 'loader.filesystem_csv_neo4j. {} ' . format ( FsNeo4jCSVLoader . NODE_DIR_PATH ): node_files_folder , 'loader.filesystem_csv_neo4j. {} ' . format ( FsNeo4jCSVLoader . RELATION_DIR_PATH ): relationship_files_folder , 'publisher.neo4j. {} ' . format ( neo4j_csv_publisher . NODE_FILES_DIR ): node_files_folder , 'publisher.neo4j. {} ' . format ( neo4j_csv_publisher . RELATION_FILES_DIR ): relationship_files_folder , 'publisher.neo4j. {} ' . format ( neo4j_csv_publisher . NEO4J_END_POINT_KEY ): neo4j_endpoint , 'publisher.neo4j. {} ' . format ( neo4j_csv_publisher . NEO4J_USER ): neo4j_user , 'publisher.neo4j. {} ' . format ( neo4j_csv_publisher . NEO4J_PASSWORD ): neo4j_password , 'publisher.neo4j. {} ' . format ( neo4j_csv_publisher . NEO4J_ENCRYPTED ): True , }) job = DefaultJob ( conf = job_config , task = DefaultTask ( extractor = AnyExtractor (), loader = FsNeo4jCSVLoader ()), publisher = Neo4jCsvPublisher ()) job . launch ()","title":"Neo4jCsvPublisher"},{"location":"databuilder/#elasticsearchpublisher","text":"Elasticsearch Publisher uses Bulk API to load data from JSON file. Elasticsearch publisher supports atomic operation by utilizing alias in Elasticsearch. A new index is created and data is uploaded into it. After the upload is complete, index alias is swapped to point to new index from old index and traffic is routed to new index. data_file_path = '/var/tmp/amundsen/search_data.json' job_config = ConfigFactory . from_dict ({ 'loader.filesystem.elasticsearch. {} ' . format ( FSElasticsearchJSONLoader . FILE_PATH_CONFIG_KEY ): data_file_path , 'loader.filesystem.elasticsearch. {} ' . format ( FSElasticsearchJSONLoader . FILE_MODE_CONFIG_KEY ): 'w' , 'publisher.elasticsearch. {} ' . format ( ElasticsearchPublisher . FILE_PATH_CONFIG_KEY ): data_file_path , 'publisher.elasticsearch. {} ' . format ( ElasticsearchPublisher . FILE_MODE_CONFIG_KEY ): 'r' , 'publisher.elasticsearch {} ' . format ( ElasticsearchPublisher . ELASTICSEARCH_CLIENT_CONFIG_KEY ): elasticsearch_client , 'publisher.elasticsearch. {} ' . format ( ElasticsearchPublisher . ELASTICSEARCH_NEW_INDEX_CONFIG_KEY ): elasticsearch_new_index , 'publisher.elasticsearch. {} ' . format ( ElasticsearchPublisher . ELASTICSEARCH_DOC_TYPE_CONFIG_KEY ): elasticsearch_doc_type , 'publisher.elasticsearch. {} ' . format ( ElasticsearchPublisher . ELASTICSEARCH_ALIAS_CONFIG_KEY ): elasticsearch_index_alias , }) job = DefaultJob ( conf = job_config , task = DefaultTask ( extractor = AnyExtractor (), loader = FSElasticsearchJSONLoader ()), publisher = ElasticsearchPublisher ()) job . launch ()","title":"ElasticsearchPublisher"},{"location":"databuilder/#atlascsvpublisher","text":"A Publisher takes two folders for input and publishes to Atlas. One folder will contain CSV file(s) for Entity where the other folder will contain CSV file(s) for Relationship.","title":"AtlasCsvPublisher"},{"location":"databuilder/#caution_4","text":"Publisher assumes that all necessary data types are already defined in atlas, otherwise publishing will fail. from apache_atlas.client.base_client import AtlasClient from pyhocon import ConfigFactory from databuilder.job.job import DefaultJob from databuilder.loader.file_system_atlas_csv_loader import FsAtlasCSVLoader from databuilder.publisher.atlas_csv_publisher import AtlasCSVPublisher from databuilder.task.task import DefaultTask tmp_folder = f '/tmp/amundsen/dashboard' job_config = ConfigFactory . from_dict ({ f 'loader.filesystem_csv_atlas. { FsAtlasCSVLoader . ENTITY_DIR_PATH } ' : f ' { tmp_folder } /entities' , f 'loader.filesystem_csv_atlas. { FsAtlasCSVLoader . RELATIONSHIP_DIR_PATH } ' : f ' { tmp_folder } /relationships' , f 'publisher.atlas_csv_publisher. { AtlasCSVPublisher . ATLAS_CLIENT } ' : AtlasClient ( 'http://localhost:21000' , ( 'admin' , 'admin' )) , f 'publisher.atlas_csv_publisher. { AtlasCSVPublisher . ENTITY_DIR_PATH } ' : f ' { tmp_folder } /entities' , f 'publisher.atlas_csv_publisher. { AtlasCSVPublisher . RELATIONSHIP_DIR_PATH } ' : f ' { tmp_folder } /relationships' , f 'publisher.atlas_csv_publisher. { AtlasCSVPublisher . ATLAS_ENTITY_CREATE_BATCH_SIZE } ' : 10 , }) job = DefaultJob ( conf = job_config , task = DefaultTask ( extractor = AnyExtractor (), loader = FsAtlasCSVLoader ()), publisher = AtlasCSVPublisher ()) job . launch ()","title":"Caution!!!"},{"location":"databuilder/#callback","text":"Callback interface is built upon a Observer pattern where the participant want to take any action when target\u2019s state changes. Publisher is the first one adopting Callback where registered Callback will be called either when publish succeeded or when publish failed. In order to register callback, Publisher provides register_call_back method. One use case is for Extractor that needs to commit when job is finished (e.g: Kafka). Having Extractor register a callback to Publisher to commit when publish is successful, extractor can safely commit by implementing commit logic into on_success method.","title":"Callback"},{"location":"databuilder/#rest-api-query","text":"Databuilder now has a generic REST API Query capability that can be joined each other. Most of the cases of extraction is currently from Database or Datawarehouse that is queryable via SQL. However, not all metadata sources provide our access to its Database and they mostly provide REST API to consume their metadata. The challenges come with REST API is that: there\u2019s no explicit standard in REST API. Here, we need to conform to majority of cases (HTTP call with JSON payload & response) but open for extension for different authentication scheme, and different way of pagination, etc. It is hardly the case that you would get what you want from one REST API call. It is usually the case that you need to snitch (JOIN) multiple REST API calls together to get the information you want. To solve this challenges, we introduce RestApiQuery RestAPIQuery is: 1. Assuming that REST API is using HTTP(S) call with GET method \u2013 RestAPIQuery intention\u2019s is read , not write \u2013 where basic HTTP auth is supported out of the box. There\u2019s extension point on other authentication scheme such as Oauth, and pagination, etc. (See ModePaginatedRestApiQuery for pagination) 2. Usually, you want the subset of the response you get from the REST API call \u2013 value extraction. To extract the value you want, RestApiQuery uses JSONPath which is similar product as XPATH of XML. 3. You can JOIN multiple RestApiQuery together. More detail on JOIN operation in RestApiQuery: 1. It joins multiple RestApiQuery together by accepting prior RestApiQuery as a constructor \u2013 a Decorator pattern 2. In REST API, URL is the one that locates the resource we want. Here, JOIN simply means we need to find resource based on the identifier that other query\u2019s result has . In other words, when RestApiQuery forms URL, it uses previous query\u2019s result to compute the URL e.g: Previous record: {\"dashboard_id\": \"foo\"}, URL before: http://foo.bar/dashboard/{dashboard_id} URL after compute: http://foo.bar/dashboard/foo With this pattern RestApiQuery supports 1:1 and 1:N JOIN relationship. (GROUP BY or any other aggregation, sub-query join is not supported) To see in action, take a peek at ModeDashboardExtractor Also, take a look at how it extends to support pagination at ModePaginatedRestApiQuery .","title":"REST API Query"},{"location":"databuilder/#removing-stale-data-in-neo4j-neo4jstalenessremovaltask","text":"As Databuilder ingestion mostly consists of either INSERT OR UPDATE, there could be some stale data that has been removed from metadata source but still remains in Neo4j database. Neo4jStalenessRemovalTask basically detects staleness and removes it. In Neo4jCsvPublisher , it adds attributes \u201cpublished_tag\u201d and \u201cpublisher_last_updated_epoch_ms\u201d on every nodes and relations. You can use either of these two attributes to detect staleness and remove those stale node or relation from the database.","title":"Removing stale data in Neo4j -- Neo4jStalenessRemovalTask:"},{"location":"databuilder/#using-published_tag-to-remove-stale-data","text":"Use published_tag to remove stale data, when it is certain that non-matching tag is stale once all the ingestion is completed. For example, suppose that you use current date (or execution date in Airflow) as a published_tag , \u201c2020-03-31\u201d. Once Databuilder ingests all tables and all columns, all table nodes and column nodes should have published_tag as \u201c2020-03-31\u201d. It is safe to assume that table nodes and column nodes whose published_tag is different \u2013 such as \u201c2020-03-30\u201d or \u201c2020-02-10\u201d \u2013 means that it is deleted from the source metadata. You can use Neo4jStalenessRemovalTask to delete those stale data. 1 2 3 4 5 6 7 8 9 10 11 12 13 task = Neo4jStalenessRemovalTask() job_config_dict = { 'job.identifier': 'remove_stale_data_job', 'task.remove_stale_data.neo4j_endpoint': neo4j_endpoint, 'task.remove_stale_data.neo4j_user': neo4j_user, 'task.remove_stale_data.neo4j_password': neo4j_password, 'task.remove_stale_data.staleness_max_pct': 10, 'task.remove_stale_data.target_nodes': ['Table', 'Column'], 'task.remove_stale_data.job_publish_tag': '2020-03-31' } job_config = ConfigFactory.from_dict(job_config_dict) job = DefaultJob(conf=job_config, task=task) job.launch() Note that there\u2019s protection mechanism, staleness_max_pct , that protect your data being wiped out when something is clearly wrong. \u201c staleness_max_pct \u201d basically first measure the proportion of elements that will be deleted and if it exceeds threshold per type ( 10% on the configuration above ), the deletion won\u2019t be executed and the task aborts.","title":"Using \"published_tag\" to remove stale data"},{"location":"databuilder/#using-publisher_last_updated_epoch_ms-to-remove-stale-data","text":"You can think this approach as TTL based eviction. This is particularly useful when there are multiple ingestion pipelines and you cannot be sure when all ingestion is done. In this case, you might still can say that if specific node or relation has not been published past 3 days, it\u2019s stale data. 1 2 3 4 5 6 7 8 9 10 11 12 13 task = Neo4jStalenessRemovalTask() job_config_dict = { 'job.identifier': 'remove_stale_data_job', 'task.remove_stale_data.neo4j_endpoint': neo4j_endpoint, 'task.remove_stale_data.neo4j_user': neo4j_user, 'task.remove_stale_data.neo4j_password': neo4j_password, 'task.remove_stale_data.staleness_max_pct': 10, 'task.remove_stale_data.target_relations': ['READ', 'READ_BY'], 'task.remove_stale_data.milliseconds_to_expire': 86400000 * 3 } job_config = ConfigFactory.from_dict(job_config_dict) job = DefaultJob(conf=job_config, task=task) job.launch() Above configuration is trying to delete stale usage relation (READ, READ_BY), by deleting READ or READ_BY relation that has not been published past 3 days. If number of elements to be removed is more than 10% per type, this task will be aborted without executing any deletion.","title":"Using \"publisher_last_updated_epoch_ms\" to remove stale data"},{"location":"databuilder/#dry-run","text":"Deletion is always scary and it\u2019s better to perform dryrun before put this into action. You can use Dry run to see what sort of Cypher query will be executed. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 task = Neo4jStalenessRemovalTask() job_config_dict = { 'job.identifier': 'remove_stale_data_job', 'task.remove_stale_data.neo4j_endpoint': neo4j_endpoint, 'task.remove_stale_data.neo4j_user': neo4j_user, 'task.remove_stale_data.neo4j_password': neo4j_password, 'task.remove_stale_data.staleness_max_pct': 10, 'task.remove_stale_data.target_relations': ['READ', 'READ_BY'], 'task.remove_stale_data.milliseconds_to_expire': 86400000 * 3 'task.remove_stale_data.dry_run': True } job_config = ConfigFactory.from_dict(job_config_dict) job = DefaultJob(conf=job_config, task=task) job.launch()","title":"Dry run"},{"location":"databuilder/CHANGELOG/","text":"4.4.0 \u00b6 Features \u00b6 Column lineage implementation & sample ingest scripts ( https://github.com/amundsen-io/amundsendatabuilder/pull/470 ) Add mysql staleness removal task ( https://github.com/amundsen-io/amundsendatabuilder/pull/474 ) Add merge query results functionality to rest_api_query ( https://github.com/amundsen-io/amundsen/pull/1058 ) Bugfixes \u00b6 Switch to discovery api for mode spaces ( https://github.com/amundsen-io/amundsendatabuilder/pull/481 ) Update ModeDashboardExtractor to Mode discovery api ( https://github.com/amundsen-io/amundsen/pull/1063 ) Updated redshift_metadata_extractor.py to extract external schema as well as local schema. ( https://github.com/amundsen-io/amundsendatabuilder/pull/479 ) Upgrade dependency of Amundsen-gremlin ( https://github.com/amundsen-io/amundsendatabuilder/pull/473 ) fix bug in rest_api_query ( https://github.com/amundsen-io/amundsen/pull/1056 ) Pre 4.4.0 changes \u00b6 Feature \u00b6 Add support for tags based on atlas terms ( #466 ) ( cc1caf3 ) Make DescriptionMetadata inherit from GraphSerializable ( #461 ) ( 7f095fb ) Add TableSerializable and mysql_serializer ( #459 ) ( 4bb4452 ) Neptune Data builder Integration ( #438 ) ( 303e8aa ) Add config key for connect_arg for SqlAlchemyExtractor ( #434 ) ( 7f3be0f ) Vertica metadata extractor ( #433 ) ( f4bd207 ) Multi-yield transformers ( #396 ) ( 49ae0ed ) Atlas_search_extractor | :tada: Initial commit. ( #415 ) ( 8c63307 ) Sample Feast job with ES publisher ( #425 ) ( 453a18b ) Adding CsvTableBadgeExtractor ( #417 ) ( 592ee71 ) Feast extractor ( #414 ) ( 2343a90 ) Adding first pass of delta lake metadata extractor as well as a sample script on how it would be used. ( #351 ) ( e8679aa ) Use parameters to allow special characters in neo4j cypher statement ( #382 ) ( 6fd5035 ) Column level badges cont. ( #381 ) ( af4b512 ) Support dashboard chart in search ( #383 ) ( 6cced36 ) Column level badges ( #375 ) ( 8beee3e ) Added Dremio extractor ( #377 ) ( 63f239f ) Add an extractor for pulling user information from BambooHR ( #369 ) ( 6802ab1 ) Add sample_glue_loader script ( #366 ) ( fa3f11b ) Parameterize Snowflake Schema in Snowflake Metadata Extractor ( #361 ) ( aa4416c ) Mode Batch dashboard charrt API ( #362 ) ( 87213c5 ) Create a RedshiftMetadataExtractor that supports late binding views ( #356 ) ( 4113cfd ) Add MySQL sample data loader ( #359 ) ( 871a176 ) Add Snowflake table last updated timestamp extractor ( #348 ) ( 0bac11b ) Add Tableau dashboard metadata extractors ( #333 ) ( 46207ee ) Add github actions for databuilder ( #336 ) ( 236e7de ) Allow hive sql to be provided as config ( #312 ) ( 8075a6c ) Enhance glue extractor ( #306 ) ( faa795c ) Add RedashDashboardExtractor for extracting dashboards from redash.io ( #300 ) ( f1b0dfa ) Add a transformer that adds tags to all tables created in a job ( #287 ) ( d2f4bd3 ) Fix \u00b6 Add support for Tableau multi-site deployment ( #463 ) ( e35af58 ) Avoid error by checking for existence before close. ( #454 ) ( 5cd0dc8 ) Correct config getter ( #455 ) ( 4b37746 ) Close SQL Alchemy connections. ( #453 ) ( 25124c1 ) Add comma between bigquery requirements listings ( #452 ) ( 027edb9 ) Increase the compatibility of id structure between the Databuilder and the Metadata Library ( #445 ) ( 6a13762 ) Move \u2018grouped_tables\u2019 into _retrieve_tables ( #430 ) ( 26a0d0a ) Address PyAthena version ( #429 ) ( 7157c24 ) Add csv badges back in Quickstart ( #418 ) ( c0296b7 ) Typo in Readme ( #424 ) ( 29bd72f ) Fix redash dashboard exporter ( #422 ) ( fa626f5 ) Update the key format of set \u2018grouped_tables\u2019 ( #421 ) ( 4c9e5f7 ) Retry loop for exception caused by deadlock on badge node ( #404 ) ( 9fd1513 ) FsNeo4jCSVLoader fails if nodes have disjoint keys ( #408 ) ( c07cec9 ) Cast dashboard usage to be int ( #412 ) ( 8bcc489 ) Pandas \u2018nan\u2019 values ( #409 ) ( 3a28f46 ) Add databuilder missing dependencies ( #400 ) ( 6718396 ) Allow BigQuery Usage Extractor to extract usage for views ( #399 ) ( 8779229 ) Hive metadata extractor not work on postgresql ( #394 ) ( 2992618 ) Issues with inconsistency in case conversion ( #388 ) ( 9595866 ) Update elasticsearch table index mapping ( #373 ) ( 88c0552 ) Fix programmatic source data ( #367 ) ( 4f5df39 ) Update connection string in Snowflake extractor to include wareh\u2026 ( #357 ) ( a11d206 ) Edge case in Snowflake information_schema.last_altered value ( #360 ) ( c3e713e ) Correct typo in Snowflake Last Updated extract query ( #358 ) ( 5c2e98e ) Set Tableau URLs (base + API) via config ( #349 ) ( 1baec33 ) Fix invalid timestamp handling in dashboard transformer ( #339 ) ( 030ef49 ) Update postgres_sample_dag to set table extract job as upstream for elastic search publisher ( #340 ) ( c79935e ) deps: Unpin attrs ( #332 ) ( 86f658d ) Cypher statement param issue in Neo4jStalenessRemovalTask ( #307 ) ( 0078761 ) Added missing job tag key in hive_sample_dag.py ( #308 ) ( d6714b7 ) Fix sql for missing columns and mysql based dialects (#550) ( #305 ) ( 4b7b147 ) Escape backslashes in Neo4jCsvPublisher ( 1faa713 ) Variable organization in Model URL ( #293 ) ( b4c24ef ) Documentation \u00b6 Minor fixes to README ( #457 ) ( 54e89ce ) Update DashboardMetadata docs ( #402 ) ( 093b3d6 ) Fix broken doc link to dashboard_execution model ( #296 ) ( 24b3b0a ) Fix README.md ( #301 ) ( ad5765a )","title":"CHANGELOG"},{"location":"databuilder/CHANGELOG/#440","text":"","title":"4.4.0"},{"location":"databuilder/CHANGELOG/#features","text":"Column lineage implementation & sample ingest scripts ( https://github.com/amundsen-io/amundsendatabuilder/pull/470 ) Add mysql staleness removal task ( https://github.com/amundsen-io/amundsendatabuilder/pull/474 ) Add merge query results functionality to rest_api_query ( https://github.com/amundsen-io/amundsen/pull/1058 )","title":"Features"},{"location":"databuilder/CHANGELOG/#bugfixes","text":"Switch to discovery api for mode spaces ( https://github.com/amundsen-io/amundsendatabuilder/pull/481 ) Update ModeDashboardExtractor to Mode discovery api ( https://github.com/amundsen-io/amundsen/pull/1063 ) Updated redshift_metadata_extractor.py to extract external schema as well as local schema. ( https://github.com/amundsen-io/amundsendatabuilder/pull/479 ) Upgrade dependency of Amundsen-gremlin ( https://github.com/amundsen-io/amundsendatabuilder/pull/473 ) fix bug in rest_api_query ( https://github.com/amundsen-io/amundsen/pull/1056 )","title":"Bugfixes"},{"location":"databuilder/CHANGELOG/#pre-440-changes","text":"","title":"Pre 4.4.0 changes"},{"location":"databuilder/CHANGELOG/#feature","text":"Add support for tags based on atlas terms ( #466 ) ( cc1caf3 ) Make DescriptionMetadata inherit from GraphSerializable ( #461 ) ( 7f095fb ) Add TableSerializable and mysql_serializer ( #459 ) ( 4bb4452 ) Neptune Data builder Integration ( #438 ) ( 303e8aa ) Add config key for connect_arg for SqlAlchemyExtractor ( #434 ) ( 7f3be0f ) Vertica metadata extractor ( #433 ) ( f4bd207 ) Multi-yield transformers ( #396 ) ( 49ae0ed ) Atlas_search_extractor | :tada: Initial commit. ( #415 ) ( 8c63307 ) Sample Feast job with ES publisher ( #425 ) ( 453a18b ) Adding CsvTableBadgeExtractor ( #417 ) ( 592ee71 ) Feast extractor ( #414 ) ( 2343a90 ) Adding first pass of delta lake metadata extractor as well as a sample script on how it would be used. ( #351 ) ( e8679aa ) Use parameters to allow special characters in neo4j cypher statement ( #382 ) ( 6fd5035 ) Column level badges cont. ( #381 ) ( af4b512 ) Support dashboard chart in search ( #383 ) ( 6cced36 ) Column level badges ( #375 ) ( 8beee3e ) Added Dremio extractor ( #377 ) ( 63f239f ) Add an extractor for pulling user information from BambooHR ( #369 ) ( 6802ab1 ) Add sample_glue_loader script ( #366 ) ( fa3f11b ) Parameterize Snowflake Schema in Snowflake Metadata Extractor ( #361 ) ( aa4416c ) Mode Batch dashboard charrt API ( #362 ) ( 87213c5 ) Create a RedshiftMetadataExtractor that supports late binding views ( #356 ) ( 4113cfd ) Add MySQL sample data loader ( #359 ) ( 871a176 ) Add Snowflake table last updated timestamp extractor ( #348 ) ( 0bac11b ) Add Tableau dashboard metadata extractors ( #333 ) ( 46207ee ) Add github actions for databuilder ( #336 ) ( 236e7de ) Allow hive sql to be provided as config ( #312 ) ( 8075a6c ) Enhance glue extractor ( #306 ) ( faa795c ) Add RedashDashboardExtractor for extracting dashboards from redash.io ( #300 ) ( f1b0dfa ) Add a transformer that adds tags to all tables created in a job ( #287 ) ( d2f4bd3 )","title":"Feature"},{"location":"databuilder/CHANGELOG/#fix","text":"Add support for Tableau multi-site deployment ( #463 ) ( e35af58 ) Avoid error by checking for existence before close. ( #454 ) ( 5cd0dc8 ) Correct config getter ( #455 ) ( 4b37746 ) Close SQL Alchemy connections. ( #453 ) ( 25124c1 ) Add comma between bigquery requirements listings ( #452 ) ( 027edb9 ) Increase the compatibility of id structure between the Databuilder and the Metadata Library ( #445 ) ( 6a13762 ) Move \u2018grouped_tables\u2019 into _retrieve_tables ( #430 ) ( 26a0d0a ) Address PyAthena version ( #429 ) ( 7157c24 ) Add csv badges back in Quickstart ( #418 ) ( c0296b7 ) Typo in Readme ( #424 ) ( 29bd72f ) Fix redash dashboard exporter ( #422 ) ( fa626f5 ) Update the key format of set \u2018grouped_tables\u2019 ( #421 ) ( 4c9e5f7 ) Retry loop for exception caused by deadlock on badge node ( #404 ) ( 9fd1513 ) FsNeo4jCSVLoader fails if nodes have disjoint keys ( #408 ) ( c07cec9 ) Cast dashboard usage to be int ( #412 ) ( 8bcc489 ) Pandas \u2018nan\u2019 values ( #409 ) ( 3a28f46 ) Add databuilder missing dependencies ( #400 ) ( 6718396 ) Allow BigQuery Usage Extractor to extract usage for views ( #399 ) ( 8779229 ) Hive metadata extractor not work on postgresql ( #394 ) ( 2992618 ) Issues with inconsistency in case conversion ( #388 ) ( 9595866 ) Update elasticsearch table index mapping ( #373 ) ( 88c0552 ) Fix programmatic source data ( #367 ) ( 4f5df39 ) Update connection string in Snowflake extractor to include wareh\u2026 ( #357 ) ( a11d206 ) Edge case in Snowflake information_schema.last_altered value ( #360 ) ( c3e713e ) Correct typo in Snowflake Last Updated extract query ( #358 ) ( 5c2e98e ) Set Tableau URLs (base + API) via config ( #349 ) ( 1baec33 ) Fix invalid timestamp handling in dashboard transformer ( #339 ) ( 030ef49 ) Update postgres_sample_dag to set table extract job as upstream for elastic search publisher ( #340 ) ( c79935e ) deps: Unpin attrs ( #332 ) ( 86f658d ) Cypher statement param issue in Neo4jStalenessRemovalTask ( #307 ) ( 0078761 ) Added missing job tag key in hive_sample_dag.py ( #308 ) ( d6714b7 ) Fix sql for missing columns and mysql based dialects (#550) ( #305 ) ( 4b7b147 ) Escape backslashes in Neo4jCsvPublisher ( 1faa713 ) Variable organization in Model URL ( #293 ) ( b4c24ef )","title":"Fix"},{"location":"databuilder/CHANGELOG/#documentation","text":"Minor fixes to README ( #457 ) ( 54e89ce ) Update DashboardMetadata docs ( #402 ) ( 093b3d6 ) Fix broken doc link to dashboard_execution model ( #296 ) ( 24b3b0a ) Fix README.md ( #301 ) ( ad5765a )","title":"Documentation"},{"location":"databuilder/docs/dashboard_ingestion_guide/","text":"Dashboard Ingestion guidance \u00b6 (Currently this guidance is about using Databuilder to ingest Dashboard metadata into Neo4j and Elasticsearch) Dashboard ingestion consists of multiple Databuilder jobs and it can be described in four steps: Ingest base data to Neo4j. Ingest additional data and decorate Neo4j over base data. Update Elasticsearch index using Neo4j data Remove stale data Note that Databuilder jobs need to be sequenced as 1 -> 2 -> 3 -> 4. To sequencing these jobs, Lyft uses Airflow to orchestrate the job, but Databuilder is not limited to Airflow and you can also simply use Python script to sequence it \u2013 not recommended for production though. Also, step 1, 3, 4 is expected to have one Databuilder job where Step 2 is expected to have multiple Databuilder jobs and number of Databuilder jobs in step 2 is expected to grow as we add more metadata into Dashboard. To improve performance, it is recommended, but not required, to execute Databuilder jobs in step 2 concurrently. Once finished step 1 and 2, you will have Graph like this: Here this documentation will be using Mode Dashboard as concrete example to show how to ingest Dashboard metadata. However, this ingestion process not limited to Mode Dashboard and any other Dashboard can follow this flow. 1. Ingest base data to Neo4j. \u00b6 Using ModeDashboardExtractor along with FsNeo4jCSVLoader and Neo4jCsvPublisher to add base information such as Dashboard group name, Dashboard group id, Dashboard group description, Dashboard name, Dashboard id, Dashboard description to Neo4j. Use this job configuration example to configure the job. 2. Ingest additional data and decorate Neo4j over base data. \u00b6 Use other Mode dashboard\u2019s extractors in create & launch multiple Databuilder jobs. Note that it all Databuilder job here will use FsNeo4jCSVLoader and Neo4jCsvPublisher where their configuration should be almost the same except the NODE_FILES_DIR and RELATION_FILES_DIR that is being used for temporary location to hold data. List of other Extractors can be found here 2.1. Ingest Dashboard usage data and decorate Neo4j over base data. \u00b6 Mode provide usage data (view count) per Dashboard, but this is accumulated usage data. The main use case of usage is search ranking and accumulated usage is not that much useful for Amundsen as we don\u2019t want to show certain Dashboard that was popular years ago and potentially deprecated. To bring recent usage information, we can snapshot accumulated usage per report daily and extract recent usage information (past 30 days, 60 days, 90 days that fits our view of recency). 2.1.1. Ingest accumulated usage into Data warehouse (e.g: Hive, BigQuery, Redshift, Postgres, etc) \u00b6 In this step, you can use ModeDashboardUsageExtractor to extract accumulated_view_count and load into Data warehouse of your choice by using GenericLoader. Note that GenericLoader just takes a callback function, and you need to provide a function that INSERT record into your Dataware house. extractor = ModeDashboardUsageExtractor () loader = GenericLoader () task = DefaultTask ( extractor = extractor , loader = loader ) job_config = ConfigFactory . from_dict ({ ' {} . {} ' . format ( extractor . get_scope (), ORGANIZATION ): organization , ' {} . {} ' . format ( extractor . get_scope (), MODE_ACCESS_TOKEN ): mode_token , ' {} . {} ' . format ( extractor . get_scope (), MODE_PASSWORD_TOKEN ): mode_password , ' {} . {} ' . format ( loader . get_scope (), 'callback_function' ): mode_dashboard_usage_loader_callback_function , }) job = DefaultJob ( conf = job_config , task = task ) job . launch () Step 2. Extract past ? days usage data from your Data warehouse and publish it to Neo4j. You could use existing extractors to achieve this with DashboardUsage model along with FsNeo4jCSVLoader and Neo4jCsvPublisher . 3. Update Elasticsearch index using Neo4j data \u00b6 Once data is ready in Neo4j, extract Neo4j data and push it to Elasticsearch using Neo4jSearchDataExtractor and ElasticsearchPublisher tmp_es_file_path = '/var/tmp/amundsen_dashboard/elasticsearch_dashboard_upload/es_data.json' elasticsearch_new_index_name = 'dashboard_search_index_ {ds} _ {hex_str} ' . format ( ds = '2020-05-12' , hex_str = uuid . uuid4 () . hex ) elasticsearch_doc_type = 'dashboard' elasticsearch_index_alias = 'dashboard_search_index' job_config = ConfigFactory . from_dict ({ 'extractor.search_data.extractor.neo4j. {} ' . format ( Neo4jExtractor . GRAPH_URL_CONFIG_KEY ): neo4j_endpoint , 'extractor.search_data.extractor.neo4j. {} ' . format ( Neo4jExtractor . MODEL_CLASS_CONFIG_KEY ): 'databuilder.models.dashboard_elasticsearch_document.DashboardESDocument' , 'extractor.search_data.extractor.neo4j. {} ' . format ( Neo4jExtractor . NEO4J_AUTH_USER ): neo4j_user , 'extractor.search_data.extractor.neo4j. {} ' . format ( Neo4jExtractor . NEO4J_AUTH_PW ): neo4j_password , 'extractor.search_data.extractor.neo4j. {} ' . format ( Neo4jExtractor . NEO4J_ENCRYPTED ): False , 'extractor.search_data. {} ' . format ( Neo4jSearchDataExtractor . CYPHER_QUERY_CONFIG_KEY ): Neo4jSearchDataExtractor . DEFAULT_NEO4J_DASHBOARD_CYPHER_QUERY , 'extractor.search_data. {} ' . format ( neo4j_csv_publisher . JOB_PUBLISH_TAG ): job_publish_tag , 'loader.filesystem.elasticsearch. {} ' . format ( FSElasticsearchJSONLoader . FILE_PATH_CONFIG_KEY ): tmp_es_file_path , 'loader.filesystem.elasticsearch. {} ' . format ( FSElasticsearchJSONLoader . FILE_MODE_CONFIG_KEY ): 'w' , 'publisher.elasticsearch. {} ' . format ( ElasticsearchPublisher . FILE_PATH_CONFIG_KEY ): tmp_es_file_path , 'publisher.elasticsearch. {} ' . format ( ElasticsearchPublisher . FILE_MODE_CONFIG_KEY ): 'r' , 'publisher.elasticsearch. {} ' . format ( ElasticsearchPublisher . ELASTICSEARCH_CLIENT_CONFIG_KEY ): elasticsearch_client , 'publisher.elasticsearch. {} ' . format ( ElasticsearchPublisher . ELASTICSEARCH_NEW_INDEX_CONFIG_KEY ): elasticsearch_new_index , 'publisher.elasticsearch. {} ' . format ( ElasticsearchPublisher . ELASTICSEARCH_DOC_TYPE_CONFIG_KEY ): elasticsearch_doc_type , 'publisher.elasticsearch. {} ' . format ( ElasticsearchPublisher . ELASTICSEARCH_MAPPING_CONFIG_KEY ): DASHBOARD_ELASTICSEARCH_INDEX_MAPPING , 'publisher.elasticsearch. {} ' . format ( ElasticsearchPublisher . ELASTICSEARCH_ALIAS_CONFIG_KEY ): elasticsearch_index_alias , }) job = DefaultJob ( conf = job_config , task = DefaultTask ( extractor = Neo4jSearchDataExtractor (), loader = FSElasticsearchJSONLoader ()), publisher = ElasticsearchPublisher ()) job . launch () *Note that DASHBOARD_ELASTICSEARCH_INDEX_MAPPING is defined here . 4. Remove stale data \u00b6 Dashboard ingestion, like Table ingestion, is UPSERT (CREATE OR UPDATE) operation and there could be some data deleted on source. Not removing it in Neo4j basically leaving a stale data in Amundsen. You can use Neo4jStalenessRemovalTask to remove stale data. There are two strategies to remove stale data. One is to use job_publish_tag and the other one is to use milliseconds_to_expire . For example, you could use job_publish_tag to remove stale DashboardGroup , Dashboard , and Query nodes. And you could use milliseconds_to_expire on Timestamp node, READ relation, and READ_BY . One of the main reasons to use milliseconds_to_expire is to avoid race condition and it is explained more here","title":"Dashboard Ingestion guide"},{"location":"databuilder/docs/dashboard_ingestion_guide/#dashboard-ingestion-guidance","text":"(Currently this guidance is about using Databuilder to ingest Dashboard metadata into Neo4j and Elasticsearch) Dashboard ingestion consists of multiple Databuilder jobs and it can be described in four steps: Ingest base data to Neo4j. Ingest additional data and decorate Neo4j over base data. Update Elasticsearch index using Neo4j data Remove stale data Note that Databuilder jobs need to be sequenced as 1 -> 2 -> 3 -> 4. To sequencing these jobs, Lyft uses Airflow to orchestrate the job, but Databuilder is not limited to Airflow and you can also simply use Python script to sequence it \u2013 not recommended for production though. Also, step 1, 3, 4 is expected to have one Databuilder job where Step 2 is expected to have multiple Databuilder jobs and number of Databuilder jobs in step 2 is expected to grow as we add more metadata into Dashboard. To improve performance, it is recommended, but not required, to execute Databuilder jobs in step 2 concurrently. Once finished step 1 and 2, you will have Graph like this: Here this documentation will be using Mode Dashboard as concrete example to show how to ingest Dashboard metadata. However, this ingestion process not limited to Mode Dashboard and any other Dashboard can follow this flow.","title":"Dashboard Ingestion guidance"},{"location":"databuilder/docs/dashboard_ingestion_guide/#1-ingest-base-data-to-neo4j","text":"Using ModeDashboardExtractor along with FsNeo4jCSVLoader and Neo4jCsvPublisher to add base information such as Dashboard group name, Dashboard group id, Dashboard group description, Dashboard name, Dashboard id, Dashboard description to Neo4j. Use this job configuration example to configure the job.","title":"1. Ingest base data to Neo4j."},{"location":"databuilder/docs/dashboard_ingestion_guide/#2-ingest-additional-data-and-decorate-neo4j-over-base-data","text":"Use other Mode dashboard\u2019s extractors in create & launch multiple Databuilder jobs. Note that it all Databuilder job here will use FsNeo4jCSVLoader and Neo4jCsvPublisher where their configuration should be almost the same except the NODE_FILES_DIR and RELATION_FILES_DIR that is being used for temporary location to hold data. List of other Extractors can be found here","title":"2. Ingest additional data and decorate Neo4j over base data."},{"location":"databuilder/docs/dashboard_ingestion_guide/#21-ingest-dashboard-usage-data-and-decorate-neo4j-over-base-data","text":"Mode provide usage data (view count) per Dashboard, but this is accumulated usage data. The main use case of usage is search ranking and accumulated usage is not that much useful for Amundsen as we don\u2019t want to show certain Dashboard that was popular years ago and potentially deprecated. To bring recent usage information, we can snapshot accumulated usage per report daily and extract recent usage information (past 30 days, 60 days, 90 days that fits our view of recency).","title":"2.1. Ingest Dashboard usage data and decorate Neo4j over base data."},{"location":"databuilder/docs/dashboard_ingestion_guide/#211-ingest-accumulated-usage-into-data-warehouse-eg-hive-bigquery-redshift-postgres-etc","text":"In this step, you can use ModeDashboardUsageExtractor to extract accumulated_view_count and load into Data warehouse of your choice by using GenericLoader. Note that GenericLoader just takes a callback function, and you need to provide a function that INSERT record into your Dataware house. extractor = ModeDashboardUsageExtractor () loader = GenericLoader () task = DefaultTask ( extractor = extractor , loader = loader ) job_config = ConfigFactory . from_dict ({ ' {} . {} ' . format ( extractor . get_scope (), ORGANIZATION ): organization , ' {} . {} ' . format ( extractor . get_scope (), MODE_ACCESS_TOKEN ): mode_token , ' {} . {} ' . format ( extractor . get_scope (), MODE_PASSWORD_TOKEN ): mode_password , ' {} . {} ' . format ( loader . get_scope (), 'callback_function' ): mode_dashboard_usage_loader_callback_function , }) job = DefaultJob ( conf = job_config , task = task ) job . launch () Step 2. Extract past ? days usage data from your Data warehouse and publish it to Neo4j. You could use existing extractors to achieve this with DashboardUsage model along with FsNeo4jCSVLoader and Neo4jCsvPublisher .","title":"2.1.1. Ingest accumulated usage into Data warehouse (e.g: Hive, BigQuery, Redshift, Postgres, etc)"},{"location":"databuilder/docs/dashboard_ingestion_guide/#3-update-elasticsearch-index-using-neo4j-data","text":"Once data is ready in Neo4j, extract Neo4j data and push it to Elasticsearch using Neo4jSearchDataExtractor and ElasticsearchPublisher tmp_es_file_path = '/var/tmp/amundsen_dashboard/elasticsearch_dashboard_upload/es_data.json' elasticsearch_new_index_name = 'dashboard_search_index_ {ds} _ {hex_str} ' . format ( ds = '2020-05-12' , hex_str = uuid . uuid4 () . hex ) elasticsearch_doc_type = 'dashboard' elasticsearch_index_alias = 'dashboard_search_index' job_config = ConfigFactory . from_dict ({ 'extractor.search_data.extractor.neo4j. {} ' . format ( Neo4jExtractor . GRAPH_URL_CONFIG_KEY ): neo4j_endpoint , 'extractor.search_data.extractor.neo4j. {} ' . format ( Neo4jExtractor . MODEL_CLASS_CONFIG_KEY ): 'databuilder.models.dashboard_elasticsearch_document.DashboardESDocument' , 'extractor.search_data.extractor.neo4j. {} ' . format ( Neo4jExtractor . NEO4J_AUTH_USER ): neo4j_user , 'extractor.search_data.extractor.neo4j. {} ' . format ( Neo4jExtractor . NEO4J_AUTH_PW ): neo4j_password , 'extractor.search_data.extractor.neo4j. {} ' . format ( Neo4jExtractor . NEO4J_ENCRYPTED ): False , 'extractor.search_data. {} ' . format ( Neo4jSearchDataExtractor . CYPHER_QUERY_CONFIG_KEY ): Neo4jSearchDataExtractor . DEFAULT_NEO4J_DASHBOARD_CYPHER_QUERY , 'extractor.search_data. {} ' . format ( neo4j_csv_publisher . JOB_PUBLISH_TAG ): job_publish_tag , 'loader.filesystem.elasticsearch. {} ' . format ( FSElasticsearchJSONLoader . FILE_PATH_CONFIG_KEY ): tmp_es_file_path , 'loader.filesystem.elasticsearch. {} ' . format ( FSElasticsearchJSONLoader . FILE_MODE_CONFIG_KEY ): 'w' , 'publisher.elasticsearch. {} ' . format ( ElasticsearchPublisher . FILE_PATH_CONFIG_KEY ): tmp_es_file_path , 'publisher.elasticsearch. {} ' . format ( ElasticsearchPublisher . FILE_MODE_CONFIG_KEY ): 'r' , 'publisher.elasticsearch. {} ' . format ( ElasticsearchPublisher . ELASTICSEARCH_CLIENT_CONFIG_KEY ): elasticsearch_client , 'publisher.elasticsearch. {} ' . format ( ElasticsearchPublisher . ELASTICSEARCH_NEW_INDEX_CONFIG_KEY ): elasticsearch_new_index , 'publisher.elasticsearch. {} ' . format ( ElasticsearchPublisher . ELASTICSEARCH_DOC_TYPE_CONFIG_KEY ): elasticsearch_doc_type , 'publisher.elasticsearch. {} ' . format ( ElasticsearchPublisher . ELASTICSEARCH_MAPPING_CONFIG_KEY ): DASHBOARD_ELASTICSEARCH_INDEX_MAPPING , 'publisher.elasticsearch. {} ' . format ( ElasticsearchPublisher . ELASTICSEARCH_ALIAS_CONFIG_KEY ): elasticsearch_index_alias , }) job = DefaultJob ( conf = job_config , task = DefaultTask ( extractor = Neo4jSearchDataExtractor (), loader = FSElasticsearchJSONLoader ()), publisher = ElasticsearchPublisher ()) job . launch () *Note that DASHBOARD_ELASTICSEARCH_INDEX_MAPPING is defined here .","title":"3. Update Elasticsearch index using Neo4j data"},{"location":"databuilder/docs/dashboard_ingestion_guide/#4-remove-stale-data","text":"Dashboard ingestion, like Table ingestion, is UPSERT (CREATE OR UPDATE) operation and there could be some data deleted on source. Not removing it in Neo4j basically leaving a stale data in Amundsen. You can use Neo4jStalenessRemovalTask to remove stale data. There are two strategies to remove stale data. One is to use job_publish_tag and the other one is to use milliseconds_to_expire . For example, you could use job_publish_tag to remove stale DashboardGroup , Dashboard , and Query nodes. And you could use milliseconds_to_expire on Timestamp node, READ relation, and READ_BY . One of the main reasons to use milliseconds_to_expire is to avoid race condition and it is explained more here","title":"4. Remove stale data"},{"location":"databuilder/docs/models/","text":"Amundsen Models \u00b6 Overview \u00b6 These are the python classes that live in databuilder/models/ . Models represent the data structures that live in either neo4j (if the model extends Neo4jSerializable) or in elasticsearch. Models that extend Neo4jSerializable have methods to create: - the nodes - the relationships In this way, amundsendatabuilder pipelines can create python objects that can then be loaded into neo4j / elastic search without developers needing to know the internals of the neo4j schema. The Models \u00b6 TableMetadata \u00b6 What datasets does my org have? Description \u00b6 This corresponds to a dataset in amundsen and is the core building block. In addition to ColumnMetadata, tableMetadata is one of the first datasets you should extract as almost everything else depends on these being populated. Extraction \u00b6 In general, for Table and Column Metadata, you should be able to use one of the pre-made extractors in the extractor package Watermark \u00b6 What is the earliest data that this table has? What is the latest data? This is NOT the same as when the data was last updated. Description \u00b6 Corresponds to the earliest and latest date that a dataset has. Only makes sense if the dataset is timeseries data. For example, a given table may have data from 2019/01/01 -> 2020/01/01 In that case the low watermark is 2019/01/01 and the high watermark is 2020/01/01. Extraction \u00b6 Depending on the datastore of your dataset, you would extract this by: - a query on the minimum and maximum partition (hive) - a query for the minimum and maximum record of a given timestamp column ColumnUsageModel \u00b6 How many queries is a given column getting? By which users? Has query counts per a given column per a user. This can help identify Description \u00b6 who uses given datasets so people can contact them if they have questions on how to use a given dataset or if a dataset is changing. It is also used as a search boost so that the most used tables are put to the top of the search results. For more traditional databases, there should be system tables where you can obtain Extraction \u00b6 these sorts of usage statistics. In other cases, you may need to use audit logs which could require a custom solution. Finally, for none traditional data lakes, getting this information exactly maybe difficult and you may need to rely on a heuristic. User \u00b6 What users are there out there? Which team is this user on? Description \u00b6 Represents all of the metadata for a user at your company. This is required if you are going to be having authentication turned on. Extraction \u00b6 TODO TableColumnStats \u00b6 What are the min/max values for this column? How many nulls are in this column? * Description \u00b6 This represents statistics on the column level (this is not for table level metrics). The idea is that different companies will want to track different things about different columns, so this is highly customizable. It also will probably require a distributed cluster in order to calculate these regularly and in general is probably the least accessible metrics to get at without a custom solution. Extraction \u00b6 The idea here would be to implement something that does the following: For each table you care about: For each column you care about: Calculate statistics that you care about such as min/max/average etc. Application \u00b6 What job/application is writing to this table? * Description \u00b6 This is used to provide users a way to find out what job/application is responsible for writing to this dataset. Currently the model assumes the application has to be in airflow, but in theory it could be generalized to other orchestration frameworks. Extraction \u00b6 TODO Table Owner \u00b6 What team or user owns this dataset? * Description \u00b6 A dataset can have one or more owners. These owners are used when requesting table descriptions or could be just a useful point of contact for a user inquiring about how to use a dataset. Extraction \u00b6 Although the main point of entry for owners is through the WebUI, you could in theory extract this information based on who created a given table. Table Source \u00b6 Where is the source code for the application that writes to this dataset? * Description \u00b6 Generally there is going to be code that your company owns that describes how a dataset is created. This model is what represents the link and type of repository to this source code so it is available to users. Extraction \u00b6 You will need a github/gitlab/your repository crawler in order to populate this automatically. The idea there would be to search for a given table name or something else that is a unique identifier such that you can be confident that the source correctly matches to this table. TableLastUpdated \u00b6 When was the last time this data was updated? Is this table stale or deprecated? * Description \u00b6 This value is used to describe the last time the table had datapoints inserted into it. It is a very useful value as it can help users identify if there are tables that are no longer being updated. Extraction \u00b6 There are some extractors available for this like hive_table_last_updated_extractor that you can refer to. But you will need access to history that provides information on when the last data write happened on a given table. If this data isn\u2019t available for your data source, you maybe able to approximate it by looking at the max of some timestamp column. Dashboard models \u00b6 Dashboard models are normalized which means that the model is separated so that it can be easily decoupled with how data is extracted. (If model is denormalized, all metadata is in model, then one extraction needs to able to pull all the data which makes extraction hard and complex) There\u2019s trade off in this decision of normalized design where it can be inefficient in the case that some ingestion can be done in one job for metadata source happen to provide all data it need. However, to make model flexible for most of metadata, it is normalized. DashboardMetadata \u00b6 Description \u00b6 A baseline of Dashboard metadata that consists of dashboard group name, dashboard group description, dashboard description, etc. This model needs to be ingested first as other model builds relation to this. Extraction \u00b6 ModeDashboardExtractor DashboardOwner \u00b6 Description \u00b6 A model that encapsulate Dashboard\u2019s owner. Note that it does not create new user as it has insufficient information about user but it builds relation between User and Dashboard Extraction \u00b6 ModeDashboardOwnerExtractor DashboardTable \u00b6 A model that link Dashboard with the tables used in various charts of the dashboard. Note that it does not create new dashboard, table as it has insufficient information but it builds relation between Tables and Dashboard. Supporting extractor: Currently there\u2019s no open sourced extractor for this. In Lyft, there\u2019s audit table that records SQL query, where it came from with identifier, along with tables that is used in SQL query. We basically query this table via DBAPIExtractor DashboardUsage \u00b6 Description \u00b6 A model that encapsulate Dashboard usage between Dashboard and User Extraction \u00b6 You can use ModeDashboardUsageExtractor . However, currently Mode only provides accumulated view count where we need recent view counts (past 30, 60, or 90 days). To get recent view count, in Lyft, we use ModeDashboardUsageExtractor to extract accumulated view count and GenericLoader to load its record (no publisher here and publisher is not mandatory in DefaultJob) as a event where event materialized as daily snapshot. Once it captures daily accumulated view count, ingest recent view count by querying the datastore. In Lyft, we query via DBAPIExtractor through Presto. DashboardLastModifiedTimestamp \u00b6 Description \u00b6 A model that encapsulate Dashboard\u2019s last modified timestamp in epoch Extraction \u00b6 ModeDashboardLastModifiedTimestampExtractor DashboardExecution \u00b6 A model that encapsulate Dashboard\u2019s execution timestamp in epoch and execution state. Note that this model supports last_execution and last_successful_execution by using different identifier in the URI. Extraction \u00b6 ModeDashboardExecutionsExtractor which extracts last_execution. ModeDashboardLastSuccessfulExecutionExtractor DashboardQuery \u00b6 Description \u00b6 A model that encapsulate Dashboard\u2019s query information. Supporting extractor: ModeDashboardQueriesExtractor DashboardChart \u00b6 Description \u00b6 A model that encapsulate Dashboard\u2019s charts where chart is associated with query. Extraction \u00b6 ModeDashboardChartsExtractor Feature models \u00b6 Feature models include FeatureMetadata , which encapsulates the basic feature details, and supplemental models Feature_Generation_Code and Feature_Watermark for adding extra metadata. In addition, the Tag, Badge, Owner, and Programmatic_Description models work with features. FeatureMetadata \u00b6 Description \u00b6 A baseline of Feature metadata. This model needs to be ingested first as other models build relations to it. Extraction \u00b6 No specific extractors are provided at this time. We expect users will either write custom extractors, or use generic extractors (e.g. SQLAlchemyExtractor). Feature_Generation_Code \u00b6 Description \u00b6 Allows ingesting the text of the generation code (SQL or otherwise) which was used to create a feature. Extraction \u00b6 No specific extractors are provided at this time. We expect users will either write custom extractors, or use generic extractors (e.g. SQLAlchemyExtractor). Feature_Watermark \u00b6 Description \u00b6 Allows ingesting the high and low data range of a feature. Unlike Watermark , which is specific to tables (requires a partition, for example), Feature_Watermark is more general and does not care about how the feature is stored. Extraction \u00b6 No specific extractors are provided at this time. We expect users will either write custom extractors, or use generic extractors (e.g. SQLAlchemyExtractor).","title":"Models"},{"location":"databuilder/docs/models/#amundsen-models","text":"","title":"Amundsen Models"},{"location":"databuilder/docs/models/#overview","text":"These are the python classes that live in databuilder/models/ . Models represent the data structures that live in either neo4j (if the model extends Neo4jSerializable) or in elasticsearch. Models that extend Neo4jSerializable have methods to create: - the nodes - the relationships In this way, amundsendatabuilder pipelines can create python objects that can then be loaded into neo4j / elastic search without developers needing to know the internals of the neo4j schema.","title":"Overview"},{"location":"databuilder/docs/models/#the-models","text":"","title":"The Models"},{"location":"databuilder/docs/models/#tablemetadata","text":"What datasets does my org have?","title":"TableMetadata"},{"location":"databuilder/docs/models/#description","text":"This corresponds to a dataset in amundsen and is the core building block. In addition to ColumnMetadata, tableMetadata is one of the first datasets you should extract as almost everything else depends on these being populated.","title":"Description"},{"location":"databuilder/docs/models/#extraction","text":"In general, for Table and Column Metadata, you should be able to use one of the pre-made extractors in the extractor package","title":"Extraction"},{"location":"databuilder/docs/models/#watermark","text":"What is the earliest data that this table has? What is the latest data? This is NOT the same as when the data was last updated.","title":"Watermark"},{"location":"databuilder/docs/models/#description_1","text":"Corresponds to the earliest and latest date that a dataset has. Only makes sense if the dataset is timeseries data. For example, a given table may have data from 2019/01/01 -> 2020/01/01 In that case the low watermark is 2019/01/01 and the high watermark is 2020/01/01.","title":"Description"},{"location":"databuilder/docs/models/#extraction_1","text":"Depending on the datastore of your dataset, you would extract this by: - a query on the minimum and maximum partition (hive) - a query for the minimum and maximum record of a given timestamp column","title":"Extraction"},{"location":"databuilder/docs/models/#columnusagemodel","text":"How many queries is a given column getting? By which users? Has query counts per a given column per a user. This can help identify","title":"ColumnUsageModel"},{"location":"databuilder/docs/models/#description_2","text":"who uses given datasets so people can contact them if they have questions on how to use a given dataset or if a dataset is changing. It is also used as a search boost so that the most used tables are put to the top of the search results. For more traditional databases, there should be system tables where you can obtain","title":"Description"},{"location":"databuilder/docs/models/#extraction_2","text":"these sorts of usage statistics. In other cases, you may need to use audit logs which could require a custom solution. Finally, for none traditional data lakes, getting this information exactly maybe difficult and you may need to rely on a heuristic.","title":"Extraction"},{"location":"databuilder/docs/models/#user","text":"What users are there out there? Which team is this user on?","title":"User"},{"location":"databuilder/docs/models/#description_3","text":"Represents all of the metadata for a user at your company. This is required if you are going to be having authentication turned on.","title":"Description"},{"location":"databuilder/docs/models/#extraction_3","text":"TODO","title":"Extraction"},{"location":"databuilder/docs/models/#tablecolumnstats","text":"What are the min/max values for this column? How many nulls are in this column? *","title":"TableColumnStats"},{"location":"databuilder/docs/models/#description_4","text":"This represents statistics on the column level (this is not for table level metrics). The idea is that different companies will want to track different things about different columns, so this is highly customizable. It also will probably require a distributed cluster in order to calculate these regularly and in general is probably the least accessible metrics to get at without a custom solution.","title":"Description"},{"location":"databuilder/docs/models/#extraction_4","text":"The idea here would be to implement something that does the following: For each table you care about: For each column you care about: Calculate statistics that you care about such as min/max/average etc.","title":"Extraction"},{"location":"databuilder/docs/models/#application","text":"What job/application is writing to this table? *","title":"Application"},{"location":"databuilder/docs/models/#description_5","text":"This is used to provide users a way to find out what job/application is responsible for writing to this dataset. Currently the model assumes the application has to be in airflow, but in theory it could be generalized to other orchestration frameworks.","title":"Description"},{"location":"databuilder/docs/models/#extraction_5","text":"TODO","title":"Extraction"},{"location":"databuilder/docs/models/#table-owner","text":"What team or user owns this dataset? *","title":"Table Owner"},{"location":"databuilder/docs/models/#description_6","text":"A dataset can have one or more owners. These owners are used when requesting table descriptions or could be just a useful point of contact for a user inquiring about how to use a dataset.","title":"Description"},{"location":"databuilder/docs/models/#extraction_6","text":"Although the main point of entry for owners is through the WebUI, you could in theory extract this information based on who created a given table.","title":"Extraction"},{"location":"databuilder/docs/models/#table-source","text":"Where is the source code for the application that writes to this dataset? *","title":"Table Source"},{"location":"databuilder/docs/models/#description_7","text":"Generally there is going to be code that your company owns that describes how a dataset is created. This model is what represents the link and type of repository to this source code so it is available to users.","title":"Description"},{"location":"databuilder/docs/models/#extraction_7","text":"You will need a github/gitlab/your repository crawler in order to populate this automatically. The idea there would be to search for a given table name or something else that is a unique identifier such that you can be confident that the source correctly matches to this table.","title":"Extraction"},{"location":"databuilder/docs/models/#tablelastupdated","text":"When was the last time this data was updated? Is this table stale or deprecated? *","title":"TableLastUpdated"},{"location":"databuilder/docs/models/#description_8","text":"This value is used to describe the last time the table had datapoints inserted into it. It is a very useful value as it can help users identify if there are tables that are no longer being updated.","title":"Description"},{"location":"databuilder/docs/models/#extraction_8","text":"There are some extractors available for this like hive_table_last_updated_extractor that you can refer to. But you will need access to history that provides information on when the last data write happened on a given table. If this data isn\u2019t available for your data source, you maybe able to approximate it by looking at the max of some timestamp column.","title":"Extraction"},{"location":"databuilder/docs/models/#dashboard-models","text":"Dashboard models are normalized which means that the model is separated so that it can be easily decoupled with how data is extracted. (If model is denormalized, all metadata is in model, then one extraction needs to able to pull all the data which makes extraction hard and complex) There\u2019s trade off in this decision of normalized design where it can be inefficient in the case that some ingestion can be done in one job for metadata source happen to provide all data it need. However, to make model flexible for most of metadata, it is normalized.","title":"Dashboard models"},{"location":"databuilder/docs/models/#dashboardmetadata","text":"","title":"DashboardMetadata"},{"location":"databuilder/docs/models/#description_9","text":"A baseline of Dashboard metadata that consists of dashboard group name, dashboard group description, dashboard description, etc. This model needs to be ingested first as other model builds relation to this.","title":"Description"},{"location":"databuilder/docs/models/#extraction_9","text":"ModeDashboardExtractor","title":"Extraction"},{"location":"databuilder/docs/models/#dashboardowner","text":"","title":"DashboardOwner"},{"location":"databuilder/docs/models/#description_10","text":"A model that encapsulate Dashboard\u2019s owner. Note that it does not create new user as it has insufficient information about user but it builds relation between User and Dashboard","title":"Description"},{"location":"databuilder/docs/models/#extraction_10","text":"ModeDashboardOwnerExtractor","title":"Extraction"},{"location":"databuilder/docs/models/#dashboardtable","text":"A model that link Dashboard with the tables used in various charts of the dashboard. Note that it does not create new dashboard, table as it has insufficient information but it builds relation between Tables and Dashboard. Supporting extractor: Currently there\u2019s no open sourced extractor for this. In Lyft, there\u2019s audit table that records SQL query, where it came from with identifier, along with tables that is used in SQL query. We basically query this table via DBAPIExtractor","title":"DashboardTable"},{"location":"databuilder/docs/models/#dashboardusage","text":"","title":"DashboardUsage"},{"location":"databuilder/docs/models/#description_11","text":"A model that encapsulate Dashboard usage between Dashboard and User","title":"Description"},{"location":"databuilder/docs/models/#extraction_11","text":"You can use ModeDashboardUsageExtractor . However, currently Mode only provides accumulated view count where we need recent view counts (past 30, 60, or 90 days). To get recent view count, in Lyft, we use ModeDashboardUsageExtractor to extract accumulated view count and GenericLoader to load its record (no publisher here and publisher is not mandatory in DefaultJob) as a event where event materialized as daily snapshot. Once it captures daily accumulated view count, ingest recent view count by querying the datastore. In Lyft, we query via DBAPIExtractor through Presto.","title":"Extraction"},{"location":"databuilder/docs/models/#dashboardlastmodifiedtimestamp","text":"","title":"DashboardLastModifiedTimestamp"},{"location":"databuilder/docs/models/#description_12","text":"A model that encapsulate Dashboard\u2019s last modified timestamp in epoch","title":"Description"},{"location":"databuilder/docs/models/#extraction_12","text":"ModeDashboardLastModifiedTimestampExtractor","title":"Extraction"},{"location":"databuilder/docs/models/#dashboardexecution","text":"A model that encapsulate Dashboard\u2019s execution timestamp in epoch and execution state. Note that this model supports last_execution and last_successful_execution by using different identifier in the URI.","title":"DashboardExecution"},{"location":"databuilder/docs/models/#extraction_13","text":"ModeDashboardExecutionsExtractor which extracts last_execution. ModeDashboardLastSuccessfulExecutionExtractor","title":"Extraction"},{"location":"databuilder/docs/models/#dashboardquery","text":"","title":"DashboardQuery"},{"location":"databuilder/docs/models/#description_13","text":"A model that encapsulate Dashboard\u2019s query information. Supporting extractor: ModeDashboardQueriesExtractor","title":"Description"},{"location":"databuilder/docs/models/#dashboardchart","text":"","title":"DashboardChart"},{"location":"databuilder/docs/models/#description_14","text":"A model that encapsulate Dashboard\u2019s charts where chart is associated with query.","title":"Description"},{"location":"databuilder/docs/models/#extraction_14","text":"ModeDashboardChartsExtractor","title":"Extraction"},{"location":"databuilder/docs/models/#feature-models","text":"Feature models include FeatureMetadata , which encapsulates the basic feature details, and supplemental models Feature_Generation_Code and Feature_Watermark for adding extra metadata. In addition, the Tag, Badge, Owner, and Programmatic_Description models work with features.","title":"Feature models"},{"location":"databuilder/docs/models/#featuremetadata","text":"","title":"FeatureMetadata"},{"location":"databuilder/docs/models/#description_15","text":"A baseline of Feature metadata. This model needs to be ingested first as other models build relations to it.","title":"Description"},{"location":"databuilder/docs/models/#extraction_15","text":"No specific extractors are provided at this time. We expect users will either write custom extractors, or use generic extractors (e.g. SQLAlchemyExtractor).","title":"Extraction"},{"location":"databuilder/docs/models/#feature_generation_code","text":"","title":"Feature_Generation_Code"},{"location":"databuilder/docs/models/#description_16","text":"Allows ingesting the text of the generation code (SQL or otherwise) which was used to create a feature.","title":"Description"},{"location":"databuilder/docs/models/#extraction_16","text":"No specific extractors are provided at this time. We expect users will either write custom extractors, or use generic extractors (e.g. SQLAlchemyExtractor).","title":"Extraction"},{"location":"databuilder/docs/models/#feature_watermark","text":"","title":"Feature_Watermark"},{"location":"databuilder/docs/models/#description_17","text":"Allows ingesting the high and low data range of a feature. Unlike Watermark , which is specific to tables (requires a partition, for example), Feature_Watermark is more general and does not care about how the feature is stored.","title":"Description"},{"location":"databuilder/docs/models/#extraction_17","text":"No specific extractors are provided at this time. We expect users will either write custom extractors, or use generic extractors (e.g. SQLAlchemyExtractor).","title":"Extraction"},{"location":"frontend/","text":"Amundsen Frontend Service \u00b6 Amundsen is a metadata driven application for improving the productivity of data analysts, data scientists and engineers when interacting with data. It does that today by indexing data resources (tables, dashboards, streams, etc.) and powering a page-rank style search based on usage patterns (e.g. highly queried tables show up earlier than less queried tables). Think of it as Google search for data. The project is named after Norwegian explorer Roald Amundsen , the first person to discover South Pole. The frontend service leverages a separate search service for allowing users to search for data resources, and a separate metadata service for viewing and editing metadata for a given resource. It is a Flask application with a React frontend. For information about Amundsen and our other services, refer to this README.md . Please also see our instructions for a quick start setup of Amundsen with dummy data, and an overview of the architecture . Requirements \u00b6 Python >= 3.6 Node = v10 or v12 npm >= 6.x.x Homepage \u00b6 https://www.amundsen.io/ Documentation \u00b6 https://www.amundsen.io/amundsen/ User Interface \u00b6 Please note that the mock images only served as demonstration purpose. Landing Page : The landing page for Amundsen including 1. search bars; 2. popular used tables; Search Preview : See inline search results as you type Table Detail Page : Visualization of a Hive / Redshift table Column detail : Visualization of columns of a Hive / Redshift table which includes an optional stats display Data Preview Page : Visualization of table data preview which could integrate with Apache Superset Installation \u00b6 Please visit Installation guideline on how to install Amundsen. Configuration \u00b6 Please visit Configuration doc on how to configure Amundsen various enviroment settings(local vs production). Developer Guidelines \u00b6 Please visit Developer guidelines if you want to build Amundsen in your local environment. License \u00b6 Apache 2.0 License.","title":"Overview"},{"location":"frontend/#amundsen-frontend-service","text":"Amundsen is a metadata driven application for improving the productivity of data analysts, data scientists and engineers when interacting with data. It does that today by indexing data resources (tables, dashboards, streams, etc.) and powering a page-rank style search based on usage patterns (e.g. highly queried tables show up earlier than less queried tables). Think of it as Google search for data. The project is named after Norwegian explorer Roald Amundsen , the first person to discover South Pole. The frontend service leverages a separate search service for allowing users to search for data resources, and a separate metadata service for viewing and editing metadata for a given resource. It is a Flask application with a React frontend. For information about Amundsen and our other services, refer to this README.md . Please also see our instructions for a quick start setup of Amundsen with dummy data, and an overview of the architecture .","title":"Amundsen Frontend Service"},{"location":"frontend/#requirements","text":"Python >= 3.6 Node = v10 or v12 npm >= 6.x.x","title":"Requirements"},{"location":"frontend/#homepage","text":"https://www.amundsen.io/","title":"Homepage"},{"location":"frontend/#documentation","text":"https://www.amundsen.io/amundsen/","title":"Documentation"},{"location":"frontend/#user-interface","text":"Please note that the mock images only served as demonstration purpose. Landing Page : The landing page for Amundsen including 1. search bars; 2. popular used tables; Search Preview : See inline search results as you type Table Detail Page : Visualization of a Hive / Redshift table Column detail : Visualization of columns of a Hive / Redshift table which includes an optional stats display Data Preview Page : Visualization of table data preview which could integrate with Apache Superset","title":"User Interface"},{"location":"frontend/#installation","text":"Please visit Installation guideline on how to install Amundsen.","title":"Installation"},{"location":"frontend/#configuration","text":"Please visit Configuration doc on how to configure Amundsen various enviroment settings(local vs production).","title":"Configuration"},{"location":"frontend/#developer-guidelines","text":"Please visit Developer guidelines if you want to build Amundsen in your local environment.","title":"Developer Guidelines"},{"location":"frontend/#license","text":"Apache 2.0 License.","title":"License"},{"location":"frontend/CHANGELOG/","text":"Feature \u00b6 Table and Column Lineage Polish ( #970 ) ( cd2f4c4 ) Table and Column Lineage Lists ( #969 ) ( df9532a ) Add Table Notices ( #957 ) ( e3be638 ) Allows for splitting stats\u2019 distinct values into a different element that shows in modal ( #960 ) ( fe04a06 ) Fix \u00b6 Upgrade mypy version to build with Python3.8 ( #975 ) ( 18963ec ) Handles parsing errors when format not expected on distinct values ( #966 ) ( 473bbdb ) Made commit author consistent ( #917 ) ( 48441cd ) Yaml syntax error ( #913 ) ( 8f49627 ) Add chore to monthly release PRs ( #912 ) ( 9323862 ) Removed echo for changelog command ( #910 ) ( bb22d4d ) Add changelog file ( #907 ) ( f06c50e ) Made change to preserve format of changelog ( #896 ) ( 0d56d72 ) Fixed reviewers field syntax error ( #892 ) ( b7f99d4 ) Made branch eval and added reviewers ( #891 ) ( dd57d44 ) Changed release workflow completely ( #882 ) ( 5dfcd09 ) Index tag info into elasticsearch immediately after ui change ( #883 ) ( b34151c )","title":"CHANGELOG"},{"location":"frontend/CHANGELOG/#feature","text":"Table and Column Lineage Polish ( #970 ) ( cd2f4c4 ) Table and Column Lineage Lists ( #969 ) ( df9532a ) Add Table Notices ( #957 ) ( e3be638 ) Allows for splitting stats\u2019 distinct values into a different element that shows in modal ( #960 ) ( fe04a06 )","title":"Feature"},{"location":"frontend/CHANGELOG/#fix","text":"Upgrade mypy version to build with Python3.8 ( #975 ) ( 18963ec ) Handles parsing errors when format not expected on distinct values ( #966 ) ( 473bbdb ) Made commit author consistent ( #917 ) ( 48441cd ) Yaml syntax error ( #913 ) ( 8f49627 ) Add chore to monthly release PRs ( #912 ) ( 9323862 ) Removed echo for changelog command ( #910 ) ( bb22d4d ) Add changelog file ( #907 ) ( f06c50e ) Made change to preserve format of changelog ( #896 ) ( 0d56d72 ) Fixed reviewers field syntax error ( #892 ) ( b7f99d4 ) Made branch eval and added reviewers ( #891 ) ( dd57d44 ) Changed release workflow completely ( #882 ) ( 5dfcd09 ) Index tag info into elasticsearch immediately after ui change ( #883 ) ( b34151c )","title":"Fix"},{"location":"frontend/docs/application_config/","text":"Application configuration \u00b6 This document describes how to leverage the frontend service\u2019s application configuration to configure particular features. After modifying the AppConfigCustom object in config-custom.ts in the ways described in this document, be sure to rebuild your application with these changes. NOTE: This document is a work in progress and does not include 100% of features. We welcome PRs to complete this document Announcements Config \u00b6 Annoncements is a feature that allows to disclose new features, changes or any other news to Amundsen\u2019s users. Announcements in the homepage To enable this feature, change the announcements.enable boolean value by overriding it on config-custom.ts . Once activated, an \u201cAnnouncements\u201d link will be available in the global navigation, and a new list of announcements will show up on the right sidebar on the Homepage. Badge Config \u00b6 Badges are a special type of tag that cannot be edited through the UI. BadgeConfig can be used to customize the text and color of badges. This config defines a mapping of badge name to a BadgeStyle and optional displayName . Badges that are not defined will default to use the BadgeStyle.default style and displayName use the badge name with any _ or - characters replaced with a space. Browse Tags Feature \u00b6 TODO: Please add doc Custom Logo \u00b6 Add your logo to the folder in amundsen_application/static/images/ . Set the the logoPath key on the to the location of your image. Date \u00b6 This config allows you to specify various date formats across the app. There are three date formats in use shown below. These correspond to the formatDate , formatDateTimeShort and formatDateTimeLong utility functions. 1 2 3 default: 'MMM DD, YYYY' dateTimeShort: 'MMM DD, YYYY ha z' dateTimeLong: 'MMMM Do YYYY [at] h:mm:ss a' Reference for formatting: https://devhints.io/datetime#momentjs-format Analytics \u00b6 Amundsen supports pluggable user behavior analytics via the analytics library. To emit analytics to a given destination, you must use one of the provided plugins (open a PR if you need to install a different vendor), then specify it the config passing the configuration of your account. Multiple destinations are supported if you wish to emit to multiple backends simultaneously. For example, to use Google analytics, you must add the import at the top of your config-custom.ts file: import googleAnalytics from '@analytics/google-analytics'; , then add this config block: analytics: { plugins: [ googleAnalytics({ trackingId: '<YOUR_UA_CODE>', sampleRate: 100 }), ], } We provide out of the box support for Mixpanel, Segment and Google Analytics. All @analytics/ plugins are potentially supported, but you must first install the plugin: npm install @analytics/<provider> and send us a PR with it before you can use it. Indexing Optional Resources \u00b6 In Amundsen, we currently support indexing other optional resources beyond tables. Index Users \u00b6 Users themselves are data resources and user metadata helps to facilitate network based discovery. When users are indexed they will show up in search results, and selecting a user surfaces a profile page that displays that user\u2019s relationships with different data resources. After ingesting user metadata into the search and metadata services, set IndexUsersConfig.enabled to true on the application configuration to display the UI for the aforementioned features. Index Dashboards \u00b6 Introducing dashboards into Amundsen allows users to discovery data analysis that has been already done. When dashboards are indexed they will show up in search results, and selecting a dashboard surfaces a page where users can explore dashboard metadata. After ingesting dashboard metadata into the search and metadata services, set IndexDashboardsConfig.enabled to true on the application configuration to display the UI for the aforementioned features. Mail Client Features \u00b6 Amundsen has two features that leverage the custom mail client \u2013 the feedback tool and notifications. As these are optional features, our MailClientFeaturesConfig can be used to hide/display any UI related to these features: Set MailClientFeaturesConfig.feedbackEnabled to true in order to display the Feedback component in the UI. Set MailClientFeaturesConfig.notificationsEnabled to true in order to display the optional UI for users to request more information about resources on the TableDetail page. For information about how to configure a custom mail client, please see this entry in our flask configuration doc. Navigation Links \u00b6 TODO: Please add doc Resource Configurations \u00b6 This configuration drives resource specific aspects of the application\u2019s user interface. Each supported resource should be mapped to an object that matches or extends the BaseResourceConfig . Base Configuration \u00b6 All resource configurations must match or extend the BaseResourceConfig . This configuration supports the following options: displayName : The name displayed throughout the application to refer to this resource type. filterCategories : An optional FilterConfig object. When set for a given resource, that resource will display filter options in the search page UI. supportedSources : An optional SourcesConfig object. Filter Categories \u00b6 The FilterConfig is an array of objects that match any of the supported filter options. We currently support a MultiSelectFilterCategory and a SingleFilterCategory . See our config-types for more information about each option. Supported Sources \u00b6 The SourcesConfig can be used for the customizations detailed below. See examples in config-default.ts . Custom Icons \u00b6 You can configure custom icons to be used throughout the UI when representing entities from particular sources. On the supportedSources object, add an entry with the id used to reference that source and map to an object that specifies the iconClass for that database. This iconClass should be defined in icons.scss . Display Names \u00b6 You can configure a specific display name to be used throughout the UI when representing entities from particular sources. On the supportedSources object, add an entry with the id used to reference that source and map to an object that specified the displayName for that source. Table Configuration \u00b6 To configure Table related features we have created a new resource configuration TableResourceConfig which extends BaseResourceConfig . In addition to the configurations explained above it also supports supportedDescriptionSources . Supported Description Sources \u00b6 A table resource may have a source of table and column description attached to it. We can customize it by using supportedDescriptionSources object which is an optional object. This object has displayName and iconPath , which can be used throughout the UI to represent a particular description source. See example in config-default.ts . For configuring new description sources, add an entry in supportedDescriptionSources with the id used to reference that source and add desired display name and icon for it. Table Stats \u00b6 If you have a stat field that is made of a JSON like set of value names and counts, you can show that as a set of \u201cunique values\u201d. You can see an example of this in the following figure: To achieve this, you will need to modify your custom configuration (config-custom.ts) by adding the name of the stat_type field that holds these values. You can find the config property in the stats section for table resource: [ResourceType.table]: { //... stats: { uniqueValueTypeName: \"keyNameExample\", }, } The unique values set needs to be an object like this: { end_epoch: 1609522182, start_epoch: 1608917382, stat_type: 'keyNameExample', stat_val: \"{'Category': 66, 'AnotherCategory': 54, 'More': 48}\", }, Notices \u00b6 We now can add notices to tables and dashboards. These notices allows Amundsen administrators to show informational, warning and alert messages related to the different resources (tables, dashboards, eventually people) we expose in Amundsen. This feature help administrators show messages related to deprecation, updates (or lack of), and informational messages related to specific resources. A notice is a small box with an icon and a message containing HTML markup (like links and bolded text). These will come in three flavors: Informational: Marked with a blue \"i\" icon on the right side Warning: Marked with an orange exclamation mark icon on the right side Alert: Marked with a red exclamation mark icon on the right side To set them up, we\u2019ll use the current configuration objects for the resources. For example, if company X wants to deprecate the use of one table or dashboard, they can opt to add new notices in their configuration file: resourceConfig: { [ResourceType.table]: { ... //Table Resource Configuration notices: { \"<CLUSTER>.<DATABASE>.<SCHEMA>.<TABLENAME>\": { severity: NoticeSeverity.ALERT, messageHtml: `This table is deprecated, please use <a href=\"<LINKTONEWTABLEDETAILPAGE>\">this new table</a> instead.`, }, }, }, [ResourceType.dashboard]: { ... //Dashboard Resource Configuration notices: { \"<PRODUCT>.<CLUSTER>.<GROUPNAME>.<DASHBOARDNAME>\": { severity: NoticeSeverity.WARNING, messageHtml: `This dashboard is deprecated, please use <a href=\"<LINKTONEWDASHBOARDDETAILPAGE>\">this new dashboard</a> instead.`, }, }, }, }, The above code will show a notice with a red exclamation icon whenever a final user visits the table\u2019s Table Detail page or the Dashboard Detail page. This feature\u2019s ultimate goal is to allow Amundsen administrators to point their users to more trusted/higher quality resources without removing the old references. Learn more about the future developments for this feature in its RFC . Table Lineage \u00b6 TODO: Please add doc Table Profile \u00b6 TODO: Please add doc* Issue Tracking Features \u00b6 In order to enable Issue Tracking set IssueTrackingConfig.enabled to true to see UI features. Further configuration is required to fully enable the feature, please see this entry","title":"Application Config"},{"location":"frontend/docs/application_config/#application-configuration","text":"This document describes how to leverage the frontend service\u2019s application configuration to configure particular features. After modifying the AppConfigCustom object in config-custom.ts in the ways described in this document, be sure to rebuild your application with these changes. NOTE: This document is a work in progress and does not include 100% of features. We welcome PRs to complete this document","title":"Application configuration"},{"location":"frontend/docs/application_config/#announcements-config","text":"Annoncements is a feature that allows to disclose new features, changes or any other news to Amundsen\u2019s users. Announcements in the homepage To enable this feature, change the announcements.enable boolean value by overriding it on config-custom.ts . Once activated, an \u201cAnnouncements\u201d link will be available in the global navigation, and a new list of announcements will show up on the right sidebar on the Homepage.","title":"Announcements Config"},{"location":"frontend/docs/application_config/#badge-config","text":"Badges are a special type of tag that cannot be edited through the UI. BadgeConfig can be used to customize the text and color of badges. This config defines a mapping of badge name to a BadgeStyle and optional displayName . Badges that are not defined will default to use the BadgeStyle.default style and displayName use the badge name with any _ or - characters replaced with a space.","title":"Badge Config"},{"location":"frontend/docs/application_config/#browse-tags-feature","text":"TODO: Please add doc","title":"Browse Tags Feature"},{"location":"frontend/docs/application_config/#custom-logo","text":"Add your logo to the folder in amundsen_application/static/images/ . Set the the logoPath key on the to the location of your image.","title":"Custom Logo"},{"location":"frontend/docs/application_config/#date","text":"This config allows you to specify various date formats across the app. There are three date formats in use shown below. These correspond to the formatDate , formatDateTimeShort and formatDateTimeLong utility functions. 1 2 3 default: 'MMM DD, YYYY' dateTimeShort: 'MMM DD, YYYY ha z' dateTimeLong: 'MMMM Do YYYY [at] h:mm:ss a' Reference for formatting: https://devhints.io/datetime#momentjs-format","title":"Date"},{"location":"frontend/docs/application_config/#analytics","text":"Amundsen supports pluggable user behavior analytics via the analytics library. To emit analytics to a given destination, you must use one of the provided plugins (open a PR if you need to install a different vendor), then specify it the config passing the configuration of your account. Multiple destinations are supported if you wish to emit to multiple backends simultaneously. For example, to use Google analytics, you must add the import at the top of your config-custom.ts file: import googleAnalytics from '@analytics/google-analytics'; , then add this config block: analytics: { plugins: [ googleAnalytics({ trackingId: '<YOUR_UA_CODE>', sampleRate: 100 }), ], } We provide out of the box support for Mixpanel, Segment and Google Analytics. All @analytics/ plugins are potentially supported, but you must first install the plugin: npm install @analytics/<provider> and send us a PR with it before you can use it.","title":"Analytics"},{"location":"frontend/docs/application_config/#indexing-optional-resources","text":"In Amundsen, we currently support indexing other optional resources beyond tables.","title":"Indexing Optional Resources"},{"location":"frontend/docs/application_config/#index-users","text":"Users themselves are data resources and user metadata helps to facilitate network based discovery. When users are indexed they will show up in search results, and selecting a user surfaces a profile page that displays that user\u2019s relationships with different data resources. After ingesting user metadata into the search and metadata services, set IndexUsersConfig.enabled to true on the application configuration to display the UI for the aforementioned features.","title":"Index Users"},{"location":"frontend/docs/application_config/#index-dashboards","text":"Introducing dashboards into Amundsen allows users to discovery data analysis that has been already done. When dashboards are indexed they will show up in search results, and selecting a dashboard surfaces a page where users can explore dashboard metadata. After ingesting dashboard metadata into the search and metadata services, set IndexDashboardsConfig.enabled to true on the application configuration to display the UI for the aforementioned features.","title":"Index Dashboards"},{"location":"frontend/docs/application_config/#mail-client-features","text":"Amundsen has two features that leverage the custom mail client \u2013 the feedback tool and notifications. As these are optional features, our MailClientFeaturesConfig can be used to hide/display any UI related to these features: Set MailClientFeaturesConfig.feedbackEnabled to true in order to display the Feedback component in the UI. Set MailClientFeaturesConfig.notificationsEnabled to true in order to display the optional UI for users to request more information about resources on the TableDetail page. For information about how to configure a custom mail client, please see this entry in our flask configuration doc.","title":"Mail Client Features"},{"location":"frontend/docs/application_config/#navigation-links","text":"TODO: Please add doc","title":"Navigation Links"},{"location":"frontend/docs/application_config/#resource-configurations","text":"This configuration drives resource specific aspects of the application\u2019s user interface. Each supported resource should be mapped to an object that matches or extends the BaseResourceConfig .","title":"Resource Configurations"},{"location":"frontend/docs/application_config/#base-configuration","text":"All resource configurations must match or extend the BaseResourceConfig . This configuration supports the following options: displayName : The name displayed throughout the application to refer to this resource type. filterCategories : An optional FilterConfig object. When set for a given resource, that resource will display filter options in the search page UI. supportedSources : An optional SourcesConfig object.","title":"Base Configuration"},{"location":"frontend/docs/application_config/#filter-categories","text":"The FilterConfig is an array of objects that match any of the supported filter options. We currently support a MultiSelectFilterCategory and a SingleFilterCategory . See our config-types for more information about each option.","title":"Filter Categories"},{"location":"frontend/docs/application_config/#supported-sources","text":"The SourcesConfig can be used for the customizations detailed below. See examples in config-default.ts .","title":"Supported Sources"},{"location":"frontend/docs/application_config/#custom-icons","text":"You can configure custom icons to be used throughout the UI when representing entities from particular sources. On the supportedSources object, add an entry with the id used to reference that source and map to an object that specifies the iconClass for that database. This iconClass should be defined in icons.scss .","title":"Custom Icons"},{"location":"frontend/docs/application_config/#display-names","text":"You can configure a specific display name to be used throughout the UI when representing entities from particular sources. On the supportedSources object, add an entry with the id used to reference that source and map to an object that specified the displayName for that source.","title":"Display Names"},{"location":"frontend/docs/application_config/#table-configuration","text":"To configure Table related features we have created a new resource configuration TableResourceConfig which extends BaseResourceConfig . In addition to the configurations explained above it also supports supportedDescriptionSources .","title":"Table Configuration"},{"location":"frontend/docs/application_config/#supported-description-sources","text":"A table resource may have a source of table and column description attached to it. We can customize it by using supportedDescriptionSources object which is an optional object. This object has displayName and iconPath , which can be used throughout the UI to represent a particular description source. See example in config-default.ts . For configuring new description sources, add an entry in supportedDescriptionSources with the id used to reference that source and add desired display name and icon for it.","title":"Supported Description Sources"},{"location":"frontend/docs/application_config/#table-stats","text":"If you have a stat field that is made of a JSON like set of value names and counts, you can show that as a set of \u201cunique values\u201d. You can see an example of this in the following figure: To achieve this, you will need to modify your custom configuration (config-custom.ts) by adding the name of the stat_type field that holds these values. You can find the config property in the stats section for table resource: [ResourceType.table]: { //... stats: { uniqueValueTypeName: \"keyNameExample\", }, } The unique values set needs to be an object like this: { end_epoch: 1609522182, start_epoch: 1608917382, stat_type: 'keyNameExample', stat_val: \"{'Category': 66, 'AnotherCategory': 54, 'More': 48}\", },","title":"Table Stats"},{"location":"frontend/docs/application_config/#notices","text":"We now can add notices to tables and dashboards. These notices allows Amundsen administrators to show informational, warning and alert messages related to the different resources (tables, dashboards, eventually people) we expose in Amundsen. This feature help administrators show messages related to deprecation, updates (or lack of), and informational messages related to specific resources. A notice is a small box with an icon and a message containing HTML markup (like links and bolded text). These will come in three flavors: Informational: Marked with a blue \"i\" icon on the right side Warning: Marked with an orange exclamation mark icon on the right side Alert: Marked with a red exclamation mark icon on the right side To set them up, we\u2019ll use the current configuration objects for the resources. For example, if company X wants to deprecate the use of one table or dashboard, they can opt to add new notices in their configuration file: resourceConfig: { [ResourceType.table]: { ... //Table Resource Configuration notices: { \"<CLUSTER>.<DATABASE>.<SCHEMA>.<TABLENAME>\": { severity: NoticeSeverity.ALERT, messageHtml: `This table is deprecated, please use <a href=\"<LINKTONEWTABLEDETAILPAGE>\">this new table</a> instead.`, }, }, }, [ResourceType.dashboard]: { ... //Dashboard Resource Configuration notices: { \"<PRODUCT>.<CLUSTER>.<GROUPNAME>.<DASHBOARDNAME>\": { severity: NoticeSeverity.WARNING, messageHtml: `This dashboard is deprecated, please use <a href=\"<LINKTONEWDASHBOARDDETAILPAGE>\">this new dashboard</a> instead.`, }, }, }, }, The above code will show a notice with a red exclamation icon whenever a final user visits the table\u2019s Table Detail page or the Dashboard Detail page. This feature\u2019s ultimate goal is to allow Amundsen administrators to point their users to more trusted/higher quality resources without removing the old references. Learn more about the future developments for this feature in its RFC .","title":"Notices"},{"location":"frontend/docs/application_config/#table-lineage","text":"TODO: Please add doc","title":"Table Lineage"},{"location":"frontend/docs/application_config/#table-profile","text":"TODO: Please add doc*","title":"Table Profile"},{"location":"frontend/docs/application_config/#issue-tracking-features","text":"In order to enable Issue Tracking set IssueTrackingConfig.enabled to true to see UI features. Further configuration is required to fully enable the feature, please see this entry","title":"Issue Tracking Features"},{"location":"frontend/docs/configuration/","text":"Configuration \u00b6 Flask \u00b6 The default Flask application uses a LocalConfig that looks for the metadata and search services running on localhost. In order to use different end point, you need to create a custom config class suitable for your use case. Once the config class has been created, it can be referenced via the environment variable : FRONTEND_SVC_CONFIG_MODULE_CLASS For more examples of how to leverage the Flask configuration for specific features, please see this extended doc . For more information on Flask configurations, please reference the official Flask documentation . React Application \u00b6 Application Config \u00b6 Certain features of the React application import variables from an AppConfig object. The configuration can be customized by modifying config-custom.ts . For examples of how to leverage the application configuration for specific features, please see this extended doc . Custom Fonts & Styles \u00b6 Fonts and css variables can be customized by modifying fonts-custom.scss and variables-custom.scss . Python Entry Points \u00b6 The application also leverages python entry points for custom features. In your local setup.py , point the entry points detailed below to custom classes or methods that have to be implemented for a given feature. Run python3 setup.py install in your virtual environment and restart the application for the entry point changes to take effect. entry_points=\"\"\" [action_log.post_exec.plugin] analytic_clients_action_log = path.to.file:custom_action_log_method [preview_client] table_preview_client_class = amundsen_application.base.examples.example_superset_preview_client:SupersetPreviewClient [announcement_client] announcement_client_class = amundsen_application.base.examples.example_announcement_client:SQLAlchemyAnnouncementClient \"\"\" Action Logging \u00b6 Create a custom method to handle action logging. Under the [ action_log.post_exec.plugin] group, point the analytic_clients_action_log entry point in your local setup.py to that method. Preview Client \u00b6 Create a custom implementation of base_preview_client . Under the [preview_client] group, point the table_preview_client_class entry point in your local setup.py to that class. For those who use Apache Superset for data exploration, see this doc for how to implement a preview client for Superset. Announcement Client \u00b6 Create a custom implementation of base_announcement_client . Under the [announcement_client] group, point the announcement_client_class entry point in your local setup.py to that class. Currently Amundsen does not own the input and storage of announcements. Consider having the client fetch announcement information from an external web feed. Authentication \u00b6 Authentication can be hooked within Amundsen using either wrapper class or using proxy to secure the microservices on the nginx/server level. Following are the ways to setup the end-to-end authentication. - OIDC / Keycloak","title":"React Configuration"},{"location":"frontend/docs/configuration/#configuration","text":"","title":"Configuration"},{"location":"frontend/docs/configuration/#flask","text":"The default Flask application uses a LocalConfig that looks for the metadata and search services running on localhost. In order to use different end point, you need to create a custom config class suitable for your use case. Once the config class has been created, it can be referenced via the environment variable : FRONTEND_SVC_CONFIG_MODULE_CLASS For more examples of how to leverage the Flask configuration for specific features, please see this extended doc . For more information on Flask configurations, please reference the official Flask documentation .","title":"Flask"},{"location":"frontend/docs/configuration/#react-application","text":"","title":"React Application"},{"location":"frontend/docs/configuration/#application-config","text":"Certain features of the React application import variables from an AppConfig object. The configuration can be customized by modifying config-custom.ts . For examples of how to leverage the application configuration for specific features, please see this extended doc .","title":"Application Config"},{"location":"frontend/docs/configuration/#custom-fonts-styles","text":"Fonts and css variables can be customized by modifying fonts-custom.scss and variables-custom.scss .","title":"Custom Fonts &amp; Styles"},{"location":"frontend/docs/configuration/#python-entry-points","text":"The application also leverages python entry points for custom features. In your local setup.py , point the entry points detailed below to custom classes or methods that have to be implemented for a given feature. Run python3 setup.py install in your virtual environment and restart the application for the entry point changes to take effect. entry_points=\"\"\" [action_log.post_exec.plugin] analytic_clients_action_log = path.to.file:custom_action_log_method [preview_client] table_preview_client_class = amundsen_application.base.examples.example_superset_preview_client:SupersetPreviewClient [announcement_client] announcement_client_class = amundsen_application.base.examples.example_announcement_client:SQLAlchemyAnnouncementClient \"\"\"","title":"Python Entry Points"},{"location":"frontend/docs/configuration/#action-logging","text":"Create a custom method to handle action logging. Under the [ action_log.post_exec.plugin] group, point the analytic_clients_action_log entry point in your local setup.py to that method.","title":"Action Logging"},{"location":"frontend/docs/configuration/#preview-client","text":"Create a custom implementation of base_preview_client . Under the [preview_client] group, point the table_preview_client_class entry point in your local setup.py to that class. For those who use Apache Superset for data exploration, see this doc for how to implement a preview client for Superset.","title":"Preview Client"},{"location":"frontend/docs/configuration/#announcement-client","text":"Create a custom implementation of base_announcement_client . Under the [announcement_client] group, point the announcement_client_class entry point in your local setup.py to that class. Currently Amundsen does not own the input and storage of announcements. Consider having the client fetch announcement information from an external web feed.","title":"Announcement Client"},{"location":"frontend/docs/configuration/#authentication","text":"Authentication can be hooked within Amundsen using either wrapper class or using proxy to secure the microservices on the nginx/server level. Following are the ways to setup the end-to-end authentication. - OIDC / Keycloak","title":"Authentication"},{"location":"frontend/docs/developer_guide/","text":"Developer Guide \u00b6 Environment \u00b6 Follow the installation instructions in the section Install standalone application directly from the source . Install the javascript development requirements: # in ~/<your-path-to-cloned-repo>/amundsenfrontendlibrary/amundsen_application $ cd static $ npm install --only = dev To test local changes to the javascript static files: # in ~/<your-path-to-cloned-repo>/amundsenfrontendlibrary/amundsen_application $ cd static $ npm run dev-build # builds the development bundle To test local changes to the python files, re-run the wsgi: # in ~/<your-path-to-cloned-repo>/amundsenfrontendlibrary/amundsen_application $ python3 wsgi.py Contributing \u00b6 We describe our general contributing process in the main repository of Amundsen, so here we\u2019ll cover the items specific to the Frontend library. Testing Python Code \u00b6 If changes were made to any python files, run the python unit tests, linter, and type checker. Unit tests are run with py.test . They are located in tests/unit . Type checks are run with mypy . Linting is flake8 . There are friendly make targets for each of these tests: # after setting up the environment make test # unit tests in Python 3 make lint # flake8 make mypy # type checks Fix all errors before submitting a PR. Testing Frontend Code \u00b6 npm run test runs our Frontend unit tests. Please add unit tests to cover new code additions and fix any test failures before submitting a PR. You can also have a dedicated terminal running npm run test:watch while developing, which would continuously run tests over your modified files. To run specific tests, run npm run test-nocov -t <regex> , where <regex> is any pattern that matches the names of the test blocks that you want to run. See our recommendations for writing unit tests . Developing React Components \u00b6 To preview React components in isolation, use Storybook . Just add a <componentName>.story.tsx file in the same folder as your component. In that file, show your component in different states. Then run npm run storybook , which will open your browser to the storybook browse page. Using Storybook makes it much easier to quickly iterate on components when getting to certain states requires multiple steps of UI manipulation. The gallery also serves as a convenient place to see what reusable components are available so you can avoid reinventing the wheel. Frontend Type Checking \u00b6 We use TypeScript in our codebase, so npm run tsc conducts type checking. The build commands npm run build and npm run dev-build also conduct type checking, but are slower because they also build the source code. Run any of these commands and fix all failed checks before submitting a PR. Frontend Linting and Formatting \u00b6 We have in place two linters \u2013 ESLint for our JavaScript and TypeScript files, Stylelint for our Sass files. If you have both ESLint and Stylelint extensions installed on your IDE, you should get warnings on your editor by default. We also use Prettier to help us keep consistent formatting on our TypeScript and Sass code. Whenever you want to run these tasks manually, you can execute: npm run lint to run ESLint and npm run lint:fix to auto-fix most of them. npm run stylelint to run Stylelint and npm run stylelint:fix to trigger the auto-fix. npm run format to run Prettier on both the TypeScript and Sass files We also check your changed files and format them when you create a new commit, making it easy for you and for the project to keep a consistent code style. We do this leveraging Husky and Lint-staged . Looking forward, we aim at setting more strict best practices using ESLint and Stylelint. For that, we are leveraging a project called betterer , which keeps track of our errors when a given test is passed. You can run it using npm run betterer and it will break if you introduce any new eslint errors. If you want to ignore the new errors you can run npm run betterer:update to update the betterer.results file. We do not recommend adding or introducing new eslint errors. You can read about our plans to improve our TypeScript, Styles and general code style on these issues: Adopt Typescript Recommended Guidelines on the Frontend library Adopt Stylelint\u2019s Sass Guidelines on the Frontend library Adopt Airbnb-Typescript Code Guidelines on the Frontend library Accessibility and Semantic Markup \u00b6 We strive to keep our application accessible. For that, we use the \u2018airbnb-typescript\u2019 preset for ESLint, which includes a bunch of accessibility rules. We also have a set of \u201cjsx-a11y/\u201d prefixed rules, which are currently on a \u201cwarn\u201d level, so they don\u2019t throw errors. Our goal is to remove that \u201cwarn\u201d level and comply with all the accessibility rules we list on our ESLint configuration . We also try to model our application\u2019s markup on best practices regarding semantic markup. If you are making large markup changes on one of your PRs, make sure your changes comply with this HTML semantics checklist . Typography \u00b6 In the past, we have used several classes to set the styling of our heading and body text. Nowadays, we recommend to use classes in our stylesheets for each component, and extend those classes with the proper text styling by using an @extend to a placehoder selector: @import \"variables\" ; @import \"typography\" ; .header-title-text { @extend % text-headline-w2 ; } .header-subtitle-text { @extend % text-subtitle-w3 ; } You can find the complete list of placeholder selectors for text in this file . In the cases were a text class works best, you can use their equivalent classes.","title":"FE Developer Guide"},{"location":"frontend/docs/developer_guide/#developer-guide","text":"","title":"Developer Guide"},{"location":"frontend/docs/developer_guide/#environment","text":"Follow the installation instructions in the section Install standalone application directly from the source . Install the javascript development requirements: # in ~/<your-path-to-cloned-repo>/amundsenfrontendlibrary/amundsen_application $ cd static $ npm install --only = dev To test local changes to the javascript static files: # in ~/<your-path-to-cloned-repo>/amundsenfrontendlibrary/amundsen_application $ cd static $ npm run dev-build # builds the development bundle To test local changes to the python files, re-run the wsgi: # in ~/<your-path-to-cloned-repo>/amundsenfrontendlibrary/amundsen_application $ python3 wsgi.py","title":"Environment"},{"location":"frontend/docs/developer_guide/#contributing","text":"We describe our general contributing process in the main repository of Amundsen, so here we\u2019ll cover the items specific to the Frontend library.","title":"Contributing"},{"location":"frontend/docs/developer_guide/#testing-python-code","text":"If changes were made to any python files, run the python unit tests, linter, and type checker. Unit tests are run with py.test . They are located in tests/unit . Type checks are run with mypy . Linting is flake8 . There are friendly make targets for each of these tests: # after setting up the environment make test # unit tests in Python 3 make lint # flake8 make mypy # type checks Fix all errors before submitting a PR.","title":"Testing Python Code"},{"location":"frontend/docs/developer_guide/#testing-frontend-code","text":"npm run test runs our Frontend unit tests. Please add unit tests to cover new code additions and fix any test failures before submitting a PR. You can also have a dedicated terminal running npm run test:watch while developing, which would continuously run tests over your modified files. To run specific tests, run npm run test-nocov -t <regex> , where <regex> is any pattern that matches the names of the test blocks that you want to run. See our recommendations for writing unit tests .","title":"Testing Frontend Code"},{"location":"frontend/docs/developer_guide/#developing-react-components","text":"To preview React components in isolation, use Storybook . Just add a <componentName>.story.tsx file in the same folder as your component. In that file, show your component in different states. Then run npm run storybook , which will open your browser to the storybook browse page. Using Storybook makes it much easier to quickly iterate on components when getting to certain states requires multiple steps of UI manipulation. The gallery also serves as a convenient place to see what reusable components are available so you can avoid reinventing the wheel.","title":"Developing React Components"},{"location":"frontend/docs/developer_guide/#frontend-type-checking","text":"We use TypeScript in our codebase, so npm run tsc conducts type checking. The build commands npm run build and npm run dev-build also conduct type checking, but are slower because they also build the source code. Run any of these commands and fix all failed checks before submitting a PR.","title":"Frontend Type Checking"},{"location":"frontend/docs/developer_guide/#frontend-linting-and-formatting","text":"We have in place two linters \u2013 ESLint for our JavaScript and TypeScript files, Stylelint for our Sass files. If you have both ESLint and Stylelint extensions installed on your IDE, you should get warnings on your editor by default. We also use Prettier to help us keep consistent formatting on our TypeScript and Sass code. Whenever you want to run these tasks manually, you can execute: npm run lint to run ESLint and npm run lint:fix to auto-fix most of them. npm run stylelint to run Stylelint and npm run stylelint:fix to trigger the auto-fix. npm run format to run Prettier on both the TypeScript and Sass files We also check your changed files and format them when you create a new commit, making it easy for you and for the project to keep a consistent code style. We do this leveraging Husky and Lint-staged . Looking forward, we aim at setting more strict best practices using ESLint and Stylelint. For that, we are leveraging a project called betterer , which keeps track of our errors when a given test is passed. You can run it using npm run betterer and it will break if you introduce any new eslint errors. If you want to ignore the new errors you can run npm run betterer:update to update the betterer.results file. We do not recommend adding or introducing new eslint errors. You can read about our plans to improve our TypeScript, Styles and general code style on these issues: Adopt Typescript Recommended Guidelines on the Frontend library Adopt Stylelint\u2019s Sass Guidelines on the Frontend library Adopt Airbnb-Typescript Code Guidelines on the Frontend library","title":"Frontend Linting and Formatting"},{"location":"frontend/docs/developer_guide/#accessibility-and-semantic-markup","text":"We strive to keep our application accessible. For that, we use the \u2018airbnb-typescript\u2019 preset for ESLint, which includes a bunch of accessibility rules. We also have a set of \u201cjsx-a11y/\u201d prefixed rules, which are currently on a \u201cwarn\u201d level, so they don\u2019t throw errors. Our goal is to remove that \u201cwarn\u201d level and comply with all the accessibility rules we list on our ESLint configuration . We also try to model our application\u2019s markup on best practices regarding semantic markup. If you are making large markup changes on one of your PRs, make sure your changes comply with this HTML semantics checklist .","title":"Accessibility and Semantic Markup"},{"location":"frontend/docs/developer_guide/#typography","text":"In the past, we have used several classes to set the styling of our heading and body text. Nowadays, we recommend to use classes in our stylesheets for each component, and extend those classes with the proper text styling by using an @extend to a placehoder selector: @import \"variables\" ; @import \"typography\" ; .header-title-text { @extend % text-headline-w2 ; } .header-subtitle-text { @extend % text-subtitle-w3 ; } You can find the complete list of placeholder selectors for text in this file . In the cases were a text class works best, you can use their equivalent classes.","title":"Typography"},{"location":"frontend/docs/flask_config/","text":"Flask configuration \u00b6 After modifying any variable in config.py described in this document, be sure to rebuild your application with these changes. NOTE: This document is a work in progress and does not include 100% of features. We welcome PRs to complete this document Custom Routes \u00b6 In order to add any custom Flask endpoints to Amundsen\u2019s frontend application, configure a function on the INIT_CUSTOM_ROUTES variable. This function takes the created Flask application and can leverage Flask\u2019s add_url_rule method to add custom routes. Example: Setting INIT_CUSTOM_ROUTES to the init_custom_routes method below will expose a /custom_route endpoint on the frontend application. def init_custom_routes ( app: Flask ) -> None: app.add_url_rule ( '/custom_route' , 'custom_route' , custom_route ) def custom_route () : pass Dashboard Preview \u00b6 This service provides an API to download preview image of dashboard resources, which currently only supports Mode . The dashboard preview image is cached in user\u2019s browser up to a day. In order to adjust this you can change the value of DASHBOARD_PREVIEW_IMAGE_CACHE_MAX_AGE_SECONDS How to configure Mode dashboard preview \u00b6 Add the following environment variables: CREDENTIALS_MODE_ADMIN_TOKEN CREDENTIALS_MODE_ADMIN_PASSWORD MODE_ORGANIZATION How to enable authorization on Mode dashboard \u00b6 By default, Amundsen does not do any authorization on showing preview. By registering name of the Mode preview class in configuration, you can enable authorization. ACL_ENABLED_DASHBOARD_PREVIEW = { 'ModePreview' } Amundsen ingests Mode dashboards only from the shared space, which all registered Mode users are able to view. Therefore our authorization first validates if the current user is registered in Mode. This feature is dependent on Amundsen also ingesting Mode user information via the ModeDashboardUserExtractor and the metadata service version must be at least v2.5.2 . How to support preview of different product? \u00b6 You can add preview support for different products by adding its preview class to DefaultPreviewMethodFactory In order to develop new preview class, you need to implement the class that inherits BasePreview class and ModePreview would be a great example. Mail Client Features \u00b6 Amundsen has two features that leverage the custom mail client \u2013 the feedback tool and notifications. For these features a custom implementation of base_mail_client must be mapped to the MAIL_CLIENT configuration variable. To fully enable these features in the UI, the application configuration variables for these features must also be set to true. Please see this entry in our application configuration doc for further information. Issue Tracking Integration Features \u00b6 Amundsen has a feature to allow display of associated tickets within the table detail view. The feature both displays open tickets and allows users to report new tickets associated with the table. These tickets must contain the table_uri within the ticket text in order to be displayed; the table_uri is automatically added to tickets created via the feature. Tickets are displayed from most recent to oldest, and currently only open tickets are displayed. Currently only JIRA is supported. The UI must also be enabled to use this feature, please see configuration notes here . There are several configuration settings in config.py that should be set in order to use this feature. Here are the settings and what they should be set to ISSUE_LABELS = [] # type: List[str] (Optional labels to be set on the created tickets) ISSUE_TRACKER_URL = None # type: str (Your JIRA environment, IE 'https://jira.net') ISSUE_TRACKER_USER = None # type: str (Recommended to be a service account) ISSUE_TRACKER_PASSWORD = None # type: str ISSUE_TRACKER_PROJECT_ID = None # type: int (Project ID for the project you would like JIRA tickets to be created in) ISSUE_TRACKER_CLIENT = None # type: str (Fully qualified class name and path) ISSUE_TRACKER_CLIENT_ENABLED = False # type: bool (Enabling the feature, must be set to True) ISSUE_TRACKER_MAX_RESULTS = None # type: int (Max issues to display at a time) ISSUE_TRACKER_ISSUE_TYPE_ID = None # type: int (Jira only: Override default issue tracker ID whenever needed for cloud/hosted deployments) Programmatic Descriptions \u00b6 Amundsen supports configuring other mark down supported non-editable description boxes on the table page. This can be useful if you have multiple writers which want to write different pieces of information to Amundsen that are either very company specific and thus would never be directly integrated into Amundsen or require long form text to properly convey the information. What are some more specific examples of what could be used for this? - You have an existing process that generates quality reports for a dataset that you want to embed in the table page. - You have a process that detects pii information (also adding the appropriate tag/badge) but also generates a simple report to provide context. - You have extended table information that is applicable to your datastore which you want to scrape and provide in the table page Programmatic descriptions are referred to by a \u201cdescription source\u201d which is a unique identifier. In the UI, they will appear on the table page under structured metadata. In config.py you can then configure the descriptions to have a custom order, as well as whether or not they should exist in the left column or right column. PROGRAMMATIC_DISPLAY = { 'RIGHT': { \"test3\" : {}, \"test2\" : { \"display_order\": 0 } }, 'LEFT': { \"test1\" : { \"display_order\": 1 }, \"test0\" : { \"display_order\": 0 }, }, 'test4': {\"display_order\": 0}, } Description sources not mentioned in the configuration will be alphabetically placed at the bottom of the list. If PROGRAMMATIC_DISPLAY is left at None all added fields will show up in the order in which they were returned from the backend. Here is a screenshot of what it would look like in the bottom left: Uneditable Table Descriptions \u00b6 Amundsen supports configuring table and column description to be non-editable for selective tables. You may want to make table descriptions non-editable due to various reasons such as table already has table description from source of truth. You can define matching rules in config.py for selecting tables. This configuration is useful as table selection criteria can be company specific which will not directly integrated with Amundsen. You can use different combinations of schema and table name for selecting tables. Here are some examples when this feature can be used: 1. You want to set all tables with a given schema or schema pattern as un-editable. 2. You want to set all tables with a specific table name pattern in a given schema pattern as un-editable. 3. You want to set all tables with a given table name pattern as un-editable. Amundsen has two variables in config.py file which can be used to define match rules: 1. UNEDITABLE_SCHEMAS : Set of schemas where all tables should be un-editable. It takes exact schema name. 2. UNEDITABLE_TABLE_DESCRIPTION_MATCH_RULES : List of MatchRuleObject, where each MatchRuleObject consists of regex for schema name or regex for table name or both. Purpose of UNEDITABLE_SCHEMAS can be fulfilled by UNEDITABLE_TABLE_DESCRIPTION_MATCH_RULES but we are keeping both variables for backward compatibility. If you want to restrict tables from a given schemas then you can use UNEDITABLE_SCHEMAS as follows: UNEDITABLE_SCHEMAS = set ([ 'schema1' , 'schema2' ]) After above configuration, all tables in \u2018schema1\u2019 and \u2018schema2\u2019 will have non-editable table and column descriptions. If you have more complex matching rules you can use UNEDITABLE_TABLE_DESCRIPTION_MATCH_RULES . It provides you more flexibility and control as you can create multiple match rules and use regex for matching schema and table names. You can configure your match rules in config.py as follow: UNEDITABLE_TABLE_DESCRIPTION_MATCH_RULES = [ # match rule for all table in schema1 MatchRuleObject ( schema_regex = r \"^(schema1)\" ), # macth rule for all tables in schema2 and schema3 MatchRuleObject ( schema_regex = r \"^(schema2|schema3)\" ), # match rule for tables in schema4 with table name pattern 'noedit_*' MatchRuleObject ( schema_regex = r \"^(schema4)\" , table_name_regex = r \"^noedit_([a-zA-Z_0-9]+)\" ), # match rule for tables in schema5, schema6 and schema7 with table name pattern 'noedit_*' MatchRuleObject ( schema_regex = r \"^(schema5|schema6|schema7)\" , table_name_regex = r \"^noedit_([a-zA-Z_0-9]+)\" ), # match rule for all tables with table name pattern 'others_*' MatchRuleObject ( table_name_regex = r \"^others_([a-zA-Z_0-9]+)\" ) ] After configuring this, users will not be able to edit table and column descriptions of any table matching above match rules from UI.","title":"Flask Configuration"},{"location":"frontend/docs/flask_config/#flask-configuration","text":"After modifying any variable in config.py described in this document, be sure to rebuild your application with these changes. NOTE: This document is a work in progress and does not include 100% of features. We welcome PRs to complete this document","title":"Flask configuration"},{"location":"frontend/docs/flask_config/#custom-routes","text":"In order to add any custom Flask endpoints to Amundsen\u2019s frontend application, configure a function on the INIT_CUSTOM_ROUTES variable. This function takes the created Flask application and can leverage Flask\u2019s add_url_rule method to add custom routes. Example: Setting INIT_CUSTOM_ROUTES to the init_custom_routes method below will expose a /custom_route endpoint on the frontend application. def init_custom_routes ( app: Flask ) -> None: app.add_url_rule ( '/custom_route' , 'custom_route' , custom_route ) def custom_route () : pass","title":"Custom Routes"},{"location":"frontend/docs/flask_config/#dashboard-preview","text":"This service provides an API to download preview image of dashboard resources, which currently only supports Mode . The dashboard preview image is cached in user\u2019s browser up to a day. In order to adjust this you can change the value of DASHBOARD_PREVIEW_IMAGE_CACHE_MAX_AGE_SECONDS","title":"Dashboard Preview"},{"location":"frontend/docs/flask_config/#how-to-configure-mode-dashboard-preview","text":"Add the following environment variables: CREDENTIALS_MODE_ADMIN_TOKEN CREDENTIALS_MODE_ADMIN_PASSWORD MODE_ORGANIZATION","title":"How to configure Mode dashboard preview"},{"location":"frontend/docs/flask_config/#how-to-enable-authorization-on-mode-dashboard","text":"By default, Amundsen does not do any authorization on showing preview. By registering name of the Mode preview class in configuration, you can enable authorization. ACL_ENABLED_DASHBOARD_PREVIEW = { 'ModePreview' } Amundsen ingests Mode dashboards only from the shared space, which all registered Mode users are able to view. Therefore our authorization first validates if the current user is registered in Mode. This feature is dependent on Amundsen also ingesting Mode user information via the ModeDashboardUserExtractor and the metadata service version must be at least v2.5.2 .","title":"How to enable authorization on Mode dashboard"},{"location":"frontend/docs/flask_config/#how-to-support-preview-of-different-product","text":"You can add preview support for different products by adding its preview class to DefaultPreviewMethodFactory In order to develop new preview class, you need to implement the class that inherits BasePreview class and ModePreview would be a great example.","title":"How to support preview of different product?"},{"location":"frontend/docs/flask_config/#mail-client-features","text":"Amundsen has two features that leverage the custom mail client \u2013 the feedback tool and notifications. For these features a custom implementation of base_mail_client must be mapped to the MAIL_CLIENT configuration variable. To fully enable these features in the UI, the application configuration variables for these features must also be set to true. Please see this entry in our application configuration doc for further information.","title":"Mail Client Features"},{"location":"frontend/docs/flask_config/#issue-tracking-integration-features","text":"Amundsen has a feature to allow display of associated tickets within the table detail view. The feature both displays open tickets and allows users to report new tickets associated with the table. These tickets must contain the table_uri within the ticket text in order to be displayed; the table_uri is automatically added to tickets created via the feature. Tickets are displayed from most recent to oldest, and currently only open tickets are displayed. Currently only JIRA is supported. The UI must also be enabled to use this feature, please see configuration notes here . There are several configuration settings in config.py that should be set in order to use this feature. Here are the settings and what they should be set to ISSUE_LABELS = [] # type: List[str] (Optional labels to be set on the created tickets) ISSUE_TRACKER_URL = None # type: str (Your JIRA environment, IE 'https://jira.net') ISSUE_TRACKER_USER = None # type: str (Recommended to be a service account) ISSUE_TRACKER_PASSWORD = None # type: str ISSUE_TRACKER_PROJECT_ID = None # type: int (Project ID for the project you would like JIRA tickets to be created in) ISSUE_TRACKER_CLIENT = None # type: str (Fully qualified class name and path) ISSUE_TRACKER_CLIENT_ENABLED = False # type: bool (Enabling the feature, must be set to True) ISSUE_TRACKER_MAX_RESULTS = None # type: int (Max issues to display at a time) ISSUE_TRACKER_ISSUE_TYPE_ID = None # type: int (Jira only: Override default issue tracker ID whenever needed for cloud/hosted deployments)","title":"Issue Tracking Integration Features"},{"location":"frontend/docs/flask_config/#programmatic-descriptions","text":"Amundsen supports configuring other mark down supported non-editable description boxes on the table page. This can be useful if you have multiple writers which want to write different pieces of information to Amundsen that are either very company specific and thus would never be directly integrated into Amundsen or require long form text to properly convey the information. What are some more specific examples of what could be used for this? - You have an existing process that generates quality reports for a dataset that you want to embed in the table page. - You have a process that detects pii information (also adding the appropriate tag/badge) but also generates a simple report to provide context. - You have extended table information that is applicable to your datastore which you want to scrape and provide in the table page Programmatic descriptions are referred to by a \u201cdescription source\u201d which is a unique identifier. In the UI, they will appear on the table page under structured metadata. In config.py you can then configure the descriptions to have a custom order, as well as whether or not they should exist in the left column or right column. PROGRAMMATIC_DISPLAY = { 'RIGHT': { \"test3\" : {}, \"test2\" : { \"display_order\": 0 } }, 'LEFT': { \"test1\" : { \"display_order\": 1 }, \"test0\" : { \"display_order\": 0 }, }, 'test4': {\"display_order\": 0}, } Description sources not mentioned in the configuration will be alphabetically placed at the bottom of the list. If PROGRAMMATIC_DISPLAY is left at None all added fields will show up in the order in which they were returned from the backend. Here is a screenshot of what it would look like in the bottom left:","title":"Programmatic Descriptions"},{"location":"frontend/docs/flask_config/#uneditable-table-descriptions","text":"Amundsen supports configuring table and column description to be non-editable for selective tables. You may want to make table descriptions non-editable due to various reasons such as table already has table description from source of truth. You can define matching rules in config.py for selecting tables. This configuration is useful as table selection criteria can be company specific which will not directly integrated with Amundsen. You can use different combinations of schema and table name for selecting tables. Here are some examples when this feature can be used: 1. You want to set all tables with a given schema or schema pattern as un-editable. 2. You want to set all tables with a specific table name pattern in a given schema pattern as un-editable. 3. You want to set all tables with a given table name pattern as un-editable. Amundsen has two variables in config.py file which can be used to define match rules: 1. UNEDITABLE_SCHEMAS : Set of schemas where all tables should be un-editable. It takes exact schema name. 2. UNEDITABLE_TABLE_DESCRIPTION_MATCH_RULES : List of MatchRuleObject, where each MatchRuleObject consists of regex for schema name or regex for table name or both. Purpose of UNEDITABLE_SCHEMAS can be fulfilled by UNEDITABLE_TABLE_DESCRIPTION_MATCH_RULES but we are keeping both variables for backward compatibility. If you want to restrict tables from a given schemas then you can use UNEDITABLE_SCHEMAS as follows: UNEDITABLE_SCHEMAS = set ([ 'schema1' , 'schema2' ]) After above configuration, all tables in \u2018schema1\u2019 and \u2018schema2\u2019 will have non-editable table and column descriptions. If you have more complex matching rules you can use UNEDITABLE_TABLE_DESCRIPTION_MATCH_RULES . It provides you more flexibility and control as you can create multiple match rules and use regex for matching schema and table names. You can configure your match rules in config.py as follow: UNEDITABLE_TABLE_DESCRIPTION_MATCH_RULES = [ # match rule for all table in schema1 MatchRuleObject ( schema_regex = r \"^(schema1)\" ), # macth rule for all tables in schema2 and schema3 MatchRuleObject ( schema_regex = r \"^(schema2|schema3)\" ), # match rule for tables in schema4 with table name pattern 'noedit_*' MatchRuleObject ( schema_regex = r \"^(schema4)\" , table_name_regex = r \"^noedit_([a-zA-Z_0-9]+)\" ), # match rule for tables in schema5, schema6 and schema7 with table name pattern 'noedit_*' MatchRuleObject ( schema_regex = r \"^(schema5|schema6|schema7)\" , table_name_regex = r \"^noedit_([a-zA-Z_0-9]+)\" ), # match rule for all tables with table name pattern 'others_*' MatchRuleObject ( table_name_regex = r \"^others_([a-zA-Z_0-9]+)\" ) ] After configuring this, users will not be able to edit table and column descriptions of any table matching above match rules from UI.","title":"Uneditable Table Descriptions"},{"location":"frontend/docs/installation/","text":"Installation \u00b6 Install standalone application directly from the source \u00b6 The following instructions are for setting up a standalone version of the Amundsen application. This approach is ideal for local development. # Clone repo $ git clone https://github.com/lyft/amundsenfrontendlibrary.git # Build static content $ cd amundsenfrontendlibrary/amundsen_application/static $ npm install $ npm run build # or npm run dev-build for un-minified source $ cd ../../ # Install python resources $ python3 -m venv venv $ source venv/bin/activate $ pip3 install -e \".[all]\" . # Start server $ python3 amundsen_application/wsgi.py # visit http://localhost:5000 to confirm the application is running You should now have the application running at http://localhost:5000 , but will notice that there is no data and interactions will throw errors. The next step is to connect the standalone application to make calls to the search and metadata services. 1. Setup a local copy of the metadata service using the instructions found here . 2. Setup a local copy of the search service using the instructions found here . 3. Modify the LOCAL_HOST , METADATA_PORT , and SEARCH_PORT variables in the LocalConfig to point to where your local metadata and search services are running, and restart the application with $ python3 amundsen_application/wsgi.py","title":"FE Installation Guide"},{"location":"frontend/docs/installation/#installation","text":"","title":"Installation"},{"location":"frontend/docs/installation/#install-standalone-application-directly-from-the-source","text":"The following instructions are for setting up a standalone version of the Amundsen application. This approach is ideal for local development. # Clone repo $ git clone https://github.com/lyft/amundsenfrontendlibrary.git # Build static content $ cd amundsenfrontendlibrary/amundsen_application/static $ npm install $ npm run build # or npm run dev-build for un-minified source $ cd ../../ # Install python resources $ python3 -m venv venv $ source venv/bin/activate $ pip3 install -e \".[all]\" . # Start server $ python3 amundsen_application/wsgi.py # visit http://localhost:5000 to confirm the application is running You should now have the application running at http://localhost:5000 , but will notice that there is no data and interactions will throw errors. The next step is to connect the standalone application to make calls to the search and metadata services. 1. Setup a local copy of the metadata service using the instructions found here . 2. Setup a local copy of the search service using the instructions found here . 3. Modify the LOCAL_HOST , METADATA_PORT , and SEARCH_PORT variables in the LocalConfig to point to where your local metadata and search services are running, and restart the application with $ python3 amundsen_application/wsgi.py","title":"Install standalone application directly from the source"},{"location":"frontend/docs/react_naming_conventions/","text":"Amundsen\u2019s React Naming Conventions \u00b6 A guide for naming React components on Amundsen React is not opinionated about the naming of our components. Having freedom is great, but this sometimes comes at the cost of lack of consistency on the API. To make sure Amundsen\u2019s React application is consistent and intuitive to use, we created this document that will describe the naming conventions for the React components. Component Names \u00b6 View Components \u00b6 We will follow this convention for naming our regular \u201cview\u201d components: [Context]ComponentName[Type] Where: * Context - the parent component or high-level page * ComponentName - what this component does. The responsibility of the component * Type - the type of component. Usually they are views, but they could be a Form, a List, a Figure, an Illustration, a Container Examples: * SideBar (root component) * FooterSideBar (with context) * SideBarForm (with component type) * FooterSideBarForm (with all) Custom Hook Components \u00b6 We will name custom hook components with a name starting with \u201cuse\u201d, as mentioned on the official docs . High-order Components \u00b6 We will name high order components (HOCs) using the \u201cwith\u201d prefix. For example: withAuthentication withSubscription Provider/Consumer Components \u00b6 Whenever we need to use the Provider/Consumer pattern and name the components, we will use the \u201cProvider\u201d and \u201cConsumer\u201d suffixes. For example: LoginProvider/LoginConsumer DataProvider/DataConsumer Prop Names \u00b6 Handler Functions \u00b6 We will use the \u201con\u201d prefix for handler functions, optionally adding a \u201csubject\u201d word before the event. The schema would be \u201con[Subject]Event\u201d, for example: onClick onItemClick onLogoHover onFormSubmit Internally, we will use the \u201chandle\u201d prefix for naming the handling functions, with an optional \u201csubject\u201d word in the middle. The schema would be then \u201chandle[Subject]Event\u201d, for example: handleClick handleButtonClick handleHeadingHover Props by Type \u00b6 Props should be named to describe the component itself, and what it does, but not why it does it. Objects - Use a singular noun item Arrays - Use a plural noun items users Numbers - Use a name with \u201cnum\u201d prefix or with \u201ccount\u201d or \u201cindex\u201d suffix. numItems userCount itemIndex Booleans - Use a \u201cis\u201d, \u201chas\u201d, or \u201ccan\u201d prefix \u201cis\u201d for visual variations isVisible isEnabled isActive \u201cis\u201d also for behavioral or conditional variations isToggleable isExpandable \u201chas\u201d for toggling UI elements hasCancelSection hasHeader React Nodes - use \u201celement\u201d suffix buttonElement itemElement Reference \u00b6 How to name props for React components \u2013 David\u2019s Blog Handy Naming Conventions for Event Handler Functions & Props in React | by Juan Bernal | JavaScript In Plain English | Medium https://medium.com/@wittydeveloper/react-components-naming-convention-%EF%B8%8F-b50303551505 https://reactjs.org/docs/hooks-custom.html#extracting-a-custom-hook https://reactjs.org/docs/higher-order-components.html","title":"Amundsen's React Naming Conventions"},{"location":"frontend/docs/react_naming_conventions/#amundsens-react-naming-conventions","text":"A guide for naming React components on Amundsen React is not opinionated about the naming of our components. Having freedom is great, but this sometimes comes at the cost of lack of consistency on the API. To make sure Amundsen\u2019s React application is consistent and intuitive to use, we created this document that will describe the naming conventions for the React components.","title":"Amundsen's React Naming Conventions"},{"location":"frontend/docs/react_naming_conventions/#component-names","text":"","title":"Component Names"},{"location":"frontend/docs/react_naming_conventions/#view-components","text":"We will follow this convention for naming our regular \u201cview\u201d components: [Context]ComponentName[Type] Where: * Context - the parent component or high-level page * ComponentName - what this component does. The responsibility of the component * Type - the type of component. Usually they are views, but they could be a Form, a List, a Figure, an Illustration, a Container Examples: * SideBar (root component) * FooterSideBar (with context) * SideBarForm (with component type) * FooterSideBarForm (with all)","title":"View Components"},{"location":"frontend/docs/react_naming_conventions/#custom-hook-components","text":"We will name custom hook components with a name starting with \u201cuse\u201d, as mentioned on the official docs .","title":"Custom Hook Components"},{"location":"frontend/docs/react_naming_conventions/#high-order-components","text":"We will name high order components (HOCs) using the \u201cwith\u201d prefix. For example: withAuthentication withSubscription","title":"High-order Components"},{"location":"frontend/docs/react_naming_conventions/#providerconsumer-components","text":"Whenever we need to use the Provider/Consumer pattern and name the components, we will use the \u201cProvider\u201d and \u201cConsumer\u201d suffixes. For example: LoginProvider/LoginConsumer DataProvider/DataConsumer","title":"Provider/Consumer Components"},{"location":"frontend/docs/react_naming_conventions/#prop-names","text":"","title":"Prop Names"},{"location":"frontend/docs/react_naming_conventions/#handler-functions","text":"We will use the \u201con\u201d prefix for handler functions, optionally adding a \u201csubject\u201d word before the event. The schema would be \u201con[Subject]Event\u201d, for example: onClick onItemClick onLogoHover onFormSubmit Internally, we will use the \u201chandle\u201d prefix for naming the handling functions, with an optional \u201csubject\u201d word in the middle. The schema would be then \u201chandle[Subject]Event\u201d, for example: handleClick handleButtonClick handleHeadingHover","title":"Handler Functions"},{"location":"frontend/docs/react_naming_conventions/#props-by-type","text":"Props should be named to describe the component itself, and what it does, but not why it does it. Objects - Use a singular noun item Arrays - Use a plural noun items users Numbers - Use a name with \u201cnum\u201d prefix or with \u201ccount\u201d or \u201cindex\u201d suffix. numItems userCount itemIndex Booleans - Use a \u201cis\u201d, \u201chas\u201d, or \u201ccan\u201d prefix \u201cis\u201d for visual variations isVisible isEnabled isActive \u201cis\u201d also for behavioral or conditional variations isToggleable isExpandable \u201chas\u201d for toggling UI elements hasCancelSection hasHeader React Nodes - use \u201celement\u201d suffix buttonElement itemElement","title":"Props by Type"},{"location":"frontend/docs/react_naming_conventions/#reference","text":"How to name props for React components \u2013 David\u2019s Blog Handy Naming Conventions for Event Handler Functions & Props in React | by Juan Bernal | JavaScript In Plain English | Medium https://medium.com/@wittydeveloper/react-components-naming-convention-%EF%B8%8F-b50303551505 https://reactjs.org/docs/hooks-custom.html#extracting-a-custom-hook https://reactjs.org/docs/higher-order-components.html","title":"Reference"},{"location":"frontend/docs/recommended_practices/","text":"Recommended Practices \u00b6 This document serves as reference for current practices and patterns that we want to standardize across Amundsen\u2019s frontend application code. Below, we provide some high-level guidelines targeted towards new contributors or any contributor who does not yet have domain knowledge in a particular framework or core library. This document is not intended to provide an exhaustive checklist for completing certain tasks. We aim to maintain a reasonably consistent code base through these practices and welcome PRs to update and improve these recommendations. Application \u00b6 Unit Testing \u00b6 We use Jest as our test framework. We leverage utility methods from Enzyme to test React components, and use redux-saga-test-plan to test our redux-saga middleware logic. General \u00b6 Leverage TypeScript to prevent bugs in unit tests and ensure that code is tested with inputs that match the defined interfaces and types. Adding and updating test fixtures helps to provide re-useable pieces of typed test data or mock implementations for this purpose. Leverage beforeAll() / beforeEach() for test setup when applicable. Leverage afterAll() / afterEach for test teardown when applicable to remove any side effects of the test block. For example if a mock implementation of a method was created in beforeAll() , the original implementation should be restored in afterAll() . See Jest\u2019s setup-teardown documentation for further understanding. Use descriptive title strings. To assist with debugging we should be able to understand what a test is checking for and under what conditions. Become familiar with the variety of Jest matchers that are available. Understanding the nuances of different matchers and the cases they are each ideal for assists with writing more robust tests. For example, there are many different ways to verify objects and the best matcher to use will depend on what exactly we are testing for. Examples: If asserting that inputObject is assigned to variable x , asserting the equivalence of x using .toBe() creates a more robust test for this case because .toBe() will verify that the variable is actually referencing the given object. Contrast this to a matcher like .toEqual() which will verify whether or not the object happens to have a particular set of properties and values. In this case using .toEqual() would risk hiding bugs where x is not actually referencing inputObject as expected, yet happens to have the same key value pairs perhaps due to side effects in the code. If asserting that outputObject matches expectedObject , asserting the equivalence of each property on outputObject using .toBe() or asserting the equality of the two objects using .toMatchObject() is useful when we only care that certain values exist on outputObject . However if it matters that certain values do not exist on outputObject \u2013 as is the case with reducer outputs \u2013 .toEqual() is a more robust alternative as it compares all properties on both objects for equivalence. When testing logic that makes use of JavaScript\u2019s Date object, note that our Jest scripts are configured to run in the UTC timezone. Developers should either: Mock the Date object and its methods\u2019 return values, and run assertions based on the mock values. Create assertions knowing that the unit test suite will run as if we are in the UTC timezone. Code coverage is important to track but it only informs us of what code was actually run and executed during the test. The onus is on the developer to focus on use case coverage and make sure that right assertions are run so that all logic is adequately tested. React \u00b6 Enzyme provides 3 different utilities for rendering React components for testing. We recommend using mount rendering so you can dive deep on the rendered output. Create a re-useable setup() function that will take any arguments needed to test conditional logic. Look for opportunities to organize tests a way such that one setup() can be used to test assertions that occur under the same conditions. For example, a test block for a method that has no conditional logic should only have one setup() . However, it is not recommended to share a setup() result across tests for different methods, or across tests for a method that has a dependency on a mutable piece of state. The reason is that we risk propagating side effects from one test block to another. Consider refactoring components or other files if they become burdensome to test. Potential options include (but are not limited to): Create subcomponents for large components. This is also especially useful for reducing the burden of updating tests when component layouts are changed. Break down large functions into smaller functions. Unit test the logic of the smaller functions individually, and mock their results when testing the larger function. Export constants from a separate file for hardcoded values and import them into the relevant source files and test files. This is especially helpful for strings. Redux \u00b6 Because the majority of Redux code consists of functions, we unit test those methods as usual and ensure the functions produce the expected output for any given input. See Redux\u2019s documentation on testing action creators , async action creators , and reducers , or check out examples in our code. Unless an action creator includes any logic other than returning the action, unit testing the reducer and saga middleware logic is sufficient and provides the most value. redux-saga generator functions can be tested by iterating through it step-by-step and running assertions at each step, or by executing the entire saga and running assertions on the side effects. See redux-saga\u2019s documentation on testing sagas for a wider breadth of examples.","title":"Recommended Practices"},{"location":"frontend/docs/recommended_practices/#recommended-practices","text":"This document serves as reference for current practices and patterns that we want to standardize across Amundsen\u2019s frontend application code. Below, we provide some high-level guidelines targeted towards new contributors or any contributor who does not yet have domain knowledge in a particular framework or core library. This document is not intended to provide an exhaustive checklist for completing certain tasks. We aim to maintain a reasonably consistent code base through these practices and welcome PRs to update and improve these recommendations.","title":"Recommended Practices"},{"location":"frontend/docs/recommended_practices/#application","text":"","title":"Application"},{"location":"frontend/docs/recommended_practices/#unit-testing","text":"We use Jest as our test framework. We leverage utility methods from Enzyme to test React components, and use redux-saga-test-plan to test our redux-saga middleware logic.","title":"Unit Testing"},{"location":"frontend/docs/recommended_practices/#general","text":"Leverage TypeScript to prevent bugs in unit tests and ensure that code is tested with inputs that match the defined interfaces and types. Adding and updating test fixtures helps to provide re-useable pieces of typed test data or mock implementations for this purpose. Leverage beforeAll() / beforeEach() for test setup when applicable. Leverage afterAll() / afterEach for test teardown when applicable to remove any side effects of the test block. For example if a mock implementation of a method was created in beforeAll() , the original implementation should be restored in afterAll() . See Jest\u2019s setup-teardown documentation for further understanding. Use descriptive title strings. To assist with debugging we should be able to understand what a test is checking for and under what conditions. Become familiar with the variety of Jest matchers that are available. Understanding the nuances of different matchers and the cases they are each ideal for assists with writing more robust tests. For example, there are many different ways to verify objects and the best matcher to use will depend on what exactly we are testing for. Examples: If asserting that inputObject is assigned to variable x , asserting the equivalence of x using .toBe() creates a more robust test for this case because .toBe() will verify that the variable is actually referencing the given object. Contrast this to a matcher like .toEqual() which will verify whether or not the object happens to have a particular set of properties and values. In this case using .toEqual() would risk hiding bugs where x is not actually referencing inputObject as expected, yet happens to have the same key value pairs perhaps due to side effects in the code. If asserting that outputObject matches expectedObject , asserting the equivalence of each property on outputObject using .toBe() or asserting the equality of the two objects using .toMatchObject() is useful when we only care that certain values exist on outputObject . However if it matters that certain values do not exist on outputObject \u2013 as is the case with reducer outputs \u2013 .toEqual() is a more robust alternative as it compares all properties on both objects for equivalence. When testing logic that makes use of JavaScript\u2019s Date object, note that our Jest scripts are configured to run in the UTC timezone. Developers should either: Mock the Date object and its methods\u2019 return values, and run assertions based on the mock values. Create assertions knowing that the unit test suite will run as if we are in the UTC timezone. Code coverage is important to track but it only informs us of what code was actually run and executed during the test. The onus is on the developer to focus on use case coverage and make sure that right assertions are run so that all logic is adequately tested.","title":"General"},{"location":"frontend/docs/recommended_practices/#react","text":"Enzyme provides 3 different utilities for rendering React components for testing. We recommend using mount rendering so you can dive deep on the rendered output. Create a re-useable setup() function that will take any arguments needed to test conditional logic. Look for opportunities to organize tests a way such that one setup() can be used to test assertions that occur under the same conditions. For example, a test block for a method that has no conditional logic should only have one setup() . However, it is not recommended to share a setup() result across tests for different methods, or across tests for a method that has a dependency on a mutable piece of state. The reason is that we risk propagating side effects from one test block to another. Consider refactoring components or other files if they become burdensome to test. Potential options include (but are not limited to): Create subcomponents for large components. This is also especially useful for reducing the burden of updating tests when component layouts are changed. Break down large functions into smaller functions. Unit test the logic of the smaller functions individually, and mock their results when testing the larger function. Export constants from a separate file for hardcoded values and import them into the relevant source files and test files. This is especially helpful for strings.","title":"React"},{"location":"frontend/docs/recommended_practices/#redux","text":"Because the majority of Redux code consists of functions, we unit test those methods as usual and ensure the functions produce the expected output for any given input. See Redux\u2019s documentation on testing action creators , async action creators , and reducers , or check out examples in our code. Unless an action creator includes any logic other than returning the action, unit testing the reducer and saga middleware logic is sufficient and provides the most value. redux-saga generator functions can be tested by iterating through it step-by-step and running assertions at each step, or by executing the entire saga and running assertions on the side effects. See redux-saga\u2019s documentation on testing sagas for a wider breadth of examples.","title":"Redux"},{"location":"frontend/docs/authentication/oidc/","text":"See this doc in our main repository for information on how to set up end-to-end authentication using OIDC.","title":"Oidc"},{"location":"frontend/docs/examples/announcement_client/","text":"Overview \u00b6 Amundsen\u2019s announcement feature requires that developers create a custom implementation of announcement_client for collecting announcements. This feature provide ability to deliver to Users announcements of different sort regarding data discovery service. Implementation \u00b6 Implement the announcement_client to make a request to system storing announcements. Shared Logic \u00b6 announcement_client implements _get_posts() of base_announcement_client with the minimal logic for this use case. It collects the posts from get_posts() method. try : announcements = self . get_posts () except Exception as e : message = 'Encountered exception getting posts: ' + str ( e ) return _create_error_response ( message ) It verifies the shape of the data before returning it to the application. If the data does not match the AnnouncementsSchema , the request will fail. # validate the returned object data , errors = AnnouncementsSchema () . dump ( announcements ) if not errors : payload = jsonify ({ 'posts' : data . get ( 'posts' ), 'msg' : 'Success' }) return make_response ( payload , HTTPStatus . OK ) else : message = 'Announcement data dump returned errors: ' + str ( errors ) return _create_error_response ( message ) Custom Logic \u00b6 announcement_client has an abstract method get_posts() . This method will contain whatever custom logic is needed to collect announcements. The system within which they are stored can be anything that has programmatic access. Announcements could be collected from database (as in exemplary SQLAlchemyAnnouncementClient), kafka persistent topic, web rss feed, etc. See the following example_announcement_client for an example implementation of base_announcement_client and get_posts() . This example assumes a temporary sqlite database with no security, authentication, persistence or authorization configured. Usage \u00b6 Under the [announcement_client] group, point the announcement_client entry point in your local setup.py to your custom class. entry_points=\"\"\" ... [announcement_client] announcement_client_class = amundsen_application.base.examples.example_announcement_client:SQLAlchemyAnnouncementClient \"\"\" What do I need to run exemplary announcement_client ? \u00b6 Exemplary client requires installation of SQLAlchemy to run properly: pip install SQLAlchemy == 1 .3.17 Run python3 setup.py install in your virtual environment and restart the application for the entry point changes to take effect","title":"Overview"},{"location":"frontend/docs/examples/announcement_client/#overview","text":"Amundsen\u2019s announcement feature requires that developers create a custom implementation of announcement_client for collecting announcements. This feature provide ability to deliver to Users announcements of different sort regarding data discovery service.","title":"Overview"},{"location":"frontend/docs/examples/announcement_client/#implementation","text":"Implement the announcement_client to make a request to system storing announcements.","title":"Implementation"},{"location":"frontend/docs/examples/announcement_client/#shared-logic","text":"announcement_client implements _get_posts() of base_announcement_client with the minimal logic for this use case. It collects the posts from get_posts() method. try : announcements = self . get_posts () except Exception as e : message = 'Encountered exception getting posts: ' + str ( e ) return _create_error_response ( message ) It verifies the shape of the data before returning it to the application. If the data does not match the AnnouncementsSchema , the request will fail. # validate the returned object data , errors = AnnouncementsSchema () . dump ( announcements ) if not errors : payload = jsonify ({ 'posts' : data . get ( 'posts' ), 'msg' : 'Success' }) return make_response ( payload , HTTPStatus . OK ) else : message = 'Announcement data dump returned errors: ' + str ( errors ) return _create_error_response ( message )","title":"Shared Logic"},{"location":"frontend/docs/examples/announcement_client/#custom-logic","text":"announcement_client has an abstract method get_posts() . This method will contain whatever custom logic is needed to collect announcements. The system within which they are stored can be anything that has programmatic access. Announcements could be collected from database (as in exemplary SQLAlchemyAnnouncementClient), kafka persistent topic, web rss feed, etc. See the following example_announcement_client for an example implementation of base_announcement_client and get_posts() . This example assumes a temporary sqlite database with no security, authentication, persistence or authorization configured.","title":"Custom Logic"},{"location":"frontend/docs/examples/announcement_client/#usage","text":"Under the [announcement_client] group, point the announcement_client entry point in your local setup.py to your custom class. entry_points=\"\"\" ... [announcement_client] announcement_client_class = amundsen_application.base.examples.example_announcement_client:SQLAlchemyAnnouncementClient \"\"\"","title":"Usage"},{"location":"frontend/docs/examples/announcement_client/#what-do-i-need-to-run-exemplary-announcement_client","text":"Exemplary client requires installation of SQLAlchemy to run properly: pip install SQLAlchemy == 1 .3.17 Run python3 setup.py install in your virtual environment and restart the application for the entry point changes to take effect","title":"What do I need to run exemplary announcement_client ?"},{"location":"frontend/docs/examples/redash_preview_client/","text":"Overview \u00b6 Amundsen\u2019s data preview feature requires that developers create a custom implementation of base_preview_client for requesting that data. This feature assists with data discovery by providing the end user the option to view a sample of the actual resource data so that they can verify whether or not they want to transition into exploring that data, or continue their search. Redash is an open-source business intelligence tool that can be used for data exploration. This document provides some insight into how to configure Amundsen\u2019s frontend application to leverage Redash for data previews. Implementation \u00b6 You will need to complete the following steps to use Redash to serve your Amundsen data preview: Create a query template in Redash Generate an API key in Redash for Authentication Create a custom RedashPreviewClient class Update your setup.py and re-install the Amundsen frontend 1. Creating a query template in Redash \u00b6 In order for Amundsen to execute a query through Redash the query must already exist and be saved in Redash. Redash allows queries to be built using parameters and Amundsen uses these parameters to inject the {schema}.{table} into the query. When defining a new query the following SQL should be used: select {{ SELECT_FIELDS }} from {{ SCHEMA_NAME }} . {{ TABLE_NAME }} {{ WHERE_CLAUSE }} limit {{ RCD_LIMIT }} Visit the Redash documentation on query parameters for more information on how query templating works. 2. Generate an API key in Redash for Authentication \u00b6 Redash uses API keys for authentication. While there are two types of API keys that Redash supports (User API keys and Query specific API keys), the Amundsen integration requires a User API key. The Query API keys do not support the dynamic templating that is required to build the queries. The API key used by Amundsen must have access to query the underlying table in Redash. By default, only a single API key is required for your Redash Preview Client in Amundsen. However, you may have different databases in Redash that require different access. The Redash Preview Client allows you to dynamically select which API key to use for authentication in the event you want more fine-grained control over your data preview access. To select an API key on the fly based off of the database, cluster, schema and table that is being queried, override the _get_query_api_key function (see using parameters ] below for an example). 3. Create a custom RedashPreviewClient class \u00b6 The file base_redash_preview_client provides two examples for implementing the Redash preview client. One simple implementation that only implements the minimal requirements: a User API key and a way to look up which Redash query ID to execute One complex implementation which uses a different API key per query, increases the # of records returned to 100, reduces the max cache time in Redash to 1 hour, provides custom field masking in the query and creates custom where clauses for specific tables to filter the correct data returned to the preview client. Using params to lookup resources \u00b6 Several of the functions in BaseRedashPreviewClient have a single input: params and expect a single output (e.g. a query ID, query API key, custom where clause, etc.). These params are a dictionary that contain the following fields: { \"database\" : \"snowflake\" , \"cluster\" : \"ca_covid\" , \"schema\" : \"open_data\" , \"tableName\" : \"statewide_cases\" } It is expected that these values can generally be used to uniquely reference your resources. For example, if you have two databases snowflake.ca_covid and snowflake.marketing the implementation to find the correct query ID may look like: ... def get_redash_query_id ( self , params : Dict ) -> Optional [ int ]: SOURCE_DB_QUERY_MAP = { 'snowflake.open_data' : 1 , 'snowflake.marketing' : 27 , } database = params [ 'database' ] cluster = params [ 'cluster' ] db_cluster_key = f ' { database } . { cluster } ' return SOURCE_DB_QUERY_MAP . get ( db_cluster_key ) This same paradigm applies to the all funcitons with the same input signature: get_redash_query_id , _get_query_api_key , get_select_fields , and get_where_clause . 4. Update setup.py and reinstall Amundsen \u00b6 In the setup.py at the root of this repo, add a [preview_client] group and point the table_preview_client_class to your custom class. entry_points = \"\"\" ... [preview_client] table_preview_client_class = amundsen_application.base.examples.example_redash_preview_client:RedashSimplePreviewClient \"\"\" Run python3 setup.py install in your virtual environment and restart the application for the entry point changes to take effect.","title":"Overview"},{"location":"frontend/docs/examples/redash_preview_client/#overview","text":"Amundsen\u2019s data preview feature requires that developers create a custom implementation of base_preview_client for requesting that data. This feature assists with data discovery by providing the end user the option to view a sample of the actual resource data so that they can verify whether or not they want to transition into exploring that data, or continue their search. Redash is an open-source business intelligence tool that can be used for data exploration. This document provides some insight into how to configure Amundsen\u2019s frontend application to leverage Redash for data previews.","title":"Overview"},{"location":"frontend/docs/examples/redash_preview_client/#implementation","text":"You will need to complete the following steps to use Redash to serve your Amundsen data preview: Create a query template in Redash Generate an API key in Redash for Authentication Create a custom RedashPreviewClient class Update your setup.py and re-install the Amundsen frontend","title":"Implementation"},{"location":"frontend/docs/examples/redash_preview_client/#1-creating-a-query-template-in-redash","text":"In order for Amundsen to execute a query through Redash the query must already exist and be saved in Redash. Redash allows queries to be built using parameters and Amundsen uses these parameters to inject the {schema}.{table} into the query. When defining a new query the following SQL should be used: select {{ SELECT_FIELDS }} from {{ SCHEMA_NAME }} . {{ TABLE_NAME }} {{ WHERE_CLAUSE }} limit {{ RCD_LIMIT }} Visit the Redash documentation on query parameters for more information on how query templating works.","title":"1. Creating a query template in Redash"},{"location":"frontend/docs/examples/redash_preview_client/#2-generate-an-api-key-in-redash-for-authentication","text":"Redash uses API keys for authentication. While there are two types of API keys that Redash supports (User API keys and Query specific API keys), the Amundsen integration requires a User API key. The Query API keys do not support the dynamic templating that is required to build the queries. The API key used by Amundsen must have access to query the underlying table in Redash. By default, only a single API key is required for your Redash Preview Client in Amundsen. However, you may have different databases in Redash that require different access. The Redash Preview Client allows you to dynamically select which API key to use for authentication in the event you want more fine-grained control over your data preview access. To select an API key on the fly based off of the database, cluster, schema and table that is being queried, override the _get_query_api_key function (see using parameters ] below for an example).","title":"2. Generate an API key in Redash for Authentication"},{"location":"frontend/docs/examples/redash_preview_client/#3-create-a-custom-redashpreviewclient-class","text":"The file base_redash_preview_client provides two examples for implementing the Redash preview client. One simple implementation that only implements the minimal requirements: a User API key and a way to look up which Redash query ID to execute One complex implementation which uses a different API key per query, increases the # of records returned to 100, reduces the max cache time in Redash to 1 hour, provides custom field masking in the query and creates custom where clauses for specific tables to filter the correct data returned to the preview client.","title":"3. Create a custom RedashPreviewClient class"},{"location":"frontend/docs/examples/redash_preview_client/#using-params-to-lookup-resources","text":"Several of the functions in BaseRedashPreviewClient have a single input: params and expect a single output (e.g. a query ID, query API key, custom where clause, etc.). These params are a dictionary that contain the following fields: { \"database\" : \"snowflake\" , \"cluster\" : \"ca_covid\" , \"schema\" : \"open_data\" , \"tableName\" : \"statewide_cases\" } It is expected that these values can generally be used to uniquely reference your resources. For example, if you have two databases snowflake.ca_covid and snowflake.marketing the implementation to find the correct query ID may look like: ... def get_redash_query_id ( self , params : Dict ) -> Optional [ int ]: SOURCE_DB_QUERY_MAP = { 'snowflake.open_data' : 1 , 'snowflake.marketing' : 27 , } database = params [ 'database' ] cluster = params [ 'cluster' ] db_cluster_key = f ' { database } . { cluster } ' return SOURCE_DB_QUERY_MAP . get ( db_cluster_key ) This same paradigm applies to the all funcitons with the same input signature: get_redash_query_id , _get_query_api_key , get_select_fields , and get_where_clause .","title":"Using params to lookup resources"},{"location":"frontend/docs/examples/redash_preview_client/#4-update-setuppy-and-reinstall-amundsen","text":"In the setup.py at the root of this repo, add a [preview_client] group and point the table_preview_client_class to your custom class. entry_points = \"\"\" ... [preview_client] table_preview_client_class = amundsen_application.base.examples.example_redash_preview_client:RedashSimplePreviewClient \"\"\" Run python3 setup.py install in your virtual environment and restart the application for the entry point changes to take effect.","title":"4. Update setup.py and reinstall Amundsen"},{"location":"frontend/docs/examples/superset_preview_client/","text":"Overview \u00b6 Amundsen\u2019s data preview feature requires that developers create a custom implementation of base_preview_client for requesting that data. This feature assists with data discovery by providing the end user the option to view a sample of the actual resource data so that they can verify whether or not they want to transition into exploring that data, or continue their search. Apache Superset is an open-source business intelligence tool that can be used for data exploration. Amundsen\u2019s data preview feature was created with Superset in mind, and it is what we leverage internally at Lyft to support the feature. This document provides some insight into how to configure Amundsen\u2019s frontend application to leverage Superset for data previews. Implementation \u00b6 Implement the base_superset_preview_client to make a request to an instance of Superset. Shared Logic \u00b6 base_superset_preview_client implements get_preview_data() of base_preview_client with the minimal logic for this use case. It updates the headers for the request if optionalHeaders are passed in get_preview_data() # Clone headers so that it does not mutate instance's state headers = dict(self.headers) # Merge optionalHeaders into headers if optionalHeaders is not None: headers.update(optionalHeaders) It verifies the shape of the data before returning it to the application. If the data does not match the PreviewDataSchema , the request will fail. # Verify and return the results response_dict = response.json() columns = [ColumnItem(c['name'], c['type']) for c in response_dict['columns']] preview_data = PreviewData(columns, response_dict['data']) data, errors = PreviewDataSchema().dump(preview_data) if not errors: payload = jsonify({'preview_data': data}) return make_response(payload, response.status_code) else: return make_response(jsonify({'preview_data': {}}), HTTPStatus.INTERNAL_SERVER_ERROR) Custom Logic \u00b6 base_superset_preview_client has an abstract method post_to_sql_json() . This method will contain whatever custom logic is needed to make a successful request to the sql_json enpoint based on the protections you have configured on this endpoint on your instance of Superset. For example, this may be where you have to append other values to the headers, or generate SQL queries based on your use case. See the following example_superset_preview_client for an example implementation of base_superset_preview_client and post_to_sql_json() . This example assumes a local instance of Superset running on port 8088 with no security, authentication, or authorization configured on the endpoint. Usage \u00b6 Under the [preview_client] group, point the table_preview_client_class entry point in your local setup.py to your custom class. entry_points=\"\"\" ... [preview_client] table_preview_client_class = amundsen_application.base.examples.example_superset_preview_client:SupersetPreviewClient \"\"\" Run python3 setup.py install in your virtual environment and restart the application for the entry point changes to take effect","title":"Preview Client Setup"},{"location":"frontend/docs/examples/superset_preview_client/#overview","text":"Amundsen\u2019s data preview feature requires that developers create a custom implementation of base_preview_client for requesting that data. This feature assists with data discovery by providing the end user the option to view a sample of the actual resource data so that they can verify whether or not they want to transition into exploring that data, or continue their search. Apache Superset is an open-source business intelligence tool that can be used for data exploration. Amundsen\u2019s data preview feature was created with Superset in mind, and it is what we leverage internally at Lyft to support the feature. This document provides some insight into how to configure Amundsen\u2019s frontend application to leverage Superset for data previews.","title":"Overview"},{"location":"frontend/docs/examples/superset_preview_client/#implementation","text":"Implement the base_superset_preview_client to make a request to an instance of Superset.","title":"Implementation"},{"location":"frontend/docs/examples/superset_preview_client/#shared-logic","text":"base_superset_preview_client implements get_preview_data() of base_preview_client with the minimal logic for this use case. It updates the headers for the request if optionalHeaders are passed in get_preview_data() # Clone headers so that it does not mutate instance's state headers = dict(self.headers) # Merge optionalHeaders into headers if optionalHeaders is not None: headers.update(optionalHeaders) It verifies the shape of the data before returning it to the application. If the data does not match the PreviewDataSchema , the request will fail. # Verify and return the results response_dict = response.json() columns = [ColumnItem(c['name'], c['type']) for c in response_dict['columns']] preview_data = PreviewData(columns, response_dict['data']) data, errors = PreviewDataSchema().dump(preview_data) if not errors: payload = jsonify({'preview_data': data}) return make_response(payload, response.status_code) else: return make_response(jsonify({'preview_data': {}}), HTTPStatus.INTERNAL_SERVER_ERROR)","title":"Shared Logic"},{"location":"frontend/docs/examples/superset_preview_client/#custom-logic","text":"base_superset_preview_client has an abstract method post_to_sql_json() . This method will contain whatever custom logic is needed to make a successful request to the sql_json enpoint based on the protections you have configured on this endpoint on your instance of Superset. For example, this may be where you have to append other values to the headers, or generate SQL queries based on your use case. See the following example_superset_preview_client for an example implementation of base_superset_preview_client and post_to_sql_json() . This example assumes a local instance of Superset running on port 8088 with no security, authentication, or authorization configured on the endpoint.","title":"Custom Logic"},{"location":"frontend/docs/examples/superset_preview_client/#usage","text":"Under the [preview_client] group, point the table_preview_client_class entry point in your local setup.py to your custom class. entry_points=\"\"\" ... [preview_client] table_preview_client_class = amundsen_application.base.examples.example_superset_preview_client:SupersetPreviewClient \"\"\" Run python3 setup.py install in your virtual environment and restart the application for the entry point changes to take effect","title":"Usage"},{"location":"installation-aws-ecs/aws-ecs-deployment/","text":"Deployment of non-production Amundsen on AWS ECS using aws-cli \u00b6 The following is a set of instructions to run Amundsen on AWS Elastic Container Service. The current configuration is very basic but it is working. It is a migration of the docker-amundsen.yml to run on AWS ECS. Install ECS CLI \u00b6 The first step is to install ECS CLI, please follow the instructions from AWS documentation Get your access and secret keys from IAM \u00b6 # in ~/<your-path-to-cloned-repo>/amundsenfrontendlibrary/docs/instalation-aws-ecs $ export AWS_ACCESS_KEY_ID = xxxxxxxx $ export AWS_SECRET_ACCESS_KEY = xxxxxx $ export AWS_PROFILE = profilename For the purpose of this instruction we used the tutorial on AWS documentation Enter the cloned directory: cd amundsen/docs/installation-aws-ecs STEP 1: Create a cluster configuration: \u00b6 # in ~/<your-path-to-cloned-repo>/amundsenfrontendlibrary/docs/instalation-aws-ecs $ ecs-cli configure --cluster amundsen --region us-west-2 --default-launch-type EC2 --config-name amundsen STEP 2: Create a profile using your access key and secret key: \u00b6 # in ~/<your-path-to-cloned-repo>/amundsen/docs/installation-aws-ecs $ ecs-cli configure profile --access-key $AWS_ACCESS_KEY_ID --secret-key $AWS_SECRET_ACCESS_KEY --profile-name amundsen STEP 3: Create the Cluster Use profile name from \\~/.aws/credentials \u00b6 # in ~/<your-path-to-cloned-repo>/amundsen/docs/installation-aws-ecs $ ecs-cli up --keypair JoaoCorreia --extra-user-data userData.sh --capability-iam --size 1 --instance-type t2.large --cluster-config amundsen --verbose --force --aws-profile $AWS_PROFILE STEP 4: Deploy the Compose File to a Cluster \u00b6 # in ~/<your-path-to-cloned-repo>/amundsen/docs/installation-aws-ecs $ ecs-cli compose --cluster-config amundsen --file docker-ecs-amundsen.yml up --create-log-groups You can use the ECS CLI to see what tasks are running. $ ecs-cli ps STEP 5 Open the EC2 Instance \u00b6 Edit the Security Group to allow traffic to your IP, you should be able to see the frontend, elasticsearch and neo4j by visiting the URLs: http://xxxxxxx:5000/ http://xxxxxxx:9200/ http://xxxxxxx:7474/browser/ TODO \u00b6 Configuration sent to services not working properly (amunsen.db vs graph.db) Create a persistent volume for graph/metadata storage. See this Refactor the VPC and default security group permissions","title":"AWS ECS Installation"},{"location":"installation-aws-ecs/aws-ecs-deployment/#deployment-of-non-production-amundsen-on-aws-ecs-using-aws-cli","text":"The following is a set of instructions to run Amundsen on AWS Elastic Container Service. The current configuration is very basic but it is working. It is a migration of the docker-amundsen.yml to run on AWS ECS.","title":"Deployment of non-production Amundsen on AWS ECS using aws-cli"},{"location":"installation-aws-ecs/aws-ecs-deployment/#install-ecs-cli","text":"The first step is to install ECS CLI, please follow the instructions from AWS documentation","title":"Install ECS CLI"},{"location":"installation-aws-ecs/aws-ecs-deployment/#get-your-access-and-secret-keys-from-iam","text":"# in ~/<your-path-to-cloned-repo>/amundsenfrontendlibrary/docs/instalation-aws-ecs $ export AWS_ACCESS_KEY_ID = xxxxxxxx $ export AWS_SECRET_ACCESS_KEY = xxxxxx $ export AWS_PROFILE = profilename For the purpose of this instruction we used the tutorial on AWS documentation Enter the cloned directory: cd amundsen/docs/installation-aws-ecs","title":"Get your access and secret keys from IAM"},{"location":"installation-aws-ecs/aws-ecs-deployment/#step-1-create-a-cluster-configuration","text":"# in ~/<your-path-to-cloned-repo>/amundsenfrontendlibrary/docs/instalation-aws-ecs $ ecs-cli configure --cluster amundsen --region us-west-2 --default-launch-type EC2 --config-name amundsen","title":"STEP 1: Create a cluster configuration:"},{"location":"installation-aws-ecs/aws-ecs-deployment/#step-2-create-a-profile-using-your-access-key-and-secret-key","text":"# in ~/<your-path-to-cloned-repo>/amundsen/docs/installation-aws-ecs $ ecs-cli configure profile --access-key $AWS_ACCESS_KEY_ID --secret-key $AWS_SECRET_ACCESS_KEY --profile-name amundsen","title":"STEP 2: Create a profile using your access key and secret key:"},{"location":"installation-aws-ecs/aws-ecs-deployment/#step-3-create-the-cluster-use-profile-name-from-awscredentials","text":"# in ~/<your-path-to-cloned-repo>/amundsen/docs/installation-aws-ecs $ ecs-cli up --keypair JoaoCorreia --extra-user-data userData.sh --capability-iam --size 1 --instance-type t2.large --cluster-config amundsen --verbose --force --aws-profile $AWS_PROFILE","title":"STEP 3: Create the Cluster Use profile name from \\~/.aws/credentials"},{"location":"installation-aws-ecs/aws-ecs-deployment/#step-4-deploy-the-compose-file-to-a-cluster","text":"# in ~/<your-path-to-cloned-repo>/amundsen/docs/installation-aws-ecs $ ecs-cli compose --cluster-config amundsen --file docker-ecs-amundsen.yml up --create-log-groups You can use the ECS CLI to see what tasks are running. $ ecs-cli ps","title":"STEP 4: Deploy the Compose File to a Cluster"},{"location":"installation-aws-ecs/aws-ecs-deployment/#step-5-open-the-ec2-instance","text":"Edit the Security Group to allow traffic to your IP, you should be able to see the frontend, elasticsearch and neo4j by visiting the URLs: http://xxxxxxx:5000/ http://xxxxxxx:9200/ http://xxxxxxx:7474/browser/","title":"STEP 5 Open the EC2 Instance"},{"location":"installation-aws-ecs/aws-ecs-deployment/#todo","text":"Configuration sent to services not working properly (amunsen.db vs graph.db) Create a persistent volume for graph/metadata storage. See this Refactor the VPC and default security group permissions","title":"TODO"},{"location":"metadata/","text":"Amundsen Metadata Service \u00b6 Amundsen Metadata service serves Restful API and is responsible for providing and also updating metadata, such as table & column description, and tags. Metadata service can use Neo4j, Apache Atlas, AWS Neptune Or Mysql RDS as a persistent layer. For information about Amundsen and our other services, refer to this README.md . Please also see our instructions for a quick start setup of Amundsen with dummy data, and an overview of the architecture . Requirements \u00b6 Python >= 3.7 Doc \u00b6 https://www.amundsen.io/amundsen/ Instructions to start the Metadata service from distribution \u00b6 $ venv_path =[ path_for_virtual_environment ] $ python3 -m venv $venv_path $ source $venv_path /bin/activate $ pip3 install amundsen-metadata $ python3 metadata_service/metadata_wsgi.py -- In a different terminal, verify getting HTTP/1.0 200 OK $ curl -v http://localhost:5002/healthcheck Instructions to start the Metadata service from the source \u00b6 $ git clone https://github.com/amundsen-io/amundsenmetadatalibrary.git $ cd amundsenmetadatalibrary $ python3 -m venv venv $ source venv/bin/activate $ pip3 install -e \".[all]\" . $ python3 metadata_service/metadata_wsgi.py -- In a different terminal, verify getting HTTP/1.0 200 OK $ curl -v http://localhost:5002/healthcheck Instructions to start the service from Docker \u00b6 $ docker pull amundsendev/amundsen-metadata:latest $ docker run -p 5002 :5002 amundsendev/amundsen-metadata # - alternative, for production environment with Gunicorn (see its homepage link below) $ ## docker run -p 5002:5002 amundsendev/amundsen-metadata gunicorn --bind 0.0.0.0:5002 metadata_service.metadata_wsgi -- In a different terminal, verify getting HTTP/1.0 200 OK $ curl -v http://localhost:5002/healthcheck Production environment \u00b6 By default, Flask comes with Werkzeug webserver, which is for development. For production environment use production grade web server such as Gunicorn . $ pip install gunicorn $ gunicorn metadata_service.metadata_wsgi Here is documentation of gunicorn configuration. Configuration outside local environment \u00b6 By default, Metadata service uses LocalConfig that looks for Neo4j running in localhost. In order to use different end point, you need to create Config suitable for your use case. Once config class has been created, it can be referenced by environment variable : METADATA_SVC_CONFIG_MODULE_CLASS For example, in order to have different config for production, you can inherit Config class, create Production config and passing production config class into environment variable. Let\u2019s say class name is ProdConfig and it\u2019s in metadata_service.config module. then you can set as below: METADATA_SVC_CONFIG_MODULE_CLASS=metadata_service.config.ProdConfig This way Metadata service will use production config in production environment. For more information on how the configuration is being loaded and used, here\u2019s reference from Flask doc . Apache Atlas \u00b6 Amundsen Metadata service can use Apache Atlas as a backend. Some of the benefits of using Apache Atlas instead of Neo4j is that Apache Atlas offers plugins to several services (e.g. Apache Hive, Apache Spark) that allow for push based updates. It also allows to set policies on what metadata is accesible and editable by means of Apache Ranger. If you would like to use Apache Atlas as a backend for Metadata service you will need to create a Config as mentioned above. Make sure to include the following: PROXY_CLIENT = PROXY_CLIENTS [ 'ATLAS' ] # or env PROXY_CLIENT='ATLAS' PROXY_PORT = 21000 # or env PROXY_PORT PROXY_USER = 'atlasuser' # or env CREDENTIALS_PROXY_USER PROXY_PASSWORD = 'password' # or env CREDENTIALS_PROXY_PASSWORD To start the service with Atlas from Docker. Make sure you have atlasserver configured in DNS (or docker-compose) $ docker run -p 5002 :5002 --env PROXY_CLIENT = ATLAS --env PROXY_PORT = 21000 --env PROXY_HOST = atlasserver --env CREDENTIALS_PROXY_USER = atlasuser --env CREDENTIALS_PROXY_PASSWORD = password amundsen-metadata:latest NOTE The support for Apache Atlas is work in progress. For example, while Apache Atlas supports fine grained access, Amundsen does not support this yet. Developer guide \u00b6 Code style \u00b6 PEP 8: Amundsen Metadata service follows PEP8 - Style Guide for Python Code . Typing hints: Amundsen Metadata service also utilizes Typing hint for better readability. API documentation \u00b6 We have Swagger documentation setup with OpenApi 3.0.2. This documentation is generated via Flasgger. When adding or updating an API please make sure to update the documentation. To see the documentation run the application locally and go to localhost:5002/apidocs/. Currently the documentation only works with local configuration. Code structure \u00b6 Please visit Code Structure to read how different modules are structured in Amundsen Metadata service. Roundtrip tests \u00b6 Roundtrip tests are a new feature - by implementing the abstract_proxy_tests and some test setup endpoints in the base_proxy, you can validate your proxy code against the actual data store. These tests do not run by default, but can be run by passing the --roundtrip-[proxy] argument. Note this requires a fully-configured backend to test against. $ python -m pytest --roundtrip-neptune .","title":"Overview"},{"location":"metadata/#amundsen-metadata-service","text":"Amundsen Metadata service serves Restful API and is responsible for providing and also updating metadata, such as table & column description, and tags. Metadata service can use Neo4j, Apache Atlas, AWS Neptune Or Mysql RDS as a persistent layer. For information about Amundsen and our other services, refer to this README.md . Please also see our instructions for a quick start setup of Amundsen with dummy data, and an overview of the architecture .","title":"Amundsen Metadata Service"},{"location":"metadata/#requirements","text":"Python >= 3.7","title":"Requirements"},{"location":"metadata/#doc","text":"https://www.amundsen.io/amundsen/","title":"Doc"},{"location":"metadata/#instructions-to-start-the-metadata-service-from-distribution","text":"$ venv_path =[ path_for_virtual_environment ] $ python3 -m venv $venv_path $ source $venv_path /bin/activate $ pip3 install amundsen-metadata $ python3 metadata_service/metadata_wsgi.py -- In a different terminal, verify getting HTTP/1.0 200 OK $ curl -v http://localhost:5002/healthcheck","title":"Instructions to start the Metadata service from distribution"},{"location":"metadata/#instructions-to-start-the-metadata-service-from-the-source","text":"$ git clone https://github.com/amundsen-io/amundsenmetadatalibrary.git $ cd amundsenmetadatalibrary $ python3 -m venv venv $ source venv/bin/activate $ pip3 install -e \".[all]\" . $ python3 metadata_service/metadata_wsgi.py -- In a different terminal, verify getting HTTP/1.0 200 OK $ curl -v http://localhost:5002/healthcheck","title":"Instructions to start the Metadata service from the source"},{"location":"metadata/#instructions-to-start-the-service-from-docker","text":"$ docker pull amundsendev/amundsen-metadata:latest $ docker run -p 5002 :5002 amundsendev/amundsen-metadata # - alternative, for production environment with Gunicorn (see its homepage link below) $ ## docker run -p 5002:5002 amundsendev/amundsen-metadata gunicorn --bind 0.0.0.0:5002 metadata_service.metadata_wsgi -- In a different terminal, verify getting HTTP/1.0 200 OK $ curl -v http://localhost:5002/healthcheck","title":"Instructions to start the service from Docker"},{"location":"metadata/#production-environment","text":"By default, Flask comes with Werkzeug webserver, which is for development. For production environment use production grade web server such as Gunicorn . $ pip install gunicorn $ gunicorn metadata_service.metadata_wsgi Here is documentation of gunicorn configuration.","title":"Production environment"},{"location":"metadata/#configuration-outside-local-environment","text":"By default, Metadata service uses LocalConfig that looks for Neo4j running in localhost. In order to use different end point, you need to create Config suitable for your use case. Once config class has been created, it can be referenced by environment variable : METADATA_SVC_CONFIG_MODULE_CLASS For example, in order to have different config for production, you can inherit Config class, create Production config and passing production config class into environment variable. Let\u2019s say class name is ProdConfig and it\u2019s in metadata_service.config module. then you can set as below: METADATA_SVC_CONFIG_MODULE_CLASS=metadata_service.config.ProdConfig This way Metadata service will use production config in production environment. For more information on how the configuration is being loaded and used, here\u2019s reference from Flask doc .","title":"Configuration outside local environment"},{"location":"metadata/#apache-atlas","text":"Amundsen Metadata service can use Apache Atlas as a backend. Some of the benefits of using Apache Atlas instead of Neo4j is that Apache Atlas offers plugins to several services (e.g. Apache Hive, Apache Spark) that allow for push based updates. It also allows to set policies on what metadata is accesible and editable by means of Apache Ranger. If you would like to use Apache Atlas as a backend for Metadata service you will need to create a Config as mentioned above. Make sure to include the following: PROXY_CLIENT = PROXY_CLIENTS [ 'ATLAS' ] # or env PROXY_CLIENT='ATLAS' PROXY_PORT = 21000 # or env PROXY_PORT PROXY_USER = 'atlasuser' # or env CREDENTIALS_PROXY_USER PROXY_PASSWORD = 'password' # or env CREDENTIALS_PROXY_PASSWORD To start the service with Atlas from Docker. Make sure you have atlasserver configured in DNS (or docker-compose) $ docker run -p 5002 :5002 --env PROXY_CLIENT = ATLAS --env PROXY_PORT = 21000 --env PROXY_HOST = atlasserver --env CREDENTIALS_PROXY_USER = atlasuser --env CREDENTIALS_PROXY_PASSWORD = password amundsen-metadata:latest NOTE The support for Apache Atlas is work in progress. For example, while Apache Atlas supports fine grained access, Amundsen does not support this yet.","title":"Apache Atlas"},{"location":"metadata/#developer-guide","text":"","title":"Developer guide"},{"location":"metadata/#code-style","text":"PEP 8: Amundsen Metadata service follows PEP8 - Style Guide for Python Code . Typing hints: Amundsen Metadata service also utilizes Typing hint for better readability.","title":"Code style"},{"location":"metadata/#api-documentation","text":"We have Swagger documentation setup with OpenApi 3.0.2. This documentation is generated via Flasgger. When adding or updating an API please make sure to update the documentation. To see the documentation run the application locally and go to localhost:5002/apidocs/. Currently the documentation only works with local configuration.","title":"API documentation"},{"location":"metadata/#code-structure","text":"Please visit Code Structure to read how different modules are structured in Amundsen Metadata service.","title":"Code structure"},{"location":"metadata/#roundtrip-tests","text":"Roundtrip tests are a new feature - by implementing the abstract_proxy_tests and some test setup endpoints in the base_proxy, you can validate your proxy code against the actual data store. These tests do not run by default, but can be run by passing the --roundtrip-[proxy] argument. Note this requires a fully-configured backend to test against. $ python -m pytest --roundtrip-neptune .","title":"Roundtrip tests"},{"location":"metadata/CHANGELOG/","text":"Feature \u00b6 Create users in metadata backend via API ( #289 ) ( eeba485 ) Add Column Badge API ( #273 ) ( ee0ac63 ) Column Lineage API ( #280 ) ( 681893f ) Table Lineage API ( #262 ) ( e306034 ) Column badges in Atlas Proxy ( #263 ) ( a3efb4c ) Added get_lineage method to neo4j proxy ( #259 ) ( b129cc7 ) Neo4j backend for popular tables personalization ( #233 ) ( d045efa ) Updated popular_tables endpoint to allow optional user_id ( #232 ) ( 5680775 ) AddSwaggerEnabledAsEnvVar ( #215 ) ( 3c9a55e ) Add neptune proxy ( #204 ) ( 09845d4 ) Return column level badges ( #205 ) ( d4d8101 ) Badges with category and badge_type fields ( #201 ) ( 19e1bf8 ) Get last updated ts for AtlasProxy ( #177 ) ( 3e92586 ) Data Owner Implementation of Atlas Proxy ( #156 ) ( 48b4c71 ) Fix \u00b6 Sort reports alphabetically ( #293 ) ( c8423c6 ) Reconcile gremlin description writes with the Databuilder ( #290 ) ( 18454fe ) Swagger docs don\u2019t align with common entity ( #283 ) ( db33af9 ) Compatibility changes to the gremlin integration ( #260 ) ( a765424 ) Proxy client creation fails after adding client_kwargs ( #258 ) ( 1880cec ) Reapply previous fix ( #245 ) ( c7dc172 ) Expire is a keyword argument for beaker cache ( #239 ) ( a7b2ec5 ) Get_tags no longer return tag_count 0 tag ( #230 ) ( 5097d2b ) Too many partitions in one Atlas query (Watermarks in Atlas Proxy) ( #217 ) ( cc3768f ) Data_type as Column.col_type. ( #203 ) ( 7b97f62 ) Add /delete bidirectional relations in case of owner ( #206 ) ( 40cd0dd ) Removed all badge_type fields from API ( #202 ) ( 6a81b97 ) Improvements to the Owned By feature ( #178 ) ( 0558d69 ) Get all tags should work for all resources ( #175 ) ( cf1ab6d ) Removing OidcConfig file and making statsd configurable through envrionment variable ( #157 ) ( 2752492 ) Documentation \u00b6 Enumeration requires newline ( #291 ) ( cf6710d )","title":"CHANGELOG"},{"location":"metadata/CHANGELOG/#feature","text":"Create users in metadata backend via API ( #289 ) ( eeba485 ) Add Column Badge API ( #273 ) ( ee0ac63 ) Column Lineage API ( #280 ) ( 681893f ) Table Lineage API ( #262 ) ( e306034 ) Column badges in Atlas Proxy ( #263 ) ( a3efb4c ) Added get_lineage method to neo4j proxy ( #259 ) ( b129cc7 ) Neo4j backend for popular tables personalization ( #233 ) ( d045efa ) Updated popular_tables endpoint to allow optional user_id ( #232 ) ( 5680775 ) AddSwaggerEnabledAsEnvVar ( #215 ) ( 3c9a55e ) Add neptune proxy ( #204 ) ( 09845d4 ) Return column level badges ( #205 ) ( d4d8101 ) Badges with category and badge_type fields ( #201 ) ( 19e1bf8 ) Get last updated ts for AtlasProxy ( #177 ) ( 3e92586 ) Data Owner Implementation of Atlas Proxy ( #156 ) ( 48b4c71 )","title":"Feature"},{"location":"metadata/CHANGELOG/#fix","text":"Sort reports alphabetically ( #293 ) ( c8423c6 ) Reconcile gremlin description writes with the Databuilder ( #290 ) ( 18454fe ) Swagger docs don\u2019t align with common entity ( #283 ) ( db33af9 ) Compatibility changes to the gremlin integration ( #260 ) ( a765424 ) Proxy client creation fails after adding client_kwargs ( #258 ) ( 1880cec ) Reapply previous fix ( #245 ) ( c7dc172 ) Expire is a keyword argument for beaker cache ( #239 ) ( a7b2ec5 ) Get_tags no longer return tag_count 0 tag ( #230 ) ( 5097d2b ) Too many partitions in one Atlas query (Watermarks in Atlas Proxy) ( #217 ) ( cc3768f ) Data_type as Column.col_type. ( #203 ) ( 7b97f62 ) Add /delete bidirectional relations in case of owner ( #206 ) ( 40cd0dd ) Removed all badge_type fields from API ( #202 ) ( 6a81b97 ) Improvements to the Owned By feature ( #178 ) ( 0558d69 ) Get all tags should work for all resources ( #175 ) ( cf1ab6d ) Removing OidcConfig file and making statsd configurable through envrionment variable ( #157 ) ( 2752492 )","title":"Fix"},{"location":"metadata/CHANGELOG/#documentation","text":"Enumeration requires newline ( #291 ) ( cf6710d )","title":"Documentation"},{"location":"metadata/docs/configurations/","text":"Most of the configurations are set through Flask Config Class . BADGES \u00b6 In order to add a badge to a resource you should first add the combination of badge name and category to the in WHITELIST_BADGES Config Class . Example: WHITELIST_BADGES : List [ Badge ] = [ Badge ( badge_name = 'beta' , category = 'table_status' )] Once this is done users will be able to add badge the badges in the whitelist by running: curl -X PUT https://{amundsen metadata url}/table/\"{table key}\"/badge/{badge_name}?category={category} USER_DETAIL_METHOD OPTIONAL \u00b6 This is a method that can be used to get the user details from any third-party or custom system. This custom function takes user_id as a parameter, and returns a dictionary consisting user details\u2019 fields defined in UserSchema . Example: def get_user_details ( user_id ): user_info = { 'email' : 'test@email.com' , 'user_id' : user_id , 'first_name' : 'Firstname' , 'last_name' : 'Lastname' , 'full_name' : 'Firstname Lastname' , } return user_info USER_DETAIL_METHOD = get_user_details STATISTICS_FORMAT_SPEC OPTIONAL \u00b6 This is a variable enabling possibility to reformat statistics displayed in UI. The key is name of statistic and a value is a dictionary with optional keys: * new_name - how to rename statistic (if absent proxy should default to old name) * format - how to format numerical statistics (if absent, proxy should default to original format) * drop - should given statistic not be displayed in UI (if absent, proxy should keep it) Example (if you\u2019re using deeque library), you might want to: STATISTICS_FORMAT_SPEC = { 'stdDev' : dict ( new_name = 'standard deviation' , format = ' {:,.2f} ' ), 'mean' : dict ( format = ' {:,.2f} ' ), 'maximum' : dict ( format = ' {:,.2f} ' ), 'minimum' : dict ( format = ' {:,.2f} ' ), 'completeness' : dict ( format = ' {:.2%} ' ), 'approximateNumDistinctValues' : dict ( new_name = 'distinct values' , format = ' {:,.0f} ' , ), 'sum' : dict ( drop = True ) }","title":"Overview"},{"location":"metadata/docs/configurations/#badges","text":"In order to add a badge to a resource you should first add the combination of badge name and category to the in WHITELIST_BADGES Config Class . Example: WHITELIST_BADGES : List [ Badge ] = [ Badge ( badge_name = 'beta' , category = 'table_status' )] Once this is done users will be able to add badge the badges in the whitelist by running: curl -X PUT https://{amundsen metadata url}/table/\"{table key}\"/badge/{badge_name}?category={category}","title":"BADGES"},{"location":"metadata/docs/configurations/#user_detail_method-optional","text":"This is a method that can be used to get the user details from any third-party or custom system. This custom function takes user_id as a parameter, and returns a dictionary consisting user details\u2019 fields defined in UserSchema . Example: def get_user_details ( user_id ): user_info = { 'email' : 'test@email.com' , 'user_id' : user_id , 'first_name' : 'Firstname' , 'last_name' : 'Lastname' , 'full_name' : 'Firstname Lastname' , } return user_info USER_DETAIL_METHOD = get_user_details","title":"USER_DETAIL_METHOD OPTIONAL"},{"location":"metadata/docs/configurations/#statistics_format_spec-optional","text":"This is a variable enabling possibility to reformat statistics displayed in UI. The key is name of statistic and a value is a dictionary with optional keys: * new_name - how to rename statistic (if absent proxy should default to old name) * format - how to format numerical statistics (if absent, proxy should default to original format) * drop - should given statistic not be displayed in UI (if absent, proxy should keep it) Example (if you\u2019re using deeque library), you might want to: STATISTICS_FORMAT_SPEC = { 'stdDev' : dict ( new_name = 'standard deviation' , format = ' {:,.2f} ' ), 'mean' : dict ( format = ' {:,.2f} ' ), 'maximum' : dict ( format = ' {:,.2f} ' ), 'minimum' : dict ( format = ' {:,.2f} ' ), 'completeness' : dict ( format = ' {:.2%} ' ), 'approximateNumDistinctValues' : dict ( new_name = 'distinct values' , format = ' {:,.0f} ' , ), 'sum' : dict ( drop = True ) }","title":"STATISTICS_FORMAT_SPEC OPTIONAL"},{"location":"metadata/docs/structure/","text":"Amundsen metadata service consists of three packages, API, Entity, and Proxy. API package \u00b6 A package that contains Flask Restful resources that serves Restful API request. The routing of API is being registered here . Proxy package \u00b6 Proxy package contains proxy modules that talks dependencies of Metadata service. There are currently three modules in Proxy package, Neo4j , Statsd and Atlas Selecting the appropriate proxy (Neo4j or Atlas) is configurable using a config variable PROXY_CLIENT , which takes the path to class name of proxy module available here . Note: Proxy\u2019s host and port are configured using config variables PROXY_HOST and PROXY_PORT respectively. Both of these variables can be set using environment variables. Neo4j proxy module \u00b6 Neo4j proxy module serves various use case of getting metadata or updating metadata from or into Neo4j. Most of the methods have Cypher query for the use case, execute the query and transform into entity . Apache Atlas proxy module \u00b6 Apache Atlas proxy module serves all of the metadata from Apache Atlas, using apache_atlas . More information on how to setup Apache Atlas to make it compatible with Amundsen can be found here Statsd utilities module \u00b6 Statsd utilities module has methods / functions to support statsd to publish metrics. By default, statsd integration is disabled and you can turn in on from Metadata service configuration . For specific configuration related to statsd, you can configure it through environment variable. Entity package \u00b6 Entity package contains many modules where each module has many Python classes in it. These Python classes are being used as a schema and a data holder. All data exchange within Amundsen Metadata service use classes in Entity to ensure validity of itself and improve readability and mainatability. Configurations \u00b6 There are different settings you might want to change depending on the application environment like toggling the debug mode, setting the proxy, and other such environment-specific things.","title":"Metadata API Structure"},{"location":"metadata/docs/structure/#api-package","text":"A package that contains Flask Restful resources that serves Restful API request. The routing of API is being registered here .","title":"API package"},{"location":"metadata/docs/structure/#proxy-package","text":"Proxy package contains proxy modules that talks dependencies of Metadata service. There are currently three modules in Proxy package, Neo4j , Statsd and Atlas Selecting the appropriate proxy (Neo4j or Atlas) is configurable using a config variable PROXY_CLIENT , which takes the path to class name of proxy module available here . Note: Proxy\u2019s host and port are configured using config variables PROXY_HOST and PROXY_PORT respectively. Both of these variables can be set using environment variables.","title":"Proxy package"},{"location":"metadata/docs/structure/#neo4j-proxy-module","text":"Neo4j proxy module serves various use case of getting metadata or updating metadata from or into Neo4j. Most of the methods have Cypher query for the use case, execute the query and transform into entity .","title":"Neo4j proxy module"},{"location":"metadata/docs/structure/#apache-atlas-proxy-module","text":"Apache Atlas proxy module serves all of the metadata from Apache Atlas, using apache_atlas . More information on how to setup Apache Atlas to make it compatible with Amundsen can be found here","title":"Apache Atlas proxy module"},{"location":"metadata/docs/structure/#statsd-utilities-module","text":"Statsd utilities module has methods / functions to support statsd to publish metrics. By default, statsd integration is disabled and you can turn in on from Metadata service configuration . For specific configuration related to statsd, you can configure it through environment variable.","title":"Statsd utilities module"},{"location":"metadata/docs/structure/#entity-package","text":"Entity package contains many modules where each module has many Python classes in it. These Python classes are being used as a schema and a data holder. All data exchange within Amundsen Metadata service use classes in Entity to ensure validity of itself and improve readability and mainatability.","title":"Entity package"},{"location":"metadata/docs/structure/#configurations","text":"There are different settings you might want to change depending on the application environment like toggling the debug mode, setting the proxy, and other such environment-specific things.","title":"Configurations"},{"location":"metadata/docs/proxy/atlas_proxy/","text":"Atlas Proxy \u00b6 In order to make the Atlas-Amundsen integration smooth, we\u2019ve developed a python package, amundsenatlastypes that has all the required entity definitions along with helper functions needed to make Atlas compatible with Amundsen. Usage and Installation of amundsenatlastypes can be found here Minimum Requirements: amundsenatlastypes==1.2.0 apache_atlas==0.0.11 Configurations \u00b6 Once you are done with setting up required entity definitions using amundsenatlastypes , you are all set to use Atlas with Amundsen. Other things to configure: Popular Tables","title":"Overview"},{"location":"metadata/docs/proxy/atlas_proxy/#atlas-proxy","text":"In order to make the Atlas-Amundsen integration smooth, we\u2019ve developed a python package, amundsenatlastypes that has all the required entity definitions along with helper functions needed to make Atlas compatible with Amundsen. Usage and Installation of amundsenatlastypes can be found here Minimum Requirements: amundsenatlastypes==1.2.0 apache_atlas==0.0.11","title":"Atlas Proxy"},{"location":"metadata/docs/proxy/atlas_proxy/#configurations","text":"Once you are done with setting up required entity definitions using amundsenatlastypes , you are all set to use Atlas with Amundsen. Other things to configure: Popular Tables","title":"Configurations"},{"location":"metadata/docs/proxy/gremlin/","text":"Gremlin Proxy \u00b6 What the heck is Gremlin? Why is it named Gremlin? \u00b6 Gremin is the graph traversal language of Apache TinkerPop . Why not Gremlin? Documentation \u00b6 The docs linked from Gremin are a good start. For example, the Getting Started and the PRACTICAL GREMLIN book How to target a new Gremlin backend \u00b6 This is not an exhaustive list, but some issues we\u2019ve found along the way: - Are there restricted property names? For example JanusGraph does not allow a property named key , so the base Gremlin proxy has a property named key_property_name which is set to _key for JanusGraph but key for others. - Is there database management required? For example AWS Neptune does now allow explicit creation of indexes, nor assigning data types to properties, but JanusGraph does and practically requires the creation of indexes. - Are there restrictions on the methods? For example, JanusGraph accepts any of the Java or Groovy names, but Neptune accepts a strict subset. JanusGraph can install any script engine, e.g. to allow Python lambdas but Neptune only allows Groovy lambdas. Other differences between Janusgraph and Neptune can be found here: https://docs.aws.amazon.com/neptune/latest/userguide/access-graph-gremlin-differences.html","title":"Gremlin Backend"},{"location":"metadata/docs/proxy/gremlin/#gremlin-proxy","text":"","title":"Gremlin Proxy"},{"location":"metadata/docs/proxy/gremlin/#what-the-heck-is-gremlin-why-is-it-named-gremlin","text":"Gremin is the graph traversal language of Apache TinkerPop . Why not Gremlin?","title":"What the heck is Gremlin?  Why is it named Gremlin?"},{"location":"metadata/docs/proxy/gremlin/#documentation","text":"The docs linked from Gremin are a good start. For example, the Getting Started and the PRACTICAL GREMLIN book","title":"Documentation"},{"location":"metadata/docs/proxy/gremlin/#how-to-target-a-new-gremlin-backend","text":"This is not an exhaustive list, but some issues we\u2019ve found along the way: - Are there restricted property names? For example JanusGraph does not allow a property named key , so the base Gremlin proxy has a property named key_property_name which is set to _key for JanusGraph but key for others. - Is there database management required? For example AWS Neptune does now allow explicit creation of indexes, nor assigning data types to properties, but JanusGraph does and practically requires the creation of indexes. - Are there restrictions on the methods? For example, JanusGraph accepts any of the Java or Groovy names, but Neptune accepts a strict subset. JanusGraph can install any script engine, e.g. to allow Python lambdas but Neptune only allows Groovy lambdas. Other differences between Janusgraph and Neptune can be found here: https://docs.aws.amazon.com/neptune/latest/userguide/access-graph-gremlin-differences.html","title":"How to target a new Gremlin backend"},{"location":"metadata/docs/proxy/mysql/","text":"MySQL Proxy \u00b6 MySQL proxy works with SQLAlchemy ORM and Amundsen RDS containing all the ORM models used for Amundsen databuilder and metadataservice. Requirements \u00b6 MySQL >= 5.7 Schema migration \u00b6 Before running MySQL as the Amundsen metadata store, we need initialize/upgrade all the table schemas first. The schema migration is managed by cli in metadata service and Alembic behind. Add config for MySQL \u00b6 PROXY_CLI = PROXY_CLIS['MYSQL'] SQLALCHEMY_DATABASE_URI = os.environ.get('SQLALCHEMY_DATABASE_URI', {mysql_connection_string}) # other fileds used for mysql proxy ...... see metadata config Add environment variables \u00b6 Specify how to load the application: export FLASK_APP=metadata_service/metadata_wsgi.py Run migration commands \u00b6 If we need initialize/upgrade db: flask rds initdb If we need reset db: flask rds resetdb --yes or flask rds resetdb (then enter y after a prompt message.)","title":"MySQL Proxy"},{"location":"metadata/docs/proxy/mysql/#mysql-proxy","text":"MySQL proxy works with SQLAlchemy ORM and Amundsen RDS containing all the ORM models used for Amundsen databuilder and metadataservice.","title":"MySQL Proxy"},{"location":"metadata/docs/proxy/mysql/#requirements","text":"MySQL >= 5.7","title":"Requirements"},{"location":"metadata/docs/proxy/mysql/#schema-migration","text":"Before running MySQL as the Amundsen metadata store, we need initialize/upgrade all the table schemas first. The schema migration is managed by cli in metadata service and Alembic behind.","title":"Schema migration"},{"location":"metadata/docs/proxy/mysql/#add-config-for-mysql","text":"PROXY_CLI = PROXY_CLIS['MYSQL'] SQLALCHEMY_DATABASE_URI = os.environ.get('SQLALCHEMY_DATABASE_URI', {mysql_connection_string}) # other fileds used for mysql proxy ...... see metadata config","title":"Add config for MySQL"},{"location":"metadata/docs/proxy/mysql/#add-environment-variables","text":"Specify how to load the application: export FLASK_APP=metadata_service/metadata_wsgi.py","title":"Add environment variables"},{"location":"metadata/docs/proxy/mysql/#run-migration-commands","text":"If we need initialize/upgrade db: flask rds initdb If we need reset db: flask rds resetdb --yes or flask rds resetdb (then enter y after a prompt message.)","title":"Run migration commands"},{"location":"metadata/docs/proxy/neptune/","text":"Neptune \u00b6 Documentation \u00b6 In particular, see Gremlin differences , and Gremlin sessions . And any time you see docs from Kelvin (like the PRACTICAL GREMLIN book or lots of stackoverflow) pay attention, he works for AWS on Neptune. IAM authentication \u00b6 The gremlin transport is usually websockets, and the requests-aws4auth library we use elsewhere is for requests, which does not support websockets at all. So we rolled our in aws4authwebsocket . The saving grace of websockets and IAM is that the IAM authentication really only applies to the initialization request and the rest of the data flows over the existing TCP connection. The usual gremlin-python transport is Tornado, which was a huge pain to try and insinuate the aws4 autentication in to, so we use the websockets-client library instead. How to get a gremlin console for AWS \u00b6 They have pretty decent recipe here","title":"Neptune Backend"},{"location":"metadata/docs/proxy/neptune/#neptune","text":"","title":"Neptune"},{"location":"metadata/docs/proxy/neptune/#documentation","text":"In particular, see Gremlin differences , and Gremlin sessions . And any time you see docs from Kelvin (like the PRACTICAL GREMLIN book or lots of stackoverflow) pay attention, he works for AWS on Neptune.","title":"Documentation"},{"location":"metadata/docs/proxy/neptune/#iam-authentication","text":"The gremlin transport is usually websockets, and the requests-aws4auth library we use elsewhere is for requests, which does not support websockets at all. So we rolled our in aws4authwebsocket . The saving grace of websockets and IAM is that the IAM authentication really only applies to the initialization request and the rest of the data flows over the existing TCP connection. The usual gremlin-python transport is Tornado, which was a huge pain to try and insinuate the aws4 autentication in to, so we use the websockets-client library instead.","title":"IAM authentication"},{"location":"metadata/docs/proxy/neptune/#how-to-get-a-gremlin-console-for-aws","text":"They have pretty decent recipe here","title":"How to get a gremlin console for AWS"},{"location":"metadata/docs/proxy/atlas/popular_tables/","text":"Popular Tables Configurations \u00b6 The required entity definitions for Atlas can be applied using amundsenatlastypes . Popular Tables \u00b6 Amundsen has a concept of popular tables, which is a default entry point of the application for now. Popular Tables API leverages popularityScore attribute of Table super type to enable custom sorting strategy. The suggested formula to generate the popularity score is provided below and should be applied by the external script or batch/stream process to update Atlas entities accordingly. Popularity score = number of distinct readers * log(total number of reads) Table entity definition with popularityScore attribute amundsenatlastypes==1.2.0 . { \"entityDefs\" : [ { \"name\" : \"Table\" , \"superTypes\" : [ \"DataSet\" ], \"attributeDefs\" : [ { \"name\" : \"popularityScore\" , \"typeName\" : \"float\" , \"isOptional\" : true , \"cardinality\" : \"SINGLE\" , \"isUnique\" : false , \"isIndexable\" : false , \"defaultValue\" : \"0.0\" }, { \"name\" : \"readers\" , \"typeName\" : \"array<Reader>\" , \"isOptional\" : true , \"cardinality\" : \"LIST\" , \"isUnique\" : false , \"isIndexable\" : true , \"includeInNotification\" : false } ] } ] }","title":"Popular Table"},{"location":"metadata/docs/proxy/atlas/popular_tables/#popular-tables-configurations","text":"The required entity definitions for Atlas can be applied using amundsenatlastypes .","title":"Popular Tables Configurations"},{"location":"metadata/docs/proxy/atlas/popular_tables/#popular-tables","text":"Amundsen has a concept of popular tables, which is a default entry point of the application for now. Popular Tables API leverages popularityScore attribute of Table super type to enable custom sorting strategy. The suggested formula to generate the popularity score is provided below and should be applied by the external script or batch/stream process to update Atlas entities accordingly. Popularity score = number of distinct readers * log(total number of reads) Table entity definition with popularityScore attribute amundsenatlastypes==1.2.0 . { \"entityDefs\" : [ { \"name\" : \"Table\" , \"superTypes\" : [ \"DataSet\" ], \"attributeDefs\" : [ { \"name\" : \"popularityScore\" , \"typeName\" : \"float\" , \"isOptional\" : true , \"cardinality\" : \"SINGLE\" , \"isUnique\" : false , \"isIndexable\" : false , \"defaultValue\" : \"0.0\" }, { \"name\" : \"readers\" , \"typeName\" : \"array<Reader>\" , \"isOptional\" : true , \"cardinality\" : \"LIST\" , \"isUnique\" : false , \"isIndexable\" : true , \"includeInNotification\" : false } ] } ] }","title":"Popular Tables"},{"location":"search/","text":"Amundsen Search Service \u00b6 Amundsen Search service serves a Restful API and is responsible for searching metadata. The service leverages Elasticsearch for most of it\u2019s search capabilites. By default, it creates in total 3 indexes: * table_search_index * user_search_index * dashboard_search_index For information about Amundsen and our other services, refer to this README.md . Please also see our instructions for a quick start setup of Amundsen with dummy data, and an overview of the architecture . Requirements \u00b6 Python >= 3.6 elasticsearch 6.x (currently it doesn\u2019t support 7.x) Doc \u00b6 https://www.amundsen.io/amundsen Instructions to start the Search service from distribution \u00b6 $ venv_path =[ path_for_virtual_environment ] $ python3 -m venv $venv_path $ source $venv_path /bin/activate $ python3 setup.py install $ python3 search_service/search_wsgi.py # In a different terminal, verify the service is up by running $ curl -v http://localhost:5001/healthcheck Instructions to start the Search service from source \u00b6 $ git clone https://github.com/amundsen-io/amundsen.git $ cd search $ venv_path =[ path_for_virtual_environment ] $ python3 -m venv $venv_path $ source $venv_path /bin/activate $ pip3 install -e \".[all]\" . $ python3 search_service/search_wsgi.py # In a different terminal, verify the service is up by running $ curl -v http://localhost:5001/healthcheck Instructions to start the service from Docker \u00b6 $ docker pull amundsendev/amundsen-search:latest $ docker run -p 5001 :5001 amundsendev/amundsen-search # - alternative, for production environment with Gunicorn (see its homepage link below) $ ## docker run -p 5001:5001 amundsendev/amundsen-search gunicorn --bind 0.0.0.0:5001 search_service.search_wsgi # In a different terminal, verify the service is up by running $ curl -v http://localhost:5001/healthcheck Production environment \u00b6 By default, Flask comes with a Werkzeug webserver, which is used for development. For production environments a production grade web server such as Gunicorn should be used. $ pip3 install gunicorn $ gunicorn search_service.search_wsgi # In a different terminal, verify the service is up by running $ curl -v http://localhost:8000/healthcheck For more imformation see the Gunicorn configuration documentation . Configuration outside local environment \u00b6 By default, Search service uses LocalConfig that looks for Elasticsearch running in localhost. In order to use different end point, you need to create a Config suitable for your use case. Once a config class has been created, it can be referenced by an environment variable : SEARCH_SVC_CONFIG_MODULE_CLASS For example, in order to have different config for production, you can inherit Config class, create Production config and passing production config class into environment variable. Let\u2019s say class name is ProdConfig and it\u2019s in search_service.config module. then you can set as below: SEARCH_SVC_CONFIG_MODULE_CLASS=search_service.config.ProdConfig This way Search service will use production config in production environment. For more information on how the configuration is being loaded and used, here\u2019s reference from Flask doc . Developer guide \u00b6 Code style \u00b6 PEP 8: Amundsen Search service follows PEP8 - Style Guide for Python Code . Typing hints: Amundsen Search service also utilizes Typing hint for better readability. API documentation \u00b6 We have Swagger documentation setup with OpenApi 3.0.2. This documentation is generated via Flasgger . When adding or updating an API please make sure to update the documentation. To see the documentation run the application locally and go to localhost:5001/apidocs/ . Currently the documentation only works with local configuration. Code structure \u00b6 Amundsen Search service consists of three packages, API, Models, and Proxy. API package \u00b6 A package that contains Flask Restful resources that serves Restful API request. The routing of API is being registered here . Proxy package \u00b6 Proxy package contains proxy modules that talks dependencies of Search service. There are currently two modules in Proxy package, Elasticsearch and Statsd . Elasticsearch proxy module \u00b6 Elasticsearch proxy module serves various use case of searching metadata from Elasticsearch. It uses Query DSL for the use case, execute the search query and transform into model . Atlas proxy module \u00b6 Apache Atlas proxy module uses Atlas to serve the Atlas requests. At the moment the Basic Search REST API is used via the Python Client . Statsd utilities module \u00b6 Statsd utilities module has methods / functions to support statsd to publish metrics. By default, statsd integration is disabled and you can turn in on from Search service configuration . For specific configuration related to statsd, you can configure it through environment variable. Models package \u00b6 Models package contains many modules where each module has many Python classes in it. These Python classes are being used as a schema and a data holder. All data exchange within Amundsen Search service use classes in Models to ensure validity of itself and improve readability and maintainability.","title":"Overview"},{"location":"search/#amundsen-search-service","text":"Amundsen Search service serves a Restful API and is responsible for searching metadata. The service leverages Elasticsearch for most of it\u2019s search capabilites. By default, it creates in total 3 indexes: * table_search_index * user_search_index * dashboard_search_index For information about Amundsen and our other services, refer to this README.md . Please also see our instructions for a quick start setup of Amundsen with dummy data, and an overview of the architecture .","title":"Amundsen Search Service"},{"location":"search/#requirements","text":"Python >= 3.6 elasticsearch 6.x (currently it doesn\u2019t support 7.x)","title":"Requirements"},{"location":"search/#doc","text":"https://www.amundsen.io/amundsen","title":"Doc"},{"location":"search/#instructions-to-start-the-search-service-from-distribution","text":"$ venv_path =[ path_for_virtual_environment ] $ python3 -m venv $venv_path $ source $venv_path /bin/activate $ python3 setup.py install $ python3 search_service/search_wsgi.py # In a different terminal, verify the service is up by running $ curl -v http://localhost:5001/healthcheck","title":"Instructions to start the Search service from distribution"},{"location":"search/#instructions-to-start-the-search-service-from-source","text":"$ git clone https://github.com/amundsen-io/amundsen.git $ cd search $ venv_path =[ path_for_virtual_environment ] $ python3 -m venv $venv_path $ source $venv_path /bin/activate $ pip3 install -e \".[all]\" . $ python3 search_service/search_wsgi.py # In a different terminal, verify the service is up by running $ curl -v http://localhost:5001/healthcheck","title":"Instructions to start the Search service from source"},{"location":"search/#instructions-to-start-the-service-from-docker","text":"$ docker pull amundsendev/amundsen-search:latest $ docker run -p 5001 :5001 amundsendev/amundsen-search # - alternative, for production environment with Gunicorn (see its homepage link below) $ ## docker run -p 5001:5001 amundsendev/amundsen-search gunicorn --bind 0.0.0.0:5001 search_service.search_wsgi # In a different terminal, verify the service is up by running $ curl -v http://localhost:5001/healthcheck","title":"Instructions to start the service from Docker"},{"location":"search/#production-environment","text":"By default, Flask comes with a Werkzeug webserver, which is used for development. For production environments a production grade web server such as Gunicorn should be used. $ pip3 install gunicorn $ gunicorn search_service.search_wsgi # In a different terminal, verify the service is up by running $ curl -v http://localhost:8000/healthcheck For more imformation see the Gunicorn configuration documentation .","title":"Production environment"},{"location":"search/#configuration-outside-local-environment","text":"By default, Search service uses LocalConfig that looks for Elasticsearch running in localhost. In order to use different end point, you need to create a Config suitable for your use case. Once a config class has been created, it can be referenced by an environment variable : SEARCH_SVC_CONFIG_MODULE_CLASS For example, in order to have different config for production, you can inherit Config class, create Production config and passing production config class into environment variable. Let\u2019s say class name is ProdConfig and it\u2019s in search_service.config module. then you can set as below: SEARCH_SVC_CONFIG_MODULE_CLASS=search_service.config.ProdConfig This way Search service will use production config in production environment. For more information on how the configuration is being loaded and used, here\u2019s reference from Flask doc .","title":"Configuration outside local environment"},{"location":"search/#developer-guide","text":"","title":"Developer guide"},{"location":"search/#code-style","text":"PEP 8: Amundsen Search service follows PEP8 - Style Guide for Python Code . Typing hints: Amundsen Search service also utilizes Typing hint for better readability.","title":"Code style"},{"location":"search/#api-documentation","text":"We have Swagger documentation setup with OpenApi 3.0.2. This documentation is generated via Flasgger . When adding or updating an API please make sure to update the documentation. To see the documentation run the application locally and go to localhost:5001/apidocs/ . Currently the documentation only works with local configuration.","title":"API documentation"},{"location":"search/#code-structure","text":"Amundsen Search service consists of three packages, API, Models, and Proxy.","title":"Code structure"},{"location":"search/#api-package","text":"A package that contains Flask Restful resources that serves Restful API request. The routing of API is being registered here .","title":"API package"},{"location":"search/#proxy-package","text":"Proxy package contains proxy modules that talks dependencies of Search service. There are currently two modules in Proxy package, Elasticsearch and Statsd .","title":"Proxy package"},{"location":"search/#elasticsearch-proxy-module","text":"Elasticsearch proxy module serves various use case of searching metadata from Elasticsearch. It uses Query DSL for the use case, execute the search query and transform into model .","title":"Elasticsearch proxy module"},{"location":"search/#atlas-proxy-module","text":"Apache Atlas proxy module uses Atlas to serve the Atlas requests. At the moment the Basic Search REST API is used via the Python Client .","title":"Atlas proxy module"},{"location":"search/#statsd-utilities-module","text":"Statsd utilities module has methods / functions to support statsd to publish metrics. By default, statsd integration is disabled and you can turn in on from Search service configuration . For specific configuration related to statsd, you can configure it through environment variable.","title":"Statsd utilities module"},{"location":"search/#models-package","text":"Models package contains many modules where each module has many Python classes in it. These Python classes are being used as a schema and a data holder. All data exchange within Amundsen Search service use classes in Models to ensure validity of itself and improve readability and maintainability.","title":"Models package"},{"location":"search/CHANGELOG/","text":"Feature \u00b6 Add dashboard search filter support ( #112 ) ( 17c6739 ) Use query parameter for basic search [AtlasProxy] ( #105 ) ( 9961fde ) Fix \u00b6 Add id field to Dashboard ( #174 ) ( 53634fa ) Fix table post/put api bug ( #172 ) ( 38bccba ) Fix dashboard model errors, change deprecated pytest function ( #160 ) ( 1304234 ) Documentation \u00b6 Fix build and coverage status links in README.md ( #134 ) ( 1c95ff4 )","title":"CHANGELOG"},{"location":"search/CHANGELOG/#feature","text":"Add dashboard search filter support ( #112 ) ( 17c6739 ) Use query parameter for basic search [AtlasProxy] ( #105 ) ( 9961fde )","title":"Feature"},{"location":"search/CHANGELOG/#fix","text":"Add id field to Dashboard ( #174 ) ( 53634fa ) Fix table post/put api bug ( #172 ) ( 38bccba ) Fix dashboard model errors, change deprecated pytest function ( #160 ) ( 1304234 )","title":"Fix"},{"location":"search/CHANGELOG/#documentation","text":"Fix build and coverage status links in README.md ( #134 ) ( 1c95ff4 )","title":"Documentation"},{"location":"search/docs/atlas-search/","text":"Atlas search investigation \u00b6 There are several approaches to integrate searching within Apache Atlas , we describe multiple options below: Use Data Builder to fill Elasticsearch from Atlas \u00b6 Atlas search data extractor can be used to synchronize Atlas with Elasticsearch. This method requires you to: deploy Elasticsearch register a process that synchronizes the data between Atlas and Elasticsearch We suggest using Elasticsearch as backend for Atlas janusgraph (it\u2019s possible with latest Atlas version) and additionally sync data with databuilder to have indices compatible with Amundsen Elasticsearch Search Proxy. Mixing Atlas Metadata Proxy with Elasticsearch Search Proxy is 100% safe and fully compatible. Raw janusgraph indices are not compatible with Amundsen Elasticsearch Search Proxy and it would require implementing custom class over Elasticsearch Search Proxy. This is preferred way of handling Amundsen search. Advantages \u00b6 The performance is 10-20x better (verified on production environment) Possibility to search on many fields at the same time (and defining importance of each field) Much better and flexible relevancy scoring Disadvantages \u00b6 Requires additional component (Elasticsearch) if Apache Solr is used for Atlas search backend Requires scheduling (cron, airflow, kubernetes cron job) of databuilder app to synchronize the data periodically The data in Elasticsearch is as fresh as frequent syncing app - there might be misalignment between Atlas Metadata and Elasticsearch index Use Atlas REST API \u00b6 Directly using the Atlas API\u2019s is quick to implement and easy to setup for administrators. Atlas uses a search engine behind the scenes (Solr and Elasticsearch are fully supported) to perform search queries. Advantages \u00b6 Quicker way to achieve Amundsen <> Atlas integration Data in search is available as soon as it\u2019s indexed in Atlas Simpler setup (less components/applications) Disadvantages \u00b6 Atlas Search API is very limited in terms of multi-field search and relevancy tuning Atlas Search API has suboptimal performance and doesn\u2019t really leverage underlying full text engine (it\u2019s heavily abstracted by janusgraph) Amundsen AtlasProxy for search might be lagging in features as it\u2019s not as popular as Elasticsearch Proxy Discussion \u00b6 Both the REST API approach and the data builder approach can be configurable. Both approaches have their own benefits, the data builder together provides a more fine-tuned search whereas the Atlas REST API comes out of the box with Atlas. The focus was initially to implement the REST API approach but after several months on production we decided to introduce Atlas search data extractor and use Elasticsearch Proxy for Amundsen search. It proved to be much more robust and flexible solution. The disadvantages were quickly eclipsed by advantages.","title":"Atlas Backend"},{"location":"search/docs/atlas-search/#atlas-search-investigation","text":"There are several approaches to integrate searching within Apache Atlas , we describe multiple options below:","title":"Atlas search investigation"},{"location":"search/docs/atlas-search/#use-data-builder-to-fill-elasticsearch-from-atlas","text":"Atlas search data extractor can be used to synchronize Atlas with Elasticsearch. This method requires you to: deploy Elasticsearch register a process that synchronizes the data between Atlas and Elasticsearch We suggest using Elasticsearch as backend for Atlas janusgraph (it\u2019s possible with latest Atlas version) and additionally sync data with databuilder to have indices compatible with Amundsen Elasticsearch Search Proxy. Mixing Atlas Metadata Proxy with Elasticsearch Search Proxy is 100% safe and fully compatible. Raw janusgraph indices are not compatible with Amundsen Elasticsearch Search Proxy and it would require implementing custom class over Elasticsearch Search Proxy. This is preferred way of handling Amundsen search.","title":"Use Data Builder to fill Elasticsearch from Atlas"},{"location":"search/docs/atlas-search/#advantages","text":"The performance is 10-20x better (verified on production environment) Possibility to search on many fields at the same time (and defining importance of each field) Much better and flexible relevancy scoring","title":"Advantages"},{"location":"search/docs/atlas-search/#disadvantages","text":"Requires additional component (Elasticsearch) if Apache Solr is used for Atlas search backend Requires scheduling (cron, airflow, kubernetes cron job) of databuilder app to synchronize the data periodically The data in Elasticsearch is as fresh as frequent syncing app - there might be misalignment between Atlas Metadata and Elasticsearch index","title":"Disadvantages"},{"location":"search/docs/atlas-search/#use-atlas-rest-api","text":"Directly using the Atlas API\u2019s is quick to implement and easy to setup for administrators. Atlas uses a search engine behind the scenes (Solr and Elasticsearch are fully supported) to perform search queries.","title":"Use Atlas REST API"},{"location":"search/docs/atlas-search/#advantages_1","text":"Quicker way to achieve Amundsen <> Atlas integration Data in search is available as soon as it\u2019s indexed in Atlas Simpler setup (less components/applications)","title":"Advantages"},{"location":"search/docs/atlas-search/#disadvantages_1","text":"Atlas Search API is very limited in terms of multi-field search and relevancy tuning Atlas Search API has suboptimal performance and doesn\u2019t really leverage underlying full text engine (it\u2019s heavily abstracted by janusgraph) Amundsen AtlasProxy for search might be lagging in features as it\u2019s not as popular as Elasticsearch Proxy","title":"Disadvantages"},{"location":"search/docs/atlas-search/#discussion","text":"Both the REST API approach and the data builder approach can be configurable. Both approaches have their own benefits, the data builder together provides a more fine-tuned search whereas the Atlas REST API comes out of the box with Atlas. The focus was initially to implement the REST API approach but after several months on production we decided to introduce Atlas search data extractor and use Elasticsearch Proxy for Amundsen search. It proved to be much more robust and flexible solution. The disadvantages were quickly eclipsed by advantages.","title":"Discussion"},{"location":"tutorials/badges/","text":"How to add table level and column level badges \u00b6 Amundsen supports use of clickable badges on tables, and non clickable badges for columns. Clickable badges trigger a search for all of the resources with the given badge name as a filter. Table badge Column badge Badges configuration \u00b6 In order for amundsen to accept new badges via metadata and to change the style in the UI there are two configs that need to be setup: On amundsen metadata library you should add your badges to the whitelist within your custom configuration file following the format of this example: # whitelist badges WHITELIST_BADGES: List[Badge] = [ Badge(badge_name='alpha', category='table_status'), Badge(badge_name='beta', category='table_status'), ] In order to set up the color and display name on amundsen frontend library you should add the desired badges style as follows: const configCustom: AppConfigCustom = { badges: { 'alpha': { style: BadgeStyle.DEFAULT, displayName: 'Alpha', }, 'partition column': { style: BadgeStyle.DEFAULT, displayName: 'Partition Column', }, } } Note: any badges that are not defined in this configuration will show up with BadgeStyle.DEFAULT . Adding table badges through metadata library \u00b6 To manually add a badge to a particular table the metadata API can be used. Here are the available requests: To add a badge on a table: curl -X PUT https://{your metadata url}/table/{table key}/badge/{badge name}?category={badge category} To delete a badge on a table: curl -X DELETE https://{your metadata url}/table/{table key}/badge/{badge name}?category={badge category} To add a badge on a column: curl -X PUT https://{your metadata url}/table/{table key}/column/{column name}/badge/{badge name}?category={badge category} To delete a badge on a column: curl -X DELETE https://{your metadata url}/table/{table key}/column/{column name}/badge/{badge name}?category={badge category} table key is in the format of datasource://database.schema/table Adding badges throught databuilder (and column level badges) \u00b6 To add badges using databuilder, you can use the BadgeMetadata class and pass in the entity you want to create a badge relationship for. For an example of how this is done search for badge in TableMetadata to see how we add badge nodes and relationships to neo4j. In hive_table_metadata_extractor.py you can see how the partition column is obtained and added to a column so the badge node can be created and related to the correct column.","title":"How to add table level and column level badges"},{"location":"tutorials/badges/#how-to-add-table-level-and-column-level-badges","text":"Amundsen supports use of clickable badges on tables, and non clickable badges for columns. Clickable badges trigger a search for all of the resources with the given badge name as a filter. Table badge Column badge","title":"How to add table level and column level badges"},{"location":"tutorials/badges/#badges-configuration","text":"In order for amundsen to accept new badges via metadata and to change the style in the UI there are two configs that need to be setup: On amundsen metadata library you should add your badges to the whitelist within your custom configuration file following the format of this example: # whitelist badges WHITELIST_BADGES: List[Badge] = [ Badge(badge_name='alpha', category='table_status'), Badge(badge_name='beta', category='table_status'), ] In order to set up the color and display name on amundsen frontend library you should add the desired badges style as follows: const configCustom: AppConfigCustom = { badges: { 'alpha': { style: BadgeStyle.DEFAULT, displayName: 'Alpha', }, 'partition column': { style: BadgeStyle.DEFAULT, displayName: 'Partition Column', }, } } Note: any badges that are not defined in this configuration will show up with BadgeStyle.DEFAULT .","title":"Badges configuration"},{"location":"tutorials/badges/#adding-table-badges-through-metadata-library","text":"To manually add a badge to a particular table the metadata API can be used. Here are the available requests: To add a badge on a table: curl -X PUT https://{your metadata url}/table/{table key}/badge/{badge name}?category={badge category} To delete a badge on a table: curl -X DELETE https://{your metadata url}/table/{table key}/badge/{badge name}?category={badge category} To add a badge on a column: curl -X PUT https://{your metadata url}/table/{table key}/column/{column name}/badge/{badge name}?category={badge category} To delete a badge on a column: curl -X DELETE https://{your metadata url}/table/{table key}/column/{column name}/badge/{badge name}?category={badge category} table key is in the format of datasource://database.schema/table","title":"Adding table badges through metadata library"},{"location":"tutorials/badges/#adding-badges-throught-databuilder-and-column-level-badges","text":"To add badges using databuilder, you can use the BadgeMetadata class and pass in the entity you want to create a badge relationship for. For an example of how this is done search for badge in TableMetadata to see how we add badge nodes and relationships to neo4j. In hive_table_metadata_extractor.py you can see how the partition column is obtained and added to a column so the badge node can be created and related to the correct column.","title":"Adding badges throught databuilder (and column level badges)"},{"location":"tutorials/data-preview-with-superset/","text":"How to setup a preview client with Apache Superset \u00b6 In the previous tutorial , we talked about how to index the table metadata for a postgres database. In this tutorial, we will walk through how to configure data preview for this films table using Apache Superset. Amundsen provides an integration between Amundsen and BI Viz tool for data preview. It is not necessary to use Apache Superset as long as the BI Viz tool provides endpoint to do querying and get the results back from the BI tool. Apache Superset is an open-source business intelligence tool that can be used for data exploration and it is what we leverage internally at Lyft to support the feature. Please setup Apache Superset following its official installation guide : # Install superset pip install apache-superset # Initialize the database superset db upgrade # Create an admin user (you will be prompted to set a username, first and last name before setting a password) $ export FLASK_APP = superset superset fab create-admin # Load some data to play with superset load_examples # Create default roles and permissions superset init # To start a development web server on port 8088, use -p to bind to another port superset run -p 8088 --with-threads --reload --debugger Once setup properly, you could view the superset UI as following: We need to add the postgres database to superset as the following: We could verify the content of the films table using superset\u2019s sqlab feature: Next, We need to build a preview client following this guide and the example client code . There are a couple of things to keep in mind: We could start with an unauthenticated Superset( example superset config ), but in production, we will need to send the impersonate info to Superset to properly verify whether the given user could view the data. When we build the client, we could need to configure the database id instead of the database name when send the request to superset. Once we configure the preview client, put it in the frontend service entry point ( example ) and restart the frontend. We could now view the preview data for the films table in Amundsen. From the above figure, the preview button on the table page is clickable. Once it clicked, you could see the actual data queried from Apache Superset:","title":"How to setup a preview client with Apache Superset"},{"location":"tutorials/data-preview-with-superset/#how-to-setup-a-preview-client-with-apache-superset","text":"In the previous tutorial , we talked about how to index the table metadata for a postgres database. In this tutorial, we will walk through how to configure data preview for this films table using Apache Superset. Amundsen provides an integration between Amundsen and BI Viz tool for data preview. It is not necessary to use Apache Superset as long as the BI Viz tool provides endpoint to do querying and get the results back from the BI tool. Apache Superset is an open-source business intelligence tool that can be used for data exploration and it is what we leverage internally at Lyft to support the feature. Please setup Apache Superset following its official installation guide : # Install superset pip install apache-superset # Initialize the database superset db upgrade # Create an admin user (you will be prompted to set a username, first and last name before setting a password) $ export FLASK_APP = superset superset fab create-admin # Load some data to play with superset load_examples # Create default roles and permissions superset init # To start a development web server on port 8088, use -p to bind to another port superset run -p 8088 --with-threads --reload --debugger Once setup properly, you could view the superset UI as following: We need to add the postgres database to superset as the following: We could verify the content of the films table using superset\u2019s sqlab feature: Next, We need to build a preview client following this guide and the example client code . There are a couple of things to keep in mind: We could start with an unauthenticated Superset( example superset config ), but in production, we will need to send the impersonate info to Superset to properly verify whether the given user could view the data. When we build the client, we could need to configure the database id instead of the database name when send the request to superset. Once we configure the preview client, put it in the frontend service entry point ( example ) and restart the frontend. We could now view the preview data for the films table in Amundsen. From the above figure, the preview button on the table page is clickable. Once it clicked, you could see the actual data queried from Apache Superset:","title":"How to setup a preview client with Apache Superset"},{"location":"tutorials/how-to-search-effective/","text":"How to search Amundsen effectively \u00b6 The goal of this tutorial is to provide a few tips on how to search for datasets effectively in Amundsen. Overview \u00b6 Amundsen currently indexes three types of entities: tables, people, and dashboards. This tutorial mostly covers how to search for a table entity effectively. We will cover other entities in the future. General Search \u00b6 Once the users are on the Amundsen home page, the users could search for any random information in the search bar. In the backend, the search system will use the same query term from users and search across three different entities (tables, people, and dashboards) and return the results with the highest ranking. For Table search, it will search across different fields, including table name, schema name, table or column descriptions, tags and etc. Amundsen also supports typeahead search which will search in the backend as soon as users enter new characters. Tips: If you know the full table name (e.g. schema.table), try to search with that full table name, which will provide as the top result in general. If you are unsure of the table name, search with word1 word2 with space in between. For example, if your table\u2019s name is test.test_rides but you don\u2019t know the exact table name but only know the table name includes test and rides, please search with test rides (space in between). In this case, Amundsen will return tables that match either test or rides and union the result together based on the search algorithm ranking. If you know your table name but don\u2019t know the schema of the table name, you could search with word1_word2 . For example, if you know your table name is test_rides , please search with test_rides that will only return the table matched that given name. Advanced Search \u00b6 If you want to do the traditional faceted search, which will allow users to apply multiple filters, you could try out the advanced search. Currently, only the table entity is supported with the advanced search. But we plan to add the support for the dashboard entity as well in the near future. You could use wildcard in the search box as well. In the above example, the users put rides* on the table box. This will search across all the tables that have rides* as table name from different databases, including bigquery/druid/hive/presto/rs, etc. If you want to narrow down the search results, you could put more filters. In the above example, the users try to search a table name that is rides* , which has beta as the badge. Once the search is finished, you could see only one table matches the criteria (test.rides in this case). Searching Ranking Algorithm Demystified \u00b6 Currently, Amundsen provides the same search ranking for all the different personas. It ranks the table based on the query count in the presto search query log from the past 90 days at Lyft. It could be different based on your company\u2019s setup. Try out different search heuristic \u00b6 You could always try out different search heuristic using the kibana devtools. For example for table, you could use: GET table_search_index/_search { \"query\": { \"function_score\": { \"query\": { \"multi_match\": { \"query\": \"$term\", \"fields\": [\"display_name^1000\", \"name.raw^75\", \"name^5\", \"schema^3\", \"description^3\", \"column_names^2\", \"column_descriptions\", \"tags\", \"badges\", \"programmatic_descriptions\"] } }, \"field_value_factor\": { \"field\": \"total_usage\", \"modifier\": \"log2p\" } } } } The result will be ranked with certain weight based on total usage. It is the same as the following with painless script: \"function_score\": { \"query\": { \"multi_match\": { \"query\": query_term, \"fields\": [\"display_name^1000\", \"name.raw^75\", \"name^5\", \"schema^3\", \"description^3\", \"column_names^2\", \"column_descriptions\", \"tags\", \"badges\", \"programmatic_descriptions\"], } }, \"functions\": [ { \"script_score\": { \"script\": \"def scores = 0; scores = doc['total_usage'].value; return _score * Math.log10(2 + scores); }\" } } ] } If you want to boot the search result that has certain badge: \"function_score\": { \"query\": { \"multi_match\": { \"query\": query_term, \"fields\": [\"display_name^1000\", \"name.raw^75\", \"name^5\", \"schema^3\", \"description^3\", \"column_names^2\", \"column_descriptions\", \"tags\", \"badges\", \"programmatic_descriptions\"], } }, \"functions\": [ { \"script_score\": { \"script\": \"def scores = 0; scores = doc['total_usage'].value; if (doc['badges'].value == \" \"'$badge_for_boost') {return _score * Math.log10(2 + scores) \" \"* 1.5} else{ return _score * Math.log10(2 + scores); }\" } } ] } In this case, the table with a badge ($badge_for_boost or replace with your own badge), the search ranking score will get boosted. For dashboard, you could try out the following: GET dashboard_search_index/_search { \"query\": { \"function_score\": { \"query\": { \"multi_match\": { \"query\": \"$search-term\", \"fields\": [\"name.raw^75\", \"name^7\", \"group_name.raw^15\", \"group_name^7\", \"description^3\", \"query_names^3\"] } }, \"field_value_factor\": { \"field\": \"total_usage\", \"modifier\": \"log2p\" } } } } Hope this tutorial gives you some ideas on how the search works.","title":"How to search Amundsen effectively"},{"location":"tutorials/how-to-search-effective/#how-to-search-amundsen-effectively","text":"The goal of this tutorial is to provide a few tips on how to search for datasets effectively in Amundsen.","title":"How to search Amundsen effectively"},{"location":"tutorials/how-to-search-effective/#overview","text":"Amundsen currently indexes three types of entities: tables, people, and dashboards. This tutorial mostly covers how to search for a table entity effectively. We will cover other entities in the future.","title":"Overview"},{"location":"tutorials/how-to-search-effective/#general-search","text":"Once the users are on the Amundsen home page, the users could search for any random information in the search bar. In the backend, the search system will use the same query term from users and search across three different entities (tables, people, and dashboards) and return the results with the highest ranking. For Table search, it will search across different fields, including table name, schema name, table or column descriptions, tags and etc. Amundsen also supports typeahead search which will search in the backend as soon as users enter new characters. Tips: If you know the full table name (e.g. schema.table), try to search with that full table name, which will provide as the top result in general. If you are unsure of the table name, search with word1 word2 with space in between. For example, if your table\u2019s name is test.test_rides but you don\u2019t know the exact table name but only know the table name includes test and rides, please search with test rides (space in between). In this case, Amundsen will return tables that match either test or rides and union the result together based on the search algorithm ranking. If you know your table name but don\u2019t know the schema of the table name, you could search with word1_word2 . For example, if you know your table name is test_rides , please search with test_rides that will only return the table matched that given name.","title":"General Search"},{"location":"tutorials/how-to-search-effective/#advanced-search","text":"If you want to do the traditional faceted search, which will allow users to apply multiple filters, you could try out the advanced search. Currently, only the table entity is supported with the advanced search. But we plan to add the support for the dashboard entity as well in the near future. You could use wildcard in the search box as well. In the above example, the users put rides* on the table box. This will search across all the tables that have rides* as table name from different databases, including bigquery/druid/hive/presto/rs, etc. If you want to narrow down the search results, you could put more filters. In the above example, the users try to search a table name that is rides* , which has beta as the badge. Once the search is finished, you could see only one table matches the criteria (test.rides in this case).","title":"Advanced Search"},{"location":"tutorials/how-to-search-effective/#searching-ranking-algorithm-demystified","text":"Currently, Amundsen provides the same search ranking for all the different personas. It ranks the table based on the query count in the presto search query log from the past 90 days at Lyft. It could be different based on your company\u2019s setup.","title":"Searching Ranking Algorithm Demystified"},{"location":"tutorials/how-to-search-effective/#try-out-different-search-heuristic","text":"You could always try out different search heuristic using the kibana devtools. For example for table, you could use: GET table_search_index/_search { \"query\": { \"function_score\": { \"query\": { \"multi_match\": { \"query\": \"$term\", \"fields\": [\"display_name^1000\", \"name.raw^75\", \"name^5\", \"schema^3\", \"description^3\", \"column_names^2\", \"column_descriptions\", \"tags\", \"badges\", \"programmatic_descriptions\"] } }, \"field_value_factor\": { \"field\": \"total_usage\", \"modifier\": \"log2p\" } } } } The result will be ranked with certain weight based on total usage. It is the same as the following with painless script: \"function_score\": { \"query\": { \"multi_match\": { \"query\": query_term, \"fields\": [\"display_name^1000\", \"name.raw^75\", \"name^5\", \"schema^3\", \"description^3\", \"column_names^2\", \"column_descriptions\", \"tags\", \"badges\", \"programmatic_descriptions\"], } }, \"functions\": [ { \"script_score\": { \"script\": \"def scores = 0; scores = doc['total_usage'].value; return _score * Math.log10(2 + scores); }\" } } ] } If you want to boot the search result that has certain badge: \"function_score\": { \"query\": { \"multi_match\": { \"query\": query_term, \"fields\": [\"display_name^1000\", \"name.raw^75\", \"name^5\", \"schema^3\", \"description^3\", \"column_names^2\", \"column_descriptions\", \"tags\", \"badges\", \"programmatic_descriptions\"], } }, \"functions\": [ { \"script_score\": { \"script\": \"def scores = 0; scores = doc['total_usage'].value; if (doc['badges'].value == \" \"'$badge_for_boost') {return _score * Math.log10(2 + scores) \" \"* 1.5} else{ return _score * Math.log10(2 + scores); }\" } } ] } In this case, the table with a badge ($badge_for_boost or replace with your own badge), the search ranking score will get boosted. For dashboard, you could try out the following: GET dashboard_search_index/_search { \"query\": { \"function_score\": { \"query\": { \"multi_match\": { \"query\": \"$search-term\", \"fields\": [\"name.raw^75\", \"name^7\", \"group_name.raw^15\", \"group_name^7\", \"description^3\", \"query_names^3\"] } }, \"field_value_factor\": { \"field\": \"total_usage\", \"modifier\": \"log2p\" } } } } Hope this tutorial gives you some ideas on how the search works.","title":"Try out different search heuristic"},{"location":"tutorials/how-to-track-user-metric/","text":"How to track Amundsen user metric \u00b6 After you have deployed Amundsen into production, you want to track how user interacts with Amundsen for various reasons. The easier way is to leverage Google Analytics for basic user tracking. You could first get the analytics token for your domain and put it as the frontend config Besides implementing Google Analytics, we provide a way called action_logging to do fine grained user action tracking. The action_logging is a decorator to allow you to integrate user info and pipe it to your inhouse event tracking system(e.g Kafka). You need to put the custom method into entry_points following this example . And here is the IDL proto we used at Lyft to send the event message: message UserAction { // Sending host name google.protobuf.StringValue host_name = 1 ; // start time in epoch ms google.protobuf.Int64Value start_epoch_ms = 2 ; // end time in epoch ms google.protobuf.Int64Value end_epoch_ms = 3 ; // json array contains positional arguments common.LongString pos_args_json = 4 ; // json object contains key word arguments common.LongString keyword_args_json = 5 ; // json object contains output of command common.LongString output = 6 ; // an error message or exception stacktrace common.LongString error = 7 ; // ` user ` google.protobuf.StringValue user = 8 ; } It matches the action log model defined in here . Once you have the event in your data warehouse, you could start building different KPI user metric: WAU Sample query if the event table named as default.event_amundsenfrontend_user_action SELECT date_trunc('week', CAST(\"ds\" AS TIMESTAMP)) AS \"__timestamp\", COUNT(DISTINCT user_value) AS \"count_distinct_active_users\" FROM (SELECT * FROM default.event_amundsenfrontend_user_action WHERE ds > '2019-09-01') AS \"expr_qry\" WHERE \"ds\" >= '2020-04-21 00:00:00.000000' AND \"ds\" <= '2020-10-21 05:31:14.000000' GROUP BY date_trunc('week', CAST(\"ds\" AS TIMESTAMP)) ORDER BY \"count_distinct_active_users\" DESC LIMIT 10000 DAU Sample query if the event table named as default.event_amundsenfrontend_user_action SELECT date_trunc('day', CAST(\"ds\" AS TIMESTAMP)) AS \"__timestamp\", COUNT(DISTINCT user_value) AS \"count_distinct_active_users\" FROM (SELECT * FROM default.event_amundsenfrontend_user_action WHERE ds > '2019-09-01') AS \"expr_qry\" WHERE \"ds\" >= '2020-07-21 00:00:00.000000' AND \"ds\" <= '2020-10-21 00:00:00.000000' GROUP BY date_trunc('day', CAST(\"ds\" AS TIMESTAMP)) ORDER BY \"count_distinct_active_users\" DESC LIMIT 50000 You could also exclude weekends: SELECT date_trunc('day', CAST(\"ds\" AS TIMESTAMP)) AS \"__timestamp\", COUNT(DISTINCT user_value) AS \"count_distinct_active_users\" FROM (SELECT * FROM default.event_amundsenfrontend_user_action WHERE ds > '2019-09-01') AS \"expr_qry\" WHERE \"ds\" >= '2020-04-21 00:00:00.000000' AND \"ds\" <= '2020-10-21 05:33:11.000000' AND day_of_week(logged_at) NOT IN (6, 7) GROUP BY date_trunc('day', CAST(\"ds\" AS TIMESTAMP)) ORDER BY \"count_distinct_active_users\" DESC LIMIT 50000 User Penetration per role Sample query if the event table named as default.event_amundsenfrontend_user_action and a table for user: SELECT \"title\" AS \"title\", COUNT(DISTINCT email) * 100 / MAX(role_count) AS \"penetration_percent\" FROM (SELECT e.occurred_at, u.email, u.title, tmp.role_count FROM default.family_user u JOIN default.event_amundsenfrontend_user_action e ON u.email = e.user_value JOIN (SELECT title, count(*) role_count FROM default.family_user GROUP BY 1) as tmp ON u.title = tmp.title where ds is not NULL) AS \"expr_qry\" WHERE \"occurred_at\" >= from_iso8601_timestamp('2020-10-14T00:00:00.000000') AND \"occurred_at\" <= from_iso8601_timestamp('2020-10-21T00:00:00.000000') AND \"role_count\" > 20 GROUP BY \"title\" ORDER BY \"penetration_percent\" DESC LIMIT 100 Usage breakdown per role_count sample query: SELECT \"title\" AS \"title\", count(\"email\") AS \"COUNT(email)\" FROM (SELECT e.occurred_at, u.email, u.title, tmp.role_count FROM default.family_user u JOIN default.event_amundsenfrontend_user_action e ON u.email = e.user_value JOIN (SELECT title, count(*) role_count FROM default.family_user GROUP BY 1) as tmp ON u.title = tmp.title where ds is not NULL) AS \"expr_qry\" WHERE \"occurred_at\" >= from_iso8601_timestamp('2020-10-14T00:00:00.000000') AND \"occurred_at\" <= from_iso8601_timestamp('2020-10-21T00:00:00.000000') GROUP BY \"title\" ORDER BY \"COUNT(email)\" DESC LIMIT 15 search click through rate sample query: SELECT date_trunc('day', CAST(\"occurred_at\" AS TIMESTAMP)) AS \"__timestamp\", SUM(CASE WHEN CAST(json_extract_scalar(keyword_args_json, '$.index') AS BIGINT) <= 3 THEN 1 ELSE 0 END) * 100 / COUNT(*) AS \"click_through_rate\" FROM (SELECT * FROM default.event_amundsenfrontend_user_action WHERE ds > '2019-09-01') AS \"expr_qry\" WHERE \"occurred_at\" >= from_iso8601_timestamp('2020-09-21T00:00:00.000000') AND \"occurred_at\" <= from_iso8601_timestamp('2020-10-21T00:00:00.000000') AND \"command\" IN ('_get_table_metadata', '_get_dashboard_metadata', '_log_get_user') AND json_extract_scalar(keyword_args_json, '$.source') IN ('search_results', 'inline_search') GROUP BY date_trunc('day', CAST(\"occurred_at\" AS TIMESTAMP)) ORDER BY \"click_through_rate\" DESC LIMIT 10000 Top 50 active user Top search term Top popular tables Search click index Metadata edits Metadata edit leaders Amundsen user per role (by joining with employee data) \u2026","title":"How to track user metric for Amundsen"},{"location":"tutorials/how-to-track-user-metric/#how-to-track-amundsen-user-metric","text":"After you have deployed Amundsen into production, you want to track how user interacts with Amundsen for various reasons. The easier way is to leverage Google Analytics for basic user tracking. You could first get the analytics token for your domain and put it as the frontend config Besides implementing Google Analytics, we provide a way called action_logging to do fine grained user action tracking. The action_logging is a decorator to allow you to integrate user info and pipe it to your inhouse event tracking system(e.g Kafka). You need to put the custom method into entry_points following this example . And here is the IDL proto we used at Lyft to send the event message: message UserAction { // Sending host name google.protobuf.StringValue host_name = 1 ; // start time in epoch ms google.protobuf.Int64Value start_epoch_ms = 2 ; // end time in epoch ms google.protobuf.Int64Value end_epoch_ms = 3 ; // json array contains positional arguments common.LongString pos_args_json = 4 ; // json object contains key word arguments common.LongString keyword_args_json = 5 ; // json object contains output of command common.LongString output = 6 ; // an error message or exception stacktrace common.LongString error = 7 ; // ` user ` google.protobuf.StringValue user = 8 ; } It matches the action log model defined in here . Once you have the event in your data warehouse, you could start building different KPI user metric: WAU Sample query if the event table named as default.event_amundsenfrontend_user_action SELECT date_trunc('week', CAST(\"ds\" AS TIMESTAMP)) AS \"__timestamp\", COUNT(DISTINCT user_value) AS \"count_distinct_active_users\" FROM (SELECT * FROM default.event_amundsenfrontend_user_action WHERE ds > '2019-09-01') AS \"expr_qry\" WHERE \"ds\" >= '2020-04-21 00:00:00.000000' AND \"ds\" <= '2020-10-21 05:31:14.000000' GROUP BY date_trunc('week', CAST(\"ds\" AS TIMESTAMP)) ORDER BY \"count_distinct_active_users\" DESC LIMIT 10000 DAU Sample query if the event table named as default.event_amundsenfrontend_user_action SELECT date_trunc('day', CAST(\"ds\" AS TIMESTAMP)) AS \"__timestamp\", COUNT(DISTINCT user_value) AS \"count_distinct_active_users\" FROM (SELECT * FROM default.event_amundsenfrontend_user_action WHERE ds > '2019-09-01') AS \"expr_qry\" WHERE \"ds\" >= '2020-07-21 00:00:00.000000' AND \"ds\" <= '2020-10-21 00:00:00.000000' GROUP BY date_trunc('day', CAST(\"ds\" AS TIMESTAMP)) ORDER BY \"count_distinct_active_users\" DESC LIMIT 50000 You could also exclude weekends: SELECT date_trunc('day', CAST(\"ds\" AS TIMESTAMP)) AS \"__timestamp\", COUNT(DISTINCT user_value) AS \"count_distinct_active_users\" FROM (SELECT * FROM default.event_amundsenfrontend_user_action WHERE ds > '2019-09-01') AS \"expr_qry\" WHERE \"ds\" >= '2020-04-21 00:00:00.000000' AND \"ds\" <= '2020-10-21 05:33:11.000000' AND day_of_week(logged_at) NOT IN (6, 7) GROUP BY date_trunc('day', CAST(\"ds\" AS TIMESTAMP)) ORDER BY \"count_distinct_active_users\" DESC LIMIT 50000 User Penetration per role Sample query if the event table named as default.event_amundsenfrontend_user_action and a table for user: SELECT \"title\" AS \"title\", COUNT(DISTINCT email) * 100 / MAX(role_count) AS \"penetration_percent\" FROM (SELECT e.occurred_at, u.email, u.title, tmp.role_count FROM default.family_user u JOIN default.event_amundsenfrontend_user_action e ON u.email = e.user_value JOIN (SELECT title, count(*) role_count FROM default.family_user GROUP BY 1) as tmp ON u.title = tmp.title where ds is not NULL) AS \"expr_qry\" WHERE \"occurred_at\" >= from_iso8601_timestamp('2020-10-14T00:00:00.000000') AND \"occurred_at\" <= from_iso8601_timestamp('2020-10-21T00:00:00.000000') AND \"role_count\" > 20 GROUP BY \"title\" ORDER BY \"penetration_percent\" DESC LIMIT 100 Usage breakdown per role_count sample query: SELECT \"title\" AS \"title\", count(\"email\") AS \"COUNT(email)\" FROM (SELECT e.occurred_at, u.email, u.title, tmp.role_count FROM default.family_user u JOIN default.event_amundsenfrontend_user_action e ON u.email = e.user_value JOIN (SELECT title, count(*) role_count FROM default.family_user GROUP BY 1) as tmp ON u.title = tmp.title where ds is not NULL) AS \"expr_qry\" WHERE \"occurred_at\" >= from_iso8601_timestamp('2020-10-14T00:00:00.000000') AND \"occurred_at\" <= from_iso8601_timestamp('2020-10-21T00:00:00.000000') GROUP BY \"title\" ORDER BY \"COUNT(email)\" DESC LIMIT 15 search click through rate sample query: SELECT date_trunc('day', CAST(\"occurred_at\" AS TIMESTAMP)) AS \"__timestamp\", SUM(CASE WHEN CAST(json_extract_scalar(keyword_args_json, '$.index') AS BIGINT) <= 3 THEN 1 ELSE 0 END) * 100 / COUNT(*) AS \"click_through_rate\" FROM (SELECT * FROM default.event_amundsenfrontend_user_action WHERE ds > '2019-09-01') AS \"expr_qry\" WHERE \"occurred_at\" >= from_iso8601_timestamp('2020-09-21T00:00:00.000000') AND \"occurred_at\" <= from_iso8601_timestamp('2020-10-21T00:00:00.000000') AND \"command\" IN ('_get_table_metadata', '_get_dashboard_metadata', '_log_get_user') AND json_extract_scalar(keyword_args_json, '$.source') IN ('search_results', 'inline_search') GROUP BY date_trunc('day', CAST(\"occurred_at\" AS TIMESTAMP)) ORDER BY \"click_through_rate\" DESC LIMIT 10000 Top 50 active user Top search term Top popular tables Search click index Metadata edits Metadata edit leaders Amundsen user per role (by joining with employee data) \u2026","title":"How to track Amundsen user metric"},{"location":"tutorials/how-to-use-amundsen-with-aws-neptune/","text":"How to use Amundsen with Amazon Neptune \u00b6 An alternative to Neo4j as Amundsen\u2019s database is Amazon Neptune . This tutorial will go into setting up Amundsen to integrate with Neptune. If you want to find out how to set up a Neptune instance you can find that information at https://docs.aws.amazon.com/neptune/latest/userguide/neptune-setup.html . Configuring your Databuilder jobs to use Neptune \u00b6 The Neptune integration follows the same pattern as the rest of Amundsen\u2019s databuilder library. Each job contains a task and a publisher and each task comprises of a extractor, transformer, and loader. The Neptune databuilder integration was built so that it was compatible with all the of the extractors (and the models produced by those extractors) so that only the loader and publisher diverge from the Neo4j integration. Note: Even though the Databuilder may support the model the Metadata Service might not. Loading data into Neptune \u00b6 The sample_data_loader_neptune.py script contains examples on how to ingest data into Neptune. However the main components are the FSNeptuneCSVLoader and the NeptuneCSVPublisher The FSNeptuneCSVLoader is responsible for converting the GraphNode and GraphRelationship into a csv format that the Neptune bulk loader expects. The FSNeptuneCSVLoader has 5 configuration keys * NODE_DIR_PATH - Where the node csv files should go * RELATION_DIR_PATH - Where the relationship csv files should go * FORCE_CREATE_DIR - Should the loader overwrite any existing files (Default is False) * SHOULD_DELETE_CREATED_DIR - Should the loader delete the files once the job is over (Default is True) * JOB_PUBLISHER_TAG - A tag that all models published by this job share. (should be unique) NeptuneCSVPublisher takes the csv files produced by the FSNeptuneCSVLoader and ingesting them into Neptune. It achieves this by using the Neptune\u2019s bulk loader API . The flow of the NeptuneCSVPublisher is: 1. Upload the csv files to S3. 2. Initiating a bulk loading request 3. Poll on that status of the request till it reports a success or failure The NeptuneCSVPublisher has the following configuration keys: * NODE_FILES_DIR - Where the publisher will look for node files * RELATION_FILES_DIR - Where the publisher will look for relationship files * AWS_S3_BUCKET_NAME - The name of the S3 bucket where the publisher will upload the files to. * AWS_BASE_S3_DATA_PATH - The location within the bucket where the publisher will upload the files * NEPTUNE_HOST - The Neptune host in the format of <HOST>:<PORT> no protocol included * AWS_REGION - The AWS region where the Neptune instance is located. * AWS_ACCESS_KEY - AWS access key (Optional) * AWS_SECRET_ACCESS_KEY - AWS access secret access key (Optional) * AWS_SESSION_TOKEN - AWS session token if you are using temporary credentials (Optional) * AWS_IAM_ROLE_NAME - IAM ROLE NAME used for the the bulk loading * FAIL_ON_ERROR - If set to True an exception will be raised on failure (default False) * STATUS_POLLING_PERIOD - Period in seconds checking on the status of the bulk loading request Publishing data to Search from Neptune \u00b6 In order to have your entities searchable on the front end you need to extract the data from Neptune and push it into your elasticsearch cluster so the search service can query it. To achieve this the data builder comes with the NeptuneSearchDataExtractor which can be integrated with the FSElasticsearchJSONLoader and the ElasticsearchPublisher . A example job can be found in the sample_data_loader_neptune.py in the create_es_publisher_sample_job function. The NeptuneSearchDataExtractor supports extracting table, user, and dashboard models in a format that FSElasticsearchJSONLoader accepts. It has the following configuration keys: * ENTITY_TYPE_CONFIG_KEY - Type of model being extracted. This supports table, user, dashboard (defaults to table) * MODEL_CLASS_CONFIG_KEY - Python path of class to cast the extracted data to. (Optional) * JOB_PUBLISH_TAG_CONFIG_KEY - Allows you to filter your extraction to a job tag. (Optional) * QUERY_FUNCTION_CONFIG_KEY - Allows you to pass in a extraction query of your own (Optional) * QUERY_FUNCTION_KWARGS_CONFIG_KEY - Keyword arguments for the custom QUERY_FUNCTION (Optional) The NeptuneSearchDataExtractor uses the NeptuneSessionClient to extract data from Neptune. The NeptuneSessionClient supports the following configuration keys: * NEPTUNE_HOST_NAME - The Neptune host in the format of <HOST>:<PORT> no protocol included * AWS_REGION - The AWS region where the Neptune instance is located. * AWS_ACCESS_KEY - AWS access key (Optional) * AWS_SECRET_ACCESS_KEY - AWS access secret access key (Optional) * AWS_SESSION_TOKEN - AWS session token if you are using temporary credentials (Optional) Removing stale data from Neptune \u00b6 Metadata often changes so the neptune_staleness_removal_task is used to remove old nodes and relationships. The databuilder contains an example script using the neptune_staleness_removal_task. Configuring the Metadata Service to use Neptune \u00b6 To set up Neptune for the Metadata Service you can copy the NeptuneConfig and point the environment variable METADATA_SVC_CONFIG_MODULE_CLASS to it. For example: export METADATA_SVC_CONFIG_MODULE_CLASS=metadata_service.config.NeptuneConfig The NeptuneConfig requires a few environment variables to be set these are: * PROXY_HOST - The host name of the Neptune instance. Formatted like: wss://<NEPTUNE_URL>:<NEPTUNE_PORT>/gremlin * AWS_REGION - The AWS region where the Neptune instance is located. * S3_BUCKET_NAME - The location where the proxy can upload S3 files for bulk uploader In addition to the Config the IGNORE_NEPTUNE_SHARD environment variable must be set to \u2018True\u2019 if you are using the default databuilder integration.","title":"How to use Amundsen with Amazon Neptune"},{"location":"tutorials/how-to-use-amundsen-with-aws-neptune/#how-to-use-amundsen-with-amazon-neptune","text":"An alternative to Neo4j as Amundsen\u2019s database is Amazon Neptune . This tutorial will go into setting up Amundsen to integrate with Neptune. If you want to find out how to set up a Neptune instance you can find that information at https://docs.aws.amazon.com/neptune/latest/userguide/neptune-setup.html .","title":"How to use Amundsen with Amazon Neptune"},{"location":"tutorials/how-to-use-amundsen-with-aws-neptune/#configuring-your-databuilder-jobs-to-use-neptune","text":"The Neptune integration follows the same pattern as the rest of Amundsen\u2019s databuilder library. Each job contains a task and a publisher and each task comprises of a extractor, transformer, and loader. The Neptune databuilder integration was built so that it was compatible with all the of the extractors (and the models produced by those extractors) so that only the loader and publisher diverge from the Neo4j integration. Note: Even though the Databuilder may support the model the Metadata Service might not.","title":"Configuring your Databuilder jobs to use Neptune"},{"location":"tutorials/how-to-use-amundsen-with-aws-neptune/#loading-data-into-neptune","text":"The sample_data_loader_neptune.py script contains examples on how to ingest data into Neptune. However the main components are the FSNeptuneCSVLoader and the NeptuneCSVPublisher The FSNeptuneCSVLoader is responsible for converting the GraphNode and GraphRelationship into a csv format that the Neptune bulk loader expects. The FSNeptuneCSVLoader has 5 configuration keys * NODE_DIR_PATH - Where the node csv files should go * RELATION_DIR_PATH - Where the relationship csv files should go * FORCE_CREATE_DIR - Should the loader overwrite any existing files (Default is False) * SHOULD_DELETE_CREATED_DIR - Should the loader delete the files once the job is over (Default is True) * JOB_PUBLISHER_TAG - A tag that all models published by this job share. (should be unique) NeptuneCSVPublisher takes the csv files produced by the FSNeptuneCSVLoader and ingesting them into Neptune. It achieves this by using the Neptune\u2019s bulk loader API . The flow of the NeptuneCSVPublisher is: 1. Upload the csv files to S3. 2. Initiating a bulk loading request 3. Poll on that status of the request till it reports a success or failure The NeptuneCSVPublisher has the following configuration keys: * NODE_FILES_DIR - Where the publisher will look for node files * RELATION_FILES_DIR - Where the publisher will look for relationship files * AWS_S3_BUCKET_NAME - The name of the S3 bucket where the publisher will upload the files to. * AWS_BASE_S3_DATA_PATH - The location within the bucket where the publisher will upload the files * NEPTUNE_HOST - The Neptune host in the format of <HOST>:<PORT> no protocol included * AWS_REGION - The AWS region where the Neptune instance is located. * AWS_ACCESS_KEY - AWS access key (Optional) * AWS_SECRET_ACCESS_KEY - AWS access secret access key (Optional) * AWS_SESSION_TOKEN - AWS session token if you are using temporary credentials (Optional) * AWS_IAM_ROLE_NAME - IAM ROLE NAME used for the the bulk loading * FAIL_ON_ERROR - If set to True an exception will be raised on failure (default False) * STATUS_POLLING_PERIOD - Period in seconds checking on the status of the bulk loading request","title":"Loading data into Neptune"},{"location":"tutorials/how-to-use-amundsen-with-aws-neptune/#publishing-data-to-search-from-neptune","text":"In order to have your entities searchable on the front end you need to extract the data from Neptune and push it into your elasticsearch cluster so the search service can query it. To achieve this the data builder comes with the NeptuneSearchDataExtractor which can be integrated with the FSElasticsearchJSONLoader and the ElasticsearchPublisher . A example job can be found in the sample_data_loader_neptune.py in the create_es_publisher_sample_job function. The NeptuneSearchDataExtractor supports extracting table, user, and dashboard models in a format that FSElasticsearchJSONLoader accepts. It has the following configuration keys: * ENTITY_TYPE_CONFIG_KEY - Type of model being extracted. This supports table, user, dashboard (defaults to table) * MODEL_CLASS_CONFIG_KEY - Python path of class to cast the extracted data to. (Optional) * JOB_PUBLISH_TAG_CONFIG_KEY - Allows you to filter your extraction to a job tag. (Optional) * QUERY_FUNCTION_CONFIG_KEY - Allows you to pass in a extraction query of your own (Optional) * QUERY_FUNCTION_KWARGS_CONFIG_KEY - Keyword arguments for the custom QUERY_FUNCTION (Optional) The NeptuneSearchDataExtractor uses the NeptuneSessionClient to extract data from Neptune. The NeptuneSessionClient supports the following configuration keys: * NEPTUNE_HOST_NAME - The Neptune host in the format of <HOST>:<PORT> no protocol included * AWS_REGION - The AWS region where the Neptune instance is located. * AWS_ACCESS_KEY - AWS access key (Optional) * AWS_SECRET_ACCESS_KEY - AWS access secret access key (Optional) * AWS_SESSION_TOKEN - AWS session token if you are using temporary credentials (Optional)","title":"Publishing data to Search from Neptune"},{"location":"tutorials/how-to-use-amundsen-with-aws-neptune/#removing-stale-data-from-neptune","text":"Metadata often changes so the neptune_staleness_removal_task is used to remove old nodes and relationships. The databuilder contains an example script using the neptune_staleness_removal_task.","title":"Removing stale data from Neptune"},{"location":"tutorials/how-to-use-amundsen-with-aws-neptune/#configuring-the-metadata-service-to-use-neptune","text":"To set up Neptune for the Metadata Service you can copy the NeptuneConfig and point the environment variable METADATA_SVC_CONFIG_MODULE_CLASS to it. For example: export METADATA_SVC_CONFIG_MODULE_CLASS=metadata_service.config.NeptuneConfig The NeptuneConfig requires a few environment variables to be set these are: * PROXY_HOST - The host name of the Neptune instance. Formatted like: wss://<NEPTUNE_URL>:<NEPTUNE_PORT>/gremlin * AWS_REGION - The AWS region where the Neptune instance is located. * S3_BUCKET_NAME - The location where the proxy can upload S3 files for bulk uploader In addition to the Config the IGNORE_NEPTUNE_SHARD environment variable must be set to \u2018True\u2019 if you are using the default databuilder integration.","title":"Configuring the Metadata Service to use Neptune"},{"location":"tutorials/index-postgres/","text":"How to index metadata for real life databases \u00b6 From previous doc , we have indexed tables from a csv files. In real production cases, the table metadata is stored in data warehouses(e.g Hive, Postgres, Mysql, Snowflake, Bigquery etc.) which Amundsen has the extractors for metadata extraction. In this tutorial, we will use a postgres db as an example to walk through how to index metadata for a postgres database. The doc won\u2019t cover how to setup a postgres database. In the example, we have a postgres table in localhost postgres named films . We leverage the postgres metadata extractor to extract the metadata information of the postgres database. We could call the metadata extractor in an adhoc python function as this example or from an Airflow DAG. Once we run the script, we could search the films table using Amundsen Search. We could also find and view the films table in the table detail page. This tutorial uses postgres to serve as an example, but you could apply the same approach for your various data warehouses. If Amundsen doesn\u2019t provide the extractor, you could build one based on the API and contribute the extractor back to us!","title":"How to index metadata for real life databases"},{"location":"tutorials/index-postgres/#how-to-index-metadata-for-real-life-databases","text":"From previous doc , we have indexed tables from a csv files. In real production cases, the table metadata is stored in data warehouses(e.g Hive, Postgres, Mysql, Snowflake, Bigquery etc.) which Amundsen has the extractors for metadata extraction. In this tutorial, we will use a postgres db as an example to walk through how to index metadata for a postgres database. The doc won\u2019t cover how to setup a postgres database. In the example, we have a postgres table in localhost postgres named films . We leverage the postgres metadata extractor to extract the metadata information of the postgres database. We could call the metadata extractor in an adhoc python function as this example or from an Airflow DAG. Once we run the script, we could search the films table using Amundsen Search. We could also find and view the films table in the table detail page. This tutorial uses postgres to serve as an example, but you could apply the same approach for your various data warehouses. If Amundsen doesn\u2019t provide the extractor, you could build one based on the API and contribute the extractor back to us!","title":"How to index metadata for real life databases"},{"location":"tutorials/user-profiles/","text":"People resources \u00b6 What can I do with User Resources? \u00b6 User profile pages and the ability to bookmark/favorite and search for users is also available as of now. See a demo of what they feels like from an end user viewpoint from around the 36 minute mark of this September 2019 talk - so you could actually argue that this video snippet can work as an end user guide. How do I enable User pages? \u00b6 The configuration to have Users available consists of: Enable the users profile page index and display feature by performing this frontend configuration There are two different alternative ways to populate user profile data. You can either: Configure the Metadata service to a do a live lookup in some directory service, like LDAP or a HR system. Setup ongoing ingest of user profile data as they onboard/change/offboard into Neo4j and Elasticsearch effectively caching it with the pros/cons of that (similar to what the Databuilder sample loader does from user CSV, see the \u201cpre-cooked demo data\u201d link in the Architecture overview Note Currently, for both of these options Amundsen only provides these hooks/interfaces to add your own implementation. If you build something you think is generally useful, contributions are welcome! Configure login, according to the Authentication guide","title":"How to setup user profiles"},{"location":"tutorials/user-profiles/#people-resources","text":"","title":"People resources"},{"location":"tutorials/user-profiles/#what-can-i-do-with-user-resources","text":"User profile pages and the ability to bookmark/favorite and search for users is also available as of now. See a demo of what they feels like from an end user viewpoint from around the 36 minute mark of this September 2019 talk - so you could actually argue that this video snippet can work as an end user guide.","title":"What can I do with User Resources?"},{"location":"tutorials/user-profiles/#how-do-i-enable-user-pages","text":"The configuration to have Users available consists of: Enable the users profile page index and display feature by performing this frontend configuration There are two different alternative ways to populate user profile data. You can either: Configure the Metadata service to a do a live lookup in some directory service, like LDAP or a HR system. Setup ongoing ingest of user profile data as they onboard/change/offboard into Neo4j and Elasticsearch effectively caching it with the pros/cons of that (similar to what the Databuilder sample loader does from user CSV, see the \u201cpre-cooked demo data\u201d link in the Architecture overview Note Currently, for both of these options Amundsen only provides these hooks/interfaces to add your own implementation. If you build something you think is generally useful, contributions are welcome! Configure login, according to the Authentication guide","title":"How do I enable User pages?"}]}